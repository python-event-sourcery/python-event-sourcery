{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Python Event Sourcery A library for event-based systems in Python. For event sourcing, CQRS, and event-driven architectures.","title":"Event Sourcery"},{"location":"#python-event-sourcery","text":"A library for event-based systems in Python. For event sourcing, CQRS, and event-driven architectures.","title":"Python Event Sourcery"},{"location":"adr/20220730-record-architecture-decisions/","text":"Record architecture decisions Date: 2022-07-30 Status Accepted Context We need to record the architectural decisions made on this project. Decision We will use Architecture Decision Records, as described by Michael Nygard . Consequences See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's adr-tools .","title":"Record architecture decisions"},{"location":"adr/20220730-record-architecture-decisions/#record-architecture-decisions","text":"Date: 2022-07-30","title":"Record architecture decisions"},{"location":"adr/20220730-record-architecture-decisions/#status","text":"Accepted","title":"Status"},{"location":"adr/20220730-record-architecture-decisions/#context","text":"We need to record the architectural decisions made on this project.","title":"Context"},{"location":"adr/20220730-record-architecture-decisions/#decision","text":"We will use Architecture Decision Records, as described by Michael Nygard .","title":"Decision"},{"location":"adr/20220730-record-architecture-decisions/#consequences","text":"See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's adr-tools .","title":"Consequences"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/","text":"Explicit init arguments for SQLAlchemy's models Date: 2023-09-03 Status Draft Context We aim for having full support regarding Possible solutions Use native support for dataclasses in SQLAlchemy Starting from SQLAlchemy 2.0, there's a native support for dataclasses . However, it's less seamless than one might initially imagine. It comes down to inheriting explicitly from another base class, namely sqlalchemy.orm.MappedAsDataclass and using it as a base for models: from sqlalchemy.orm import DeclarativeBase from sqlalchemy.orm import MappedAsDataclass class Base ( MappedAsDataclass , DeclarativeBase ): \"\"\"subclasses will be converted to dataclasses\"\"\" class User ( Base ): __tablename__ = \"user_account\" id : Mapped [ intpk ] = mapped_column ( init = False ) As a result, User becomes a dataclass with attached SQLAlchemy's behaviour. In terms of risk, this may cause compatibility issues. As a reminder, Event Sourcery's models are declared as bare classes (notice lack of inheritance from Base ): class Stream : __tablename__ = \"event_sourcery_streams\" __table_args__ = ( UniqueConstraint ( \"uuid\" , \"category\" ), UniqueConstraint ( \"name\" , \"category\" ), ) id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) The assumption is that someone setting up a project with SQLAlchemy will have their own Base class and models. The library will let them attach our models to their declarative base later using event_sourcery_sqlalchemy.models.configure_models , like: @as_declarative () class Base : pass # initialize Event Sourcery models, so they can be handled by SQLAlchemy and e.g. alembic configure_models ( Base ) The exception is raised in case MappedAsDataclass appears multiple time in model's MRO, for example: - let's say we have a base class for our models that inherits from MappedAsDataclass - someone has their own Base that also inherits from MappedAsDataclass sqlalchemy.exc.InvalidRequestError: Class <class 'event_sourcery_sqlalchemy.models.Stream'> is already a dataclass; ensure that base classes / decorator styles of establishing dataclasses are not being mixed. This can happen if a class that inherits from 'MappedAsDataclass', even indirectly, is been mapped with '@registry.mapped_as_dataclass' To reproduce, add MappedAsDataclass as a base to any model and use this snippet: from sqlalchemy import create_engine from sqlalchemy.orm import DeclarativeBase , MappedAsDataclass from event_sourcery_sqlalchemy.models import configure_models engine = create_engine ( \"sqlite+pysqlite:///:memory:\" , echo = True , future = True ) class Base ( MappedAsDataclass , DeclarativeBase ): pass Base . metadata . create_all ( bind = engine ) # initialize Event Sourcery models, so they can be handled by SQLAlchemy and e.g. alembic configure_models ( Base ) Create base class for models with overridden init There is a recipe for providing base class model with overriding __init__ : class ModelBase : def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : return super () . __init__ ( * args , ** kwargs ) This basically makes mypy unable to verify anything extra because such a signature allows everything to be passed in. Conversely, all operations are considered to be correct from type checker's perspective. This solution is a bit tricky and just gets rid of warnings, but doesn't give us any additional benefits. Define our own, explicit __init__ definition We can simply provide tailored __init__ for each model to get type safety out of the box. class Stream : def __init__ ( self ) -> None : pass This is safe & compatible with other ORM's features because SQLAlchemy uses other means to build an object when e.g. fetching them from the database. In other words, SQLAlchemy internally is not invoking __init__ . Decision Writing custom __init__ makes the most sense. Consequences Whenever someone changes fields of a model (this should be rare after release), __init__ needs to follow. However, this inconvenience can be potentially automated away in the future. At least, we could also have a custom static code check that would guard that.","title":"Explicit Init Arguments For Sqlalchemy S Models"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/#explicit-init-arguments-for-sqlalchemys-models","text":"Date: 2023-09-03","title":"Explicit init arguments for SQLAlchemy's models"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/#status","text":"Draft","title":"Status"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/#context","text":"We aim for having full support regarding","title":"Context"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/#possible-solutions","text":"","title":"Possible solutions"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/#use-native-support-for-dataclasses-in-sqlalchemy","text":"Starting from SQLAlchemy 2.0, there's a native support for dataclasses . However, it's less seamless than one might initially imagine. It comes down to inheriting explicitly from another base class, namely sqlalchemy.orm.MappedAsDataclass and using it as a base for models: from sqlalchemy.orm import DeclarativeBase from sqlalchemy.orm import MappedAsDataclass class Base ( MappedAsDataclass , DeclarativeBase ): \"\"\"subclasses will be converted to dataclasses\"\"\" class User ( Base ): __tablename__ = \"user_account\" id : Mapped [ intpk ] = mapped_column ( init = False ) As a result, User becomes a dataclass with attached SQLAlchemy's behaviour. In terms of risk, this may cause compatibility issues. As a reminder, Event Sourcery's models are declared as bare classes (notice lack of inheritance from Base ): class Stream : __tablename__ = \"event_sourcery_streams\" __table_args__ = ( UniqueConstraint ( \"uuid\" , \"category\" ), UniqueConstraint ( \"name\" , \"category\" ), ) id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) The assumption is that someone setting up a project with SQLAlchemy will have their own Base class and models. The library will let them attach our models to their declarative base later using event_sourcery_sqlalchemy.models.configure_models , like: @as_declarative () class Base : pass # initialize Event Sourcery models, so they can be handled by SQLAlchemy and e.g. alembic configure_models ( Base ) The exception is raised in case MappedAsDataclass appears multiple time in model's MRO, for example: - let's say we have a base class for our models that inherits from MappedAsDataclass - someone has their own Base that also inherits from MappedAsDataclass sqlalchemy.exc.InvalidRequestError: Class <class 'event_sourcery_sqlalchemy.models.Stream'> is already a dataclass; ensure that base classes / decorator styles of establishing dataclasses are not being mixed. This can happen if a class that inherits from 'MappedAsDataclass', even indirectly, is been mapped with '@registry.mapped_as_dataclass' To reproduce, add MappedAsDataclass as a base to any model and use this snippet: from sqlalchemy import create_engine from sqlalchemy.orm import DeclarativeBase , MappedAsDataclass from event_sourcery_sqlalchemy.models import configure_models engine = create_engine ( \"sqlite+pysqlite:///:memory:\" , echo = True , future = True ) class Base ( MappedAsDataclass , DeclarativeBase ): pass Base . metadata . create_all ( bind = engine ) # initialize Event Sourcery models, so they can be handled by SQLAlchemy and e.g. alembic configure_models ( Base )","title":"Use native support for dataclasses in SQLAlchemy"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/#create-base-class-for-models-with-overridden-init","text":"There is a recipe for providing base class model with overriding __init__ : class ModelBase : def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : return super () . __init__ ( * args , ** kwargs ) This basically makes mypy unable to verify anything extra because such a signature allows everything to be passed in. Conversely, all operations are considered to be correct from type checker's perspective. This solution is a bit tricky and just gets rid of warnings, but doesn't give us any additional benefits.","title":"Create base class for models with overridden init"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/#define-our-own-explicit-__init__-definition","text":"We can simply provide tailored __init__ for each model to get type safety out of the box. class Stream : def __init__ ( self ) -> None : pass This is safe & compatible with other ORM's features because SQLAlchemy uses other means to build an object when e.g. fetching them from the database. In other words, SQLAlchemy internally is not invoking __init__ .","title":"Define our own, explicit __init__ definition"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/#decision","text":"Writing custom __init__ makes the most sense.","title":"Decision"},{"location":"adr/20230903-explicit-init-arguments-for-sqlalchemy-s-models/#consequences","text":"Whenever someone changes fields of a model (this should be rare after release), __init__ needs to follow. However, this inconvenience can be potentially automated away in the future. At least, we could also have a custom static code check that would guard that.","title":"Consequences"},{"location":"adr/20231027_1-packaging-of-the-project/","text":"Packaging of the project Date: 2023-10-27 Status Accepted Context Previous code layout haven't stood the test of time and wasn't really aligned with e.g. tests. The latter were testing particular functionalities, also being poor man's documentation (unless we write a proper one). Hence, instead of strictly technical code organization (e.g. interfaces, dtos etc.) we think code should be rather organized around different functionalities. Decision Eventually, we agreed on a following subpackages of event_sourcery (for now): - aggregate - event_store - read_model event_store This is a \"core\" part of the library, with the Event Store itself. It also includes serialization/deserialization based on Pydantic and proper base classes for events. Additionally, a couple of other, foundational things find place here, e.g. StreamId type. aggregate Keeps a base class for Aggregate and Repository that can be used to implement Event Sourcing. read_model (experimental at the moment) This package contains code that tracks streams and can be used as a foundation to build read models. Consequences After changes, library is much more modular and oriented about particular features. However, the approach to importing code for a client of the library will be different - they are now supposed to import code not from top-level event_sourcery package, but from one of it's subpackages. Do: - from event_sourcery.event_store import Event, EventStore - from event_sourcery.aggregate import Aggregate, Repository Don't: - from event_sourcery import EventStore, Aggregate","title":"Packaging Of The Project"},{"location":"adr/20231027_1-packaging-of-the-project/#packaging-of-the-project","text":"Date: 2023-10-27","title":"Packaging of the project"},{"location":"adr/20231027_1-packaging-of-the-project/#status","text":"Accepted","title":"Status"},{"location":"adr/20231027_1-packaging-of-the-project/#context","text":"Previous code layout haven't stood the test of time and wasn't really aligned with e.g. tests. The latter were testing particular functionalities, also being poor man's documentation (unless we write a proper one). Hence, instead of strictly technical code organization (e.g. interfaces, dtos etc.) we think code should be rather organized around different functionalities.","title":"Context"},{"location":"adr/20231027_1-packaging-of-the-project/#decision","text":"Eventually, we agreed on a following subpackages of event_sourcery (for now): - aggregate - event_store - read_model","title":"Decision"},{"location":"adr/20231027_1-packaging-of-the-project/#event_store","text":"This is a \"core\" part of the library, with the Event Store itself. It also includes serialization/deserialization based on Pydantic and proper base classes for events. Additionally, a couple of other, foundational things find place here, e.g. StreamId type.","title":"event_store"},{"location":"adr/20231027_1-packaging-of-the-project/#aggregate","text":"Keeps a base class for Aggregate and Repository that can be used to implement Event Sourcing.","title":"aggregate"},{"location":"adr/20231027_1-packaging-of-the-project/#read_model","text":"(experimental at the moment) This package contains code that tracks streams and can be used as a foundation to build read models.","title":"read_model"},{"location":"adr/20231027_1-packaging-of-the-project/#consequences","text":"After changes, library is much more modular and oriented about particular features. However, the approach to importing code for a client of the library will be different - they are now supposed to import code not from top-level event_sourcery package, but from one of it's subpackages. Do: - from event_sourcery.event_store import Event, EventStore - from event_sourcery.aggregate import Aggregate, Repository Don't: - from event_sourcery import EventStore, Aggregate","title":"Consequences"},{"location":"adr/20231027_2-leave-implementation-of-synchronous-dispatcher-to-users-of-the-library/","text":"Leave implementation of in-memory events handling to users of the library Date: 2023-10-27 Status Accepted Context The library at the early stages had a functionality of dispatching in-memory events. A user of the library could plug-in listeners to particular types of events (also - catch-all listener was supported). When events were published via EventStore with configured listeners, the latter were called synchronously one after another. This happened right after events were persisted. This delivered a very basic mechanism of subscribing to events from e.g. other code modules, similar to e.g. Django signals. However, in-memory events implementation failed to conform to Liskov's Substitute Principle, because the handling and consequences of the errors are different for SQL-based backend and Event Store DB. With SQL-based, everything is wrapped in a transaction. That means if the one of listeners fails, then the whole transaction is rolled back. With Event Store DB (and possibly other stores as well) there are no transactions, so failing in listener would not reverted changes made in Event Store. Hence, some time ago we decided to remove that feature and reconsider adding it back when we understand more. Decision After consideration, we decided to NOT include this feature in the library and instead suggest users that they implement it on their own if only they need it. For SQL-based storages, one can use SQLAlchemy's events, in particular after_insert to plug their own logic after event is persisted. We shall also consider exposing deserialization, so that users can use event objects instead of their representations for persistence (i.e. SQLAlchemy models). The similar approach could be also used for future backend implementations, including Django. Consequences Our maintenance burden would be lower in expense for documenting the approach with leveraging persistence-layer mechanisms, e.g. SQLAlchemy's events.","title":"Leave Implementation Of Synchronous Dispatcher To Users Of The Library"},{"location":"adr/20231027_2-leave-implementation-of-synchronous-dispatcher-to-users-of-the-library/#leave-implementation-of-in-memory-events-handling-to-users-of-the-library","text":"Date: 2023-10-27","title":"Leave implementation of in-memory events handling to users of the library"},{"location":"adr/20231027_2-leave-implementation-of-synchronous-dispatcher-to-users-of-the-library/#status","text":"Accepted","title":"Status"},{"location":"adr/20231027_2-leave-implementation-of-synchronous-dispatcher-to-users-of-the-library/#context","text":"The library at the early stages had a functionality of dispatching in-memory events. A user of the library could plug-in listeners to particular types of events (also - catch-all listener was supported). When events were published via EventStore with configured listeners, the latter were called synchronously one after another. This happened right after events were persisted. This delivered a very basic mechanism of subscribing to events from e.g. other code modules, similar to e.g. Django signals. However, in-memory events implementation failed to conform to Liskov's Substitute Principle, because the handling and consequences of the errors are different for SQL-based backend and Event Store DB. With SQL-based, everything is wrapped in a transaction. That means if the one of listeners fails, then the whole transaction is rolled back. With Event Store DB (and possibly other stores as well) there are no transactions, so failing in listener would not reverted changes made in Event Store. Hence, some time ago we decided to remove that feature and reconsider adding it back when we understand more.","title":"Context"},{"location":"adr/20231027_2-leave-implementation-of-synchronous-dispatcher-to-users-of-the-library/#decision","text":"After consideration, we decided to NOT include this feature in the library and instead suggest users that they implement it on their own if only they need it. For SQL-based storages, one can use SQLAlchemy's events, in particular after_insert to plug their own logic after event is persisted. We shall also consider exposing deserialization, so that users can use event objects instead of their representations for persistence (i.e. SQLAlchemy models). The similar approach could be also used for future backend implementations, including Django.","title":"Decision"},{"location":"adr/20231027_2-leave-implementation-of-synchronous-dispatcher-to-users-of-the-library/#consequences","text":"Our maintenance burden would be lower in expense for documenting the approach with leveraging persistence-layer mechanisms, e.g. SQLAlchemy's events.","title":"Consequences"},{"location":"adr/20231029-in-memory-event-store/","text":"InMemory Event Store Date: 2023-10-29 Status Accepted Context Features of core event store requires integration tests with every backend implementation. In this context first tests were prepared. Decision InMemory event store strategy will be used as default for feature tests. Also, it's a good starting point to experiment with library or start a project postponing infrastructure decisions. Consequences Possible lack of testing backend-specific consequences for features. This will require additional backend-specific tests.","title":"In Memory Event Store"},{"location":"adr/20231029-in-memory-event-store/#inmemory-event-store","text":"Date: 2023-10-29","title":"InMemory Event Store"},{"location":"adr/20231029-in-memory-event-store/#status","text":"Accepted","title":"Status"},{"location":"adr/20231029-in-memory-event-store/#context","text":"Features of core event store requires integration tests with every backend implementation. In this context first tests were prepared.","title":"Context"},{"location":"adr/20231029-in-memory-event-store/#decision","text":"InMemory event store strategy will be used as default for feature tests. Also, it's a good starting point to experiment with library or start a project postponing infrastructure decisions.","title":"Decision"},{"location":"adr/20231029-in-memory-event-store/#consequences","text":"Possible lack of testing backend-specific consequences for features. This will require additional backend-specific tests.","title":"Consequences"},{"location":"adr/20231230-in_transaction_subscription/","text":"InTransaction Subscription Date: 2023-12-30 Status Proposed Context Transactional databases have a chance to simplify projections by projecting events in same transaction where event is created. It has limitations, but it's a great start for most projects, and can be enough in most of them. Decision Transactional backend will have it's own ability to use in transaction subscription. It won't be in main API of EventStore, but in every backend API separately. Events in this subscriptions won't be stored yet, so won't have EventStore position defined. Consequences Migration from in transaction subscription to async subscription will require some additional work from developers to synchronize projections on event position. This probably will need some migration strategy or a new projection from scratch.","title":"In Transaction Subscription"},{"location":"adr/20231230-in_transaction_subscription/#intransaction-subscription","text":"Date: 2023-12-30","title":"InTransaction Subscription"},{"location":"adr/20231230-in_transaction_subscription/#status","text":"Proposed","title":"Status"},{"location":"adr/20231230-in_transaction_subscription/#context","text":"Transactional databases have a chance to simplify projections by projecting events in same transaction where event is created. It has limitations, but it's a great start for most projects, and can be enough in most of them.","title":"Context"},{"location":"adr/20231230-in_transaction_subscription/#decision","text":"Transactional backend will have it's own ability to use in transaction subscription. It won't be in main API of EventStore, but in every backend API separately. Events in this subscriptions won't be stored yet, so won't have EventStore position defined.","title":"Decision"},{"location":"adr/20231230-in_transaction_subscription/#consequences","text":"Migration from in transaction subscription to async subscription will require some additional work from developers to synchronize projections on event position. This probably will need some migration strategy or a new projection from scratch.","title":"Consequences"},{"location":"adr/20240726-dependencies-and-distribution/","text":"Dependencies and distribution Date: 2024-07-26 Status Accepted Context Event Sourcery supports multiple backends, such as SQLAlchemy, EventStoreDB. More adapters will appear in the future. Each integration has its own dependencies, for example EventStoreDB relies on esdbclient which in turn depends on grpcio . When someone wants to use the library for e.g. Django or SQLAlchemy, they don't need aforementioned packages. The approach to packaging and releases should make it possible for library clients to install only packages that are required in their setup. Considered approaches Single package with extras In this approach, we release a single python_event_sourcery package that includes all modules i.e. \"core\" + adapters for esdb, django and sqlalchemy (+ possibly more in the future). To control optional dependencies, we'll use optional dependencies + \"extras\" feature which is widely supported in popular Python packaging tools. We already have it implemented for Django - when someone installs our library using \"pip install python-event-sourcery[django]\", we'll also install Django framework. NOTE Real-world Django projects would already have Django installed FIRST and they'll be adding our library LATER. However, it still makes sense to have Django as an optional dependency and extra to cross-validate the supported version. If someone has Django installed and its version frozen, but it's not compatible with our library, they'll see an error. Extras propositions for the current project structure: - sqlalchemy - SQLAlchemy adapter - esdb - EventStoreDB adapter For example, when someone wants to use our library with SQLAlchemy, they'll have to install it using pip install python-event-sourcery[sqlalchemy] . Downside: maintainers of the library are aware of some in-house solutions that doesn't support \"extras\". We acknowledge the existence of such a niche, but we won't be treating is a blocker. Multiple packages In this approach, python-event-sourcery would be split into multiple separately installable packages, i.e.: python_event_sourcery_core - core functionality python_event_sourcery_sqlalchemy - SQLAlchemy adapter, depending on python_event_sourcery_core python_event_sourcery_esdb - EventStoreDB adapter, depending on python_event_sourcery_core python_event_sourcery_django - Django adapter, depending on python_event_sourcery_core We could still have them all in a single repository and maintain a single test suite, but build & release process would be uploading separate packages to PyPI. This would require a work to create multiple pyproject.toml files, one per each package and put there the dependency on the core package. Downside: in the current setup we have only support for different backends. In the future, when e.g. support for Kafka or RabbitMQ will be added, installation would become more cumbersome. For example, if someone would be using SQLAlchemy and Kafka, they would have to install two packages. With extras, it'll still be a single package but with to extras. Decision We decided to go with the \"Single package with extras\" approach. Consequences As a consequence, we'll have to put work into maintaining the extras feature in the pyproject.toml file.","title":"Dependencies And Distribution"},{"location":"adr/20240726-dependencies-and-distribution/#dependencies-and-distribution","text":"Date: 2024-07-26","title":"Dependencies and distribution"},{"location":"adr/20240726-dependencies-and-distribution/#status","text":"Accepted","title":"Status"},{"location":"adr/20240726-dependencies-and-distribution/#context","text":"Event Sourcery supports multiple backends, such as SQLAlchemy, EventStoreDB. More adapters will appear in the future. Each integration has its own dependencies, for example EventStoreDB relies on esdbclient which in turn depends on grpcio . When someone wants to use the library for e.g. Django or SQLAlchemy, they don't need aforementioned packages. The approach to packaging and releases should make it possible for library clients to install only packages that are required in their setup.","title":"Context"},{"location":"adr/20240726-dependencies-and-distribution/#considered-approaches","text":"","title":"Considered approaches"},{"location":"adr/20240726-dependencies-and-distribution/#single-package-with-extras","text":"In this approach, we release a single python_event_sourcery package that includes all modules i.e. \"core\" + adapters for esdb, django and sqlalchemy (+ possibly more in the future). To control optional dependencies, we'll use optional dependencies + \"extras\" feature which is widely supported in popular Python packaging tools. We already have it implemented for Django - when someone installs our library using \"pip install python-event-sourcery[django]\", we'll also install Django framework. NOTE Real-world Django projects would already have Django installed FIRST and they'll be adding our library LATER. However, it still makes sense to have Django as an optional dependency and extra to cross-validate the supported version. If someone has Django installed and its version frozen, but it's not compatible with our library, they'll see an error. Extras propositions for the current project structure: - sqlalchemy - SQLAlchemy adapter - esdb - EventStoreDB adapter For example, when someone wants to use our library with SQLAlchemy, they'll have to install it using pip install python-event-sourcery[sqlalchemy] . Downside: maintainers of the library are aware of some in-house solutions that doesn't support \"extras\". We acknowledge the existence of such a niche, but we won't be treating is a blocker.","title":"Single package with extras"},{"location":"adr/20240726-dependencies-and-distribution/#multiple-packages","text":"In this approach, python-event-sourcery would be split into multiple separately installable packages, i.e.: python_event_sourcery_core - core functionality python_event_sourcery_sqlalchemy - SQLAlchemy adapter, depending on python_event_sourcery_core python_event_sourcery_esdb - EventStoreDB adapter, depending on python_event_sourcery_core python_event_sourcery_django - Django adapter, depending on python_event_sourcery_core We could still have them all in a single repository and maintain a single test suite, but build & release process would be uploading separate packages to PyPI. This would require a work to create multiple pyproject.toml files, one per each package and put there the dependency on the core package. Downside: in the current setup we have only support for different backends. In the future, when e.g. support for Kafka or RabbitMQ will be added, installation would become more cumbersome. For example, if someone would be using SQLAlchemy and Kafka, they would have to install two packages. With extras, it'll still be a single package but with to extras.","title":"Multiple packages"},{"location":"adr/20240726-dependencies-and-distribution/#decision","text":"We decided to go with the \"Single package with extras\" approach.","title":"Decision"},{"location":"adr/20240726-dependencies-and-distribution/#consequences","text":"As a consequence, we'll have to put work into maintaining the extras feature in the pyproject.toml file.","title":"Consequences"},{"location":"adr/20241007-multitenancy/","text":"Multitenancy Date: 2024-10-07 Status Proposed Context One of the missing features for v1.0 is support for multitenancy, i.e. data isolation for separate customers (tenants) inside a single application. The basic rule is that data from one tenant must not be visible for other tenant. We also need to make sure library would still operate in tenant-less context. For example, one may define a subscription that builds read model. In such a context, we expect all data to be available. Also, some applications may wish to maintain tenant-less, \"global\" data. There are multiple approaches to multitenancy: - row-level multitenancy - each row in a table has a tenant id - schema-level multitenancy - each tenant has its own schema - database-level multitenancy - each tenant has its own database. In the library, we'll natively support approach similar to row-level multitenancy. Other approaches will also be possible to implement with the library, but for now they remain of scope for this ADR or scope of row-level implementation. The blocker is Github issue #57 - Make database tables configurable . Once it's done, one will be able to create separate instances of EventStore , each connecting to other schema or database. Of course this would require e.g. doing schema migration for each tenant schema or database, but if this is a level of isolation required, it means it would have to be necessary anyway. The library itself has nothing to do with it, but should not prevent this pattern from applying. Since all events are organized into streams, the problem can be reduced to stream visibility. Stream visible for tenant A should have \"A\" associated. Conversely, \"global\" stream should have no tenant or default tenant associated. Decisions to make: - do we allow streams to coexist with the same name and ID in different tenants, as well as tenant-less context? - what type of column should be used for tenant id? - do we allow for empty (null) values for tenant id? Decision Streams coexistence with the same name and ID in different tenants To guarantee full data separation we'll allow streams with the same ID and/or name exist in any tenant, as well as in tenant-less context. For example, if we have 10 tenants, there might be 11 streams with the same ID or name. This is to avoid awkward API behaviour when trying to load a stream by id when the stream exists but in the current context we have no access to it. In an ideal world with full data separation, we'd return something like \"stream not found\" but with globally unique Stream IDs we should deny access, thus give away the fact that stream exists. If we'd returned \"stream not found\", we would suggest that one can create a stream with the same ID. When they would attempt to do so, we'd have to handle uniqueness violation, still giving away the fact that stream exists in some other context. This is a security risk for the library users and may be abused by attacker. We'd rather be on the safe side and make it impossible for users of the library to introduce vulnerabilities into their software. On the brightside, this makes exposing stream ids in URLs or other places kinda ok. UUIDs are still poor from UX perspective, but at least data separation is still guaranteed. Stream tenant mode access attempt by response null tenant-less by id yes null tenant-less by name yes null as tenant1 by id no null as tenant1 by name no 1 tenant-less by id no 1 tenant-less by name no 1 as tenant1 by id yes 1 as tenant1 by name yes 1 as tenant2 by id no 1 as tenant2 by name no Type of column for tenant id In the past we were wondering whether we should use integer or UUID type for tenant id. We acknowledged the fact that UUID is actually a superset of integer type in many databases, so we could go with UUID. However, this is not the type used in other solutions. MartenDB uses varchar column for tenant_id. Varchar is definitely less performant than integer or UUID, however this shouldn't be a blocker. We'll add an index to the column because it will be used in all queries. Value for tenant id outside any tenant In MartenDB, if some documents in are inserted in a tenant-less context they get default value of *DEFAULT* . In our case we'll use similar value - *default* . This should be safer to use in wider range of databases. In PostgreSQL one could use partial indexes and workaround nulls in the column, but AFAIR in other popular SQL databases a workaround is needed, which would make the whole thing more complex. Consequences Implementing row-level multitenancy in the library entails adding a column \"tenant_id\" of type varchar to the streams table. This column will be indexed. Since many streams can co-exist with the same ID or name, we have to start passing tenant_id to any library code that is responsible handling streams, e.g. in projections. This is quite huge change in the library, yet is required to get complete data separation and allows to build the best API for the users of the library. Because in all contexts we'll always have a tenant (whether it's *default* or a user-defined one) associated with a stream and there might be multiple streams with the same ID and name, EventStore will no longer be able to unambiguously load a stream by ID or name. Outbox and subscriptions will work \"globally\" simply by getting tenant_id along with the stream id. For now, we don't see a need for an interface to iterate over all streams in all tenants, e.g. getting all streams with the same ID or name. Should such a need arises, we'll consider adding such a thing.","title":"Multitenancy"},{"location":"adr/20241007-multitenancy/#multitenancy","text":"Date: 2024-10-07","title":"Multitenancy"},{"location":"adr/20241007-multitenancy/#status","text":"Proposed","title":"Status"},{"location":"adr/20241007-multitenancy/#context","text":"One of the missing features for v1.0 is support for multitenancy, i.e. data isolation for separate customers (tenants) inside a single application. The basic rule is that data from one tenant must not be visible for other tenant. We also need to make sure library would still operate in tenant-less context. For example, one may define a subscription that builds read model. In such a context, we expect all data to be available. Also, some applications may wish to maintain tenant-less, \"global\" data. There are multiple approaches to multitenancy: - row-level multitenancy - each row in a table has a tenant id - schema-level multitenancy - each tenant has its own schema - database-level multitenancy - each tenant has its own database. In the library, we'll natively support approach similar to row-level multitenancy. Other approaches will also be possible to implement with the library, but for now they remain of scope for this ADR or scope of row-level implementation. The blocker is Github issue #57 - Make database tables configurable . Once it's done, one will be able to create separate instances of EventStore , each connecting to other schema or database. Of course this would require e.g. doing schema migration for each tenant schema or database, but if this is a level of isolation required, it means it would have to be necessary anyway. The library itself has nothing to do with it, but should not prevent this pattern from applying. Since all events are organized into streams, the problem can be reduced to stream visibility. Stream visible for tenant A should have \"A\" associated. Conversely, \"global\" stream should have no tenant or default tenant associated. Decisions to make: - do we allow streams to coexist with the same name and ID in different tenants, as well as tenant-less context? - what type of column should be used for tenant id? - do we allow for empty (null) values for tenant id?","title":"Context"},{"location":"adr/20241007-multitenancy/#decision","text":"","title":"Decision"},{"location":"adr/20241007-multitenancy/#streams-coexistence-with-the-same-name-and-id-in-different-tenants","text":"To guarantee full data separation we'll allow streams with the same ID and/or name exist in any tenant, as well as in tenant-less context. For example, if we have 10 tenants, there might be 11 streams with the same ID or name. This is to avoid awkward API behaviour when trying to load a stream by id when the stream exists but in the current context we have no access to it. In an ideal world with full data separation, we'd return something like \"stream not found\" but with globally unique Stream IDs we should deny access, thus give away the fact that stream exists. If we'd returned \"stream not found\", we would suggest that one can create a stream with the same ID. When they would attempt to do so, we'd have to handle uniqueness violation, still giving away the fact that stream exists in some other context. This is a security risk for the library users and may be abused by attacker. We'd rather be on the safe side and make it impossible for users of the library to introduce vulnerabilities into their software. On the brightside, this makes exposing stream ids in URLs or other places kinda ok. UUIDs are still poor from UX perspective, but at least data separation is still guaranteed. Stream tenant mode access attempt by response null tenant-less by id yes null tenant-less by name yes null as tenant1 by id no null as tenant1 by name no 1 tenant-less by id no 1 tenant-less by name no 1 as tenant1 by id yes 1 as tenant1 by name yes 1 as tenant2 by id no 1 as tenant2 by name no","title":"Streams coexistence with the same name and ID in different tenants"},{"location":"adr/20241007-multitenancy/#type-of-column-for-tenant-id","text":"In the past we were wondering whether we should use integer or UUID type for tenant id. We acknowledged the fact that UUID is actually a superset of integer type in many databases, so we could go with UUID. However, this is not the type used in other solutions. MartenDB uses varchar column for tenant_id. Varchar is definitely less performant than integer or UUID, however this shouldn't be a blocker. We'll add an index to the column because it will be used in all queries.","title":"Type of column for tenant id"},{"location":"adr/20241007-multitenancy/#value-for-tenant-id-outside-any-tenant","text":"In MartenDB, if some documents in are inserted in a tenant-less context they get default value of *DEFAULT* . In our case we'll use similar value - *default* . This should be safer to use in wider range of databases. In PostgreSQL one could use partial indexes and workaround nulls in the column, but AFAIR in other popular SQL databases a workaround is needed, which would make the whole thing more complex.","title":"Value for tenant id outside any tenant"},{"location":"adr/20241007-multitenancy/#consequences","text":"Implementing row-level multitenancy in the library entails adding a column \"tenant_id\" of type varchar to the streams table. This column will be indexed. Since many streams can co-exist with the same ID or name, we have to start passing tenant_id to any library code that is responsible handling streams, e.g. in projections. This is quite huge change in the library, yet is required to get complete data separation and allows to build the best API for the users of the library. Because in all contexts we'll always have a tenant (whether it's *default* or a user-defined one) associated with a stream and there might be multiple streams with the same ID and name, EventStore will no longer be able to unambiguously load a stream by ID or name. Outbox and subscriptions will work \"globally\" simply by getting tenant_id along with the stream id. For now, we don't see a need for an interface to iterate over all streams in all tenants, e.g. getting all streams with the same ID or name. Should such a need arises, we'll consider adding such a thing.","title":"Consequences"},{"location":"adr/20250411-packaging-of-the-project-revised/","text":"Packaging of the project Date: 2025-04-11 Status Accepted Context In ADR 20231027 #1 the decision was made to package the project in the following way: aggregate event_store read_model During documentation writing it became obvious that aggregate name is not accurate because inside this package there are more things than just Aggregate class. Decision aggregate will be renamed to event_sourcing to more accurately reflect its contents.","title":"Packaging Of The Project Revised"},{"location":"adr/20250411-packaging-of-the-project-revised/#packaging-of-the-project","text":"Date: 2025-04-11","title":"Packaging of the project"},{"location":"adr/20250411-packaging-of-the-project-revised/#status","text":"Accepted","title":"Status"},{"location":"adr/20250411-packaging-of-the-project-revised/#context","text":"In ADR 20231027 #1 the decision was made to package the project in the following way: aggregate event_store read_model During documentation writing it became obvious that aggregate name is not accurate because inside this package there are more things than just Aggregate class.","title":"Context"},{"location":"adr/20250411-packaging-of-the-project-revised/#decision","text":"aggregate will be renamed to event_sourcing to more accurately reflect its contents.","title":"Decision"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/","text":"Privacy Data Encryption and Crypto-Shredding Date: 2025-07-29 Status Proposed Context We want to provide a way to protect privacy-sensitive data in events, inspired by crypto-shredding patterns. This includes marking fields as private, encrypting them, and being able to irreversibly remove access (shred) by deleting encryption keys. The design should be simple for users of the library and support masking of shredded data. Decision Core Concepts Marking Encrypted Fields: Encrypted fields are marked using Python's type annotation system: Annotated[str, Encrypted(...)] . Each event can have only one field marked as DataSubject which identifies the privacy subject. The DataSubject field is used by default for all encrypted fields in the event. Optionally, an encrypted field can specify a different subject_field if needed. Masked Value: When data is shredded (encryption key deleted), the field will return a masked value. The masked value is required and must be specified via mask_value parameter. The type of mask_value must match the type of the field being encrypted. Event-Level Encryption: The Encryption class works on entire events, not individual fields. encrypt(event, stream_id) returns a dictionary with encrypted fields. decrypt(event_type, data, stream_id) returns a dictionary with decrypted fields. Code Organization Interfaces in event_store/interfaces.py : KeyStorageStrategy - interface for key management EncryptionStrategy - interface for encryption operations Privacy Markers in event_store/event/dto.py : Field markers and metadata ( Encrypted , DataSubject ) No implementation logic Core Logic in event_store/encryption.py : Encryption class that processes entire events Handles field discovery, subject resolution, and value encryption/decryption Implementations in separate packages: In-memory implementation in core package ( event_store/in_memory.py ) Other implementations in dedicated packages (e.g., event_sourcery_fernet ) Error Handling in event_store/exceptions.py : Privacy-specific exceptions inherit from EventStoreException Clear error hierarchy for different failure modes Integration Privacy features are integrated at serialization level Encryption/decryption happens automatically during event serialization/deserialization Backend configuration through factory methods No changes required to existing event store implementations API Design @dataclass class Encryption : strategy : EncryptionStrategy key_storage : KeyStorageStrategy def encrypt ( self , event : Event , stream_id : StreamId ) -> dict : \"\"\"Encrypt an event and return the data dictionary with encrypted fields.\"\"\" def decrypt ( self , event_type : type [ Event ], data : dict , stream_id : StreamId ) -> dict : \"\"\"Decrypt event data and return the processed data dictionary.\"\"\" def shred ( self , subject_id : str ) -> None : \"\"\"Remove access to data by deleting the encryption key.\"\"\" Example from typing import Annotated from event_sourcery.event_store.event.dto import Event , Encrypted , DataSubject class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str # unencrypted field # Configuration factory = ( SQLAlchemyBackendFactory ( session ) . with_encryption ( key_storage = InMemoryKeyStorage (), strategy = FernetEncryptionStrategy (), ) ) # Usage (automatic through serialization) encryption = Encryption ( strategy = strategy , key_storage = key_storage ) encrypted_data = encryption . encrypt ( user_event , stream_id ) # Returns dict with encrypted fields decrypted_data = encryption . decrypt ( UserRegistered , encrypted_data , stream_id ) # Returns dict with decrypted fields Consequences Users can easily mark and protect privacy data in events Event-level encryption API is simple and intuitive Shredding is fast and irreversible (only requires deleting the key) Public fields remain accessible regardless of shredding Clear separation between interfaces and implementations Easy to add new encryption or storage implementations Consistent with project's modular architecture No changes needed to existing event stores Need to manage encryption keys separately Potential performance impact with many encrypted fields","title":"Privacy Data Encryption And Shredding"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#privacy-data-encryption-and-crypto-shredding","text":"Date: 2025-07-29","title":"Privacy Data Encryption and Crypto-Shredding"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#status","text":"Proposed","title":"Status"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#context","text":"We want to provide a way to protect privacy-sensitive data in events, inspired by crypto-shredding patterns. This includes marking fields as private, encrypting them, and being able to irreversibly remove access (shred) by deleting encryption keys. The design should be simple for users of the library and support masking of shredded data.","title":"Context"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#decision","text":"","title":"Decision"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#core-concepts","text":"Marking Encrypted Fields: Encrypted fields are marked using Python's type annotation system: Annotated[str, Encrypted(...)] . Each event can have only one field marked as DataSubject which identifies the privacy subject. The DataSubject field is used by default for all encrypted fields in the event. Optionally, an encrypted field can specify a different subject_field if needed. Masked Value: When data is shredded (encryption key deleted), the field will return a masked value. The masked value is required and must be specified via mask_value parameter. The type of mask_value must match the type of the field being encrypted. Event-Level Encryption: The Encryption class works on entire events, not individual fields. encrypt(event, stream_id) returns a dictionary with encrypted fields. decrypt(event_type, data, stream_id) returns a dictionary with decrypted fields.","title":"Core Concepts"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#code-organization","text":"Interfaces in event_store/interfaces.py : KeyStorageStrategy - interface for key management EncryptionStrategy - interface for encryption operations Privacy Markers in event_store/event/dto.py : Field markers and metadata ( Encrypted , DataSubject ) No implementation logic Core Logic in event_store/encryption.py : Encryption class that processes entire events Handles field discovery, subject resolution, and value encryption/decryption Implementations in separate packages: In-memory implementation in core package ( event_store/in_memory.py ) Other implementations in dedicated packages (e.g., event_sourcery_fernet ) Error Handling in event_store/exceptions.py : Privacy-specific exceptions inherit from EventStoreException Clear error hierarchy for different failure modes","title":"Code Organization"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#integration","text":"Privacy features are integrated at serialization level Encryption/decryption happens automatically during event serialization/deserialization Backend configuration through factory methods No changes required to existing event store implementations","title":"Integration"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#api-design","text":"@dataclass class Encryption : strategy : EncryptionStrategy key_storage : KeyStorageStrategy def encrypt ( self , event : Event , stream_id : StreamId ) -> dict : \"\"\"Encrypt an event and return the data dictionary with encrypted fields.\"\"\" def decrypt ( self , event_type : type [ Event ], data : dict , stream_id : StreamId ) -> dict : \"\"\"Decrypt event data and return the processed data dictionary.\"\"\" def shred ( self , subject_id : str ) -> None : \"\"\"Remove access to data by deleting the encryption key.\"\"\"","title":"API Design"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#example","text":"from typing import Annotated from event_sourcery.event_store.event.dto import Event , Encrypted , DataSubject class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str # unencrypted field # Configuration factory = ( SQLAlchemyBackendFactory ( session ) . with_encryption ( key_storage = InMemoryKeyStorage (), strategy = FernetEncryptionStrategy (), ) ) # Usage (automatic through serialization) encryption = Encryption ( strategy = strategy , key_storage = key_storage ) encrypted_data = encryption . encrypt ( user_event , stream_id ) # Returns dict with encrypted fields decrypted_data = encryption . decrypt ( UserRegistered , encrypted_data , stream_id ) # Returns dict with decrypted fields","title":"Example"},{"location":"adr/20250729-privacy-data-encryption-and-shredding/#consequences","text":"Users can easily mark and protect privacy data in events Event-level encryption API is simple and intuitive Shredding is fast and irreversible (only requires deleting the key) Public fields remain accessible regardless of shredding Clear separation between interfaces and implementations Easy to add new encryption or storage implementations Consistent with project's modular architecture No changes needed to existing event stores Need to manage encryption keys separately Potential performance impact with many encrypted fields","title":"Consequences"},{"location":"adr/20251004-packaging-of-the-event_store-package/","text":"Packaging of the event_store package Date: 2025-10-04 Status Accepted Context We want to keep the project structure clear and intuitive. Import paths should reflect the functionality provided by each package, regardless from the code evolution. That's why implementation of our core event_store package will be kept as private _event_store subpackage. In root package we will keep public API structure with proxy imports to implementation in _event_store . Proxy imports will be organized in modules reflecting functionality (e.g. encryption , event , types etc.). Decision Implementation of the event_store will be moved into _event_store subpackage. Root package will keep public API structure with proxy imports to implementation in _event_store .","title":"Packaging Of The Event Store Package"},{"location":"adr/20251004-packaging-of-the-event_store-package/#packaging-of-the-event_store-package","text":"Date: 2025-10-04","title":"Packaging of the event_store package"},{"location":"adr/20251004-packaging-of-the-event_store-package/#status","text":"Accepted","title":"Status"},{"location":"adr/20251004-packaging-of-the-event_store-package/#context","text":"We want to keep the project structure clear and intuitive. Import paths should reflect the functionality provided by each package, regardless from the code evolution. That's why implementation of our core event_store package will be kept as private _event_store subpackage. In root package we will keep public API structure with proxy imports to implementation in _event_store . Proxy imports will be organized in modules reflecting functionality (e.g. encryption , event , types etc.).","title":"Context"},{"location":"adr/20251004-packaging-of-the-event_store-package/#decision","text":"Implementation of the event_store will be moved into _event_store subpackage. Root package will keep public API structure with proxy imports to implementation in _event_store .","title":"Decision"},{"location":"concepts/basics/","text":"Basics What is an event? An event is a change of state. It is a piece of information stating a fact with extra information, letting to say what just actually happened and, potentially, react to it. Since events merely state the fact, that something happened, we write them in past tense. Examples Sprint started Payment failed Order cancelled Invoice issued You can stop reading for a moment and think about examples from the projects you're working on. When an event occurs it cannot be denied anymore. It can be only reacted to. Let's say that the payment failed but for some reason it should not. We can not negate the event, but we can always try to e.g. pay with another payment card. This is an example of reacting to an event. In an application that uses events explicitly, they will somehow be represented in the source code. They can be coded as simple data structures: @dataclass ( frozen = True ) class SprintStarted : when_started : datetime when_ends : datetime project_key : ProjectKey How Event Sourcery helps with using events? Event Sourcery provides a simple base class that is currently using Pydantic . It brings all goodies of that library plus provides a few basic fields, like unique identifier of an event or timestamp of event creation. from event_sourcery import Event class SprintStarted ( Event ): when_started : datetime when_ends : datetime project_key : ProjectKey event = SprintStarted ( when_started = datetime . now (), when_ends = datetime . now () + timedelta ( days = 7 ), project_key = \"PRO\" , ) # SprintStarted( # uuid=UUID('48c3ecb1-2d58-4b99-b964-2fb9ccfba601'), # created_at=datetime.datetime(2022, 8, 7, 16, 56, 35, 719248), # when_started=datetime.datetime(2022, 8, 7, 18, 56, 35, 719177), # when_ends=datetime.datetime(2022, 8, 14, 18, 56, 35, 719184), # project_key='PRO' # ) Other baked-in feature includes tracing ids - correlation id and causation id, useful for tracking flows of events in the system. What is Event-Driven Architecture? Systems that use Event-Driven Architecture (or EDA in short) use events for connecting its parts. One part of the system publishes an event, letting all other interested parts know that something significant happened. In turn, these parts may trigger some action on their side. This pattern is used with microservices publishing events via brokers, such as RabbitMQ or Apache Kafka . Event-Driven Distributed App Integration with events is a pattern that makes publishing part of the system ignorant of other parts, so there's loose coupling between them. For example, Order Service does not have to know that Payment Service or Invoice Service even exists. Another benefit from asynchronous, event-driven architecture is that even if something is temporarily wrong with Payment Service , system can still operate. Broker will receive messages and once Payment Service is back online, the process can continue. The same integration method can be used in much simpler environments, e.g. monorepo applications. One doesn't need a broker right away. How Event Sourcery helps with that? Event Sourcery provides implementation of EventStore and so-called Outbox . The former is a class to provide persistence of events while the Outbox makes sure they will be published eventually, even if there's something with the broker. First thing is to ask EventStore to not only save the event, but also to put it in the Outbox . You can do it using publish method instead of append . an_event = SomeEvent ( first_name = \"John\" ) # publish additionally puts an event in outbox event_store . append ( stream_id = uuid4 (), events = [ an_event ]) Then, one has to implement publishing mechanism - e.g. publishing to Kafka or RabbitMQ, depending on your stack. Event Sourcery does not provide this out of the box. What it does provide is Outbox class that accepts publisher argument to send the message. from event_sourcery import get_outbox outbox = get_outbox ( session = session , # SQLAlchemy session # a function that receives published event and does something with it publisher = lambda event : print ( event ), ) The last step is to run Outbox in a separate process, e.g. separate docker container in an infinite loop: while True : try : outbox . run_once () session . commit () except : session . rollback () logger . exception ( \"Outbox processing failed\" ) time . sleep ( 5 ) Regarding the simpler variant - that is monorepo app, running in a single process - Event Sourcery has a system of synchronous subscriptions. Simply saying, we can set up callbacks that will be triggered once a certain event is saved. store = get_event_store ( session = session , subscriptions = { UserRegistered : [ lambda event : send_email ( event . email )], }, ) stream_id = uuid4 () event = UserRegistered ( email = \"test@example.com\" ) store . append ( stream_id = stream_id , events = [ event ]) # here, callback runs and email gets sent! What is Event Sourcing? Recall any entity/model/being from a piece of software you recently worked on. Let's consider e-commerce Order . It might hold current status (new, confirmed, shipped, etc) and summaries \u2013 total price, shipping and taxes. Naturally, Order does not exist on its own. We usually wire it with another entity, OrderLine , that refers to a single product ordered with a quantity information. This structure could be represented in a relational database in a following way: orders id status total_price 1 new 169.99 order_lines id order_id product_id quantity 1 1 512 1 2 1 614 3 By storing data this way we can always cheaply get CURRENT state of our Order . We store a dump of serialized object after latest changes. Changing anything, for example switching status from new to shipped causes data overwrite. We irreversibly lose old state. What if we need to track all changes? Let\u2019s see how that fits in another database table: order_history id order_id event_name datetime data 1 1 OrderCreated 2018-01-20 18:33:12 2 1 LineAdded 2018-01-20 18:33:44 {\"product_id\": 512, \"quantity\": 1} 3 1 StatusChanged 2018-01-20 18:42:59 {\"status\": \"confirmed\"} Such a representation enables us to confidently say what was changed and when. But this order_history table plays only a second fiddle. It is merely an extra record of Order , added just to fulfill some business requirement. We still reach to original orders table when we want to know exact state of any Order in all other scenarios. However, notice that order_history is as good as orders table when we have to get current Order state. How so? We just have to fetch all entries for given Order and replay them from the start. In the end we\u2019ll get exactly the same information that is saved in orders table. So do we even need orders and orders_lines table as a source of truth...? Event Sourcing proposes we don't. We can still keep them around to optimize reading data for UI, but no longer have to rely on it in any situation that would actually change Order . To sum up, Event Sourcing comes down to: Keeping your business objects (called Aggregate ) as a series of replayable events. This is often called an event stream. Never deleting any events from a system, only appending new ones Using events as the only reliable way of telling in what state a given aggregate is If you need to query data or present them in a table-like format, keep a copy of them in a denormalized format. This is called projection Designing your Aggregate to protect certain vital business invariants, such as Order encapsulates costs summary. A good rule of thumb is to keep Aggregate as small as possible How Event Sourcery helps with that? Event Sourcery provides a base class for an Aggregate and repository implementation that makes it much easy to create or read/change Aggregate . class LightSwitch ( Aggregate ): \"\"\"A simple aggregate that models a light switch.\"\"\" class AlreadyTurnedOn ( Exception ): pass class AlreadyTurnedOff ( Exception ): pass def __init__ ( self , past_events : list [ Event ], changes : list [ Event ], stream_id : StreamId ) -> None : # init any state you need in aggregate class to check conditions self . _shines = False # required for base class super () . __init__ ( past_events , changes , stream_id ) def _apply ( self , event : Event ) -> None : # each aggregate need an _apply method to parse events match event : case TurnedOn () as event : self . _shines = True case TurnedOff () as event : self . _shines = False def turn_on ( self ) -> None : # this is one of command methods # we can rejest it (i.e. raise an exception) # if current state does not allow this to proceed # e.g. light is already on if self . _shines : raise LightSwitch . AlreadyTurnedOn self . _event ( TurnedOn ) def turn_off ( self ) -> None : if not self . _shines : raise LightSwitch . AlreadyTurnedOff self . _event ( TurnedOff ) To create a Repository tailored for a particular Aggregate class, we need that class and EventStore instance: repository = Repository [ LightSwitch ]( event_store , LightSwitch ) A Repository exposes method to create a new instance of Aggregate : stream_id = uuid4 () with repository . new ( stream_id = stream_id ) as switch : switch . turn_on () ...or to work with existing Aggregate , making sure changes are saved at the end: with repository . aggregate ( stream_id = stream_id ) as switch_second_incarnation : try : switch_second_incarnation . turn_on () except LightSwitch . AlreadyTurnedOn : # o mon Dieu, I made a mistake! switch_second_incarnation . turn_off () A Repository is a thin wrapper over Event Store. One can also write Aggregates even without using our base class and use EventStore directly!","title":"Basics"},{"location":"concepts/basics/#basics","text":"","title":"Basics"},{"location":"concepts/basics/#what-is-an-event","text":"An event is a change of state. It is a piece of information stating a fact with extra information, letting to say what just actually happened and, potentially, react to it. Since events merely state the fact, that something happened, we write them in past tense. Examples Sprint started Payment failed Order cancelled Invoice issued You can stop reading for a moment and think about examples from the projects you're working on. When an event occurs it cannot be denied anymore. It can be only reacted to. Let's say that the payment failed but for some reason it should not. We can not negate the event, but we can always try to e.g. pay with another payment card. This is an example of reacting to an event. In an application that uses events explicitly, they will somehow be represented in the source code. They can be coded as simple data structures: @dataclass ( frozen = True ) class SprintStarted : when_started : datetime when_ends : datetime project_key : ProjectKey","title":"What is an event?"},{"location":"concepts/basics/#how-event-sourcery-helps-with-using-events","text":"Event Sourcery provides a simple base class that is currently using Pydantic . It brings all goodies of that library plus provides a few basic fields, like unique identifier of an event or timestamp of event creation. from event_sourcery import Event class SprintStarted ( Event ): when_started : datetime when_ends : datetime project_key : ProjectKey event = SprintStarted ( when_started = datetime . now (), when_ends = datetime . now () + timedelta ( days = 7 ), project_key = \"PRO\" , ) # SprintStarted( # uuid=UUID('48c3ecb1-2d58-4b99-b964-2fb9ccfba601'), # created_at=datetime.datetime(2022, 8, 7, 16, 56, 35, 719248), # when_started=datetime.datetime(2022, 8, 7, 18, 56, 35, 719177), # when_ends=datetime.datetime(2022, 8, 14, 18, 56, 35, 719184), # project_key='PRO' # ) Other baked-in feature includes tracing ids - correlation id and causation id, useful for tracking flows of events in the system.","title":"How Event Sourcery helps with using events?"},{"location":"concepts/basics/#what-is-event-driven-architecture","text":"Systems that use Event-Driven Architecture (or EDA in short) use events for connecting its parts. One part of the system publishes an event, letting all other interested parts know that something significant happened. In turn, these parts may trigger some action on their side. This pattern is used with microservices publishing events via brokers, such as RabbitMQ or Apache Kafka . Event-Driven Distributed App Integration with events is a pattern that makes publishing part of the system ignorant of other parts, so there's loose coupling between them. For example, Order Service does not have to know that Payment Service or Invoice Service even exists. Another benefit from asynchronous, event-driven architecture is that even if something is temporarily wrong with Payment Service , system can still operate. Broker will receive messages and once Payment Service is back online, the process can continue. The same integration method can be used in much simpler environments, e.g. monorepo applications. One doesn't need a broker right away.","title":"What is Event-Driven Architecture?"},{"location":"concepts/basics/#how-event-sourcery-helps-with-that","text":"Event Sourcery provides implementation of EventStore and so-called Outbox . The former is a class to provide persistence of events while the Outbox makes sure they will be published eventually, even if there's something with the broker. First thing is to ask EventStore to not only save the event, but also to put it in the Outbox . You can do it using publish method instead of append . an_event = SomeEvent ( first_name = \"John\" ) # publish additionally puts an event in outbox event_store . append ( stream_id = uuid4 (), events = [ an_event ]) Then, one has to implement publishing mechanism - e.g. publishing to Kafka or RabbitMQ, depending on your stack. Event Sourcery does not provide this out of the box. What it does provide is Outbox class that accepts publisher argument to send the message. from event_sourcery import get_outbox outbox = get_outbox ( session = session , # SQLAlchemy session # a function that receives published event and does something with it publisher = lambda event : print ( event ), ) The last step is to run Outbox in a separate process, e.g. separate docker container in an infinite loop: while True : try : outbox . run_once () session . commit () except : session . rollback () logger . exception ( \"Outbox processing failed\" ) time . sleep ( 5 ) Regarding the simpler variant - that is monorepo app, running in a single process - Event Sourcery has a system of synchronous subscriptions. Simply saying, we can set up callbacks that will be triggered once a certain event is saved. store = get_event_store ( session = session , subscriptions = { UserRegistered : [ lambda event : send_email ( event . email )], }, ) stream_id = uuid4 () event = UserRegistered ( email = \"test@example.com\" ) store . append ( stream_id = stream_id , events = [ event ]) # here, callback runs and email gets sent!","title":"How Event Sourcery helps with that?"},{"location":"concepts/basics/#what-is-event-sourcing","text":"Recall any entity/model/being from a piece of software you recently worked on. Let's consider e-commerce Order . It might hold current status (new, confirmed, shipped, etc) and summaries \u2013 total price, shipping and taxes. Naturally, Order does not exist on its own. We usually wire it with another entity, OrderLine , that refers to a single product ordered with a quantity information. This structure could be represented in a relational database in a following way: orders id status total_price 1 new 169.99 order_lines id order_id product_id quantity 1 1 512 1 2 1 614 3 By storing data this way we can always cheaply get CURRENT state of our Order . We store a dump of serialized object after latest changes. Changing anything, for example switching status from new to shipped causes data overwrite. We irreversibly lose old state. What if we need to track all changes? Let\u2019s see how that fits in another database table: order_history id order_id event_name datetime data 1 1 OrderCreated 2018-01-20 18:33:12 2 1 LineAdded 2018-01-20 18:33:44 {\"product_id\": 512, \"quantity\": 1} 3 1 StatusChanged 2018-01-20 18:42:59 {\"status\": \"confirmed\"} Such a representation enables us to confidently say what was changed and when. But this order_history table plays only a second fiddle. It is merely an extra record of Order , added just to fulfill some business requirement. We still reach to original orders table when we want to know exact state of any Order in all other scenarios. However, notice that order_history is as good as orders table when we have to get current Order state. How so? We just have to fetch all entries for given Order and replay them from the start. In the end we\u2019ll get exactly the same information that is saved in orders table. So do we even need orders and orders_lines table as a source of truth...? Event Sourcing proposes we don't. We can still keep them around to optimize reading data for UI, but no longer have to rely on it in any situation that would actually change Order . To sum up, Event Sourcing comes down to: Keeping your business objects (called Aggregate ) as a series of replayable events. This is often called an event stream. Never deleting any events from a system, only appending new ones Using events as the only reliable way of telling in what state a given aggregate is If you need to query data or present them in a table-like format, keep a copy of them in a denormalized format. This is called projection Designing your Aggregate to protect certain vital business invariants, such as Order encapsulates costs summary. A good rule of thumb is to keep Aggregate as small as possible","title":"What is Event Sourcing?"},{"location":"concepts/basics/#how-event-sourcery-helps-with-that_1","text":"Event Sourcery provides a base class for an Aggregate and repository implementation that makes it much easy to create or read/change Aggregate . class LightSwitch ( Aggregate ): \"\"\"A simple aggregate that models a light switch.\"\"\" class AlreadyTurnedOn ( Exception ): pass class AlreadyTurnedOff ( Exception ): pass def __init__ ( self , past_events : list [ Event ], changes : list [ Event ], stream_id : StreamId ) -> None : # init any state you need in aggregate class to check conditions self . _shines = False # required for base class super () . __init__ ( past_events , changes , stream_id ) def _apply ( self , event : Event ) -> None : # each aggregate need an _apply method to parse events match event : case TurnedOn () as event : self . _shines = True case TurnedOff () as event : self . _shines = False def turn_on ( self ) -> None : # this is one of command methods # we can rejest it (i.e. raise an exception) # if current state does not allow this to proceed # e.g. light is already on if self . _shines : raise LightSwitch . AlreadyTurnedOn self . _event ( TurnedOn ) def turn_off ( self ) -> None : if not self . _shines : raise LightSwitch . AlreadyTurnedOff self . _event ( TurnedOff ) To create a Repository tailored for a particular Aggregate class, we need that class and EventStore instance: repository = Repository [ LightSwitch ]( event_store , LightSwitch ) A Repository exposes method to create a new instance of Aggregate : stream_id = uuid4 () with repository . new ( stream_id = stream_id ) as switch : switch . turn_on () ...or to work with existing Aggregate , making sure changes are saved at the end: with repository . aggregate ( stream_id = stream_id ) as switch_second_incarnation : try : switch_second_incarnation . turn_on () except LightSwitch . AlreadyTurnedOn : # o mon Dieu, I made a mistake! switch_second_incarnation . turn_off () A Repository is a thin wrapper over Event Store. One can also write Aggregates even without using our base class and use EventStore directly!","title":"How Event Sourcery helps with that?"},{"location":"recipes/data_privacy/","text":"\ud83d\udd12 Data Privacy: Crypto-Shredding Introduction Crypto-shredding is a technique for protecting privacy-sensitive data in event-sourced systems. It allows you to irreversibly remove access to encrypted data by deleting encryption keys, ensuring compliance with regulations like GDPR (\"right to be forgotten\"). This approach is especially useful in event sourcing, where data is immutable and cannot be physically deleted from the event log. Our framework makes it easy to mark fields for encryption and shredding using Python type annotations. Once a subject\u2019s data needs to be removed, you can shred their encryption keys, and all encrypted fields will be masked automatically. Use-cases GDPR Compliance: Instantly fulfill user requests to erase personal data by shredding their encryption keys. Audit & Compliance: Demonstrate that sensitive data is irreversibly masked, even in backups and logs. Access Control: Limit access to personal data after a user relationship ends, without deleting events. Testing & Debugging: Mask sensitive fields in test environments or logs to prevent accidental leaks. Quickstart Crypto-shredding lets you irreversibly remove access to privacy-sensitive data in events by deleting encryption keys. Mark fields for encryption and shredding using Python type annotations: from typing import Annotated from event_sourcery.event_store.event import Event , Encrypted , DataSubject class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str # unencrypted field Note: mask_value must match the type of the field. Usage Configure backend with encryption: factory = ( SQLAlchemyBackendFactory ( session ) . with_encryption ( key_storage = InMemoryKeyStorage (), strategy = FernetEncryptionStrategy (), ) ) # Usage (automatic through serialization) encryption = Encryption ( strategy = strategy , key_storage = key_storage ) encrypted_data = encryption . encrypt ( user_event , stream_id ) # Returns dict with encrypted fields decrypted_data = encryption . decrypt ( UserRegistered , encrypted_data , stream_id ) # Returns dict with decrypted fields Validation: DataSubject Required Every event with Encrypted fields must have exactly one field marked as DataSubject . If you forget to add it, you'll get a clear error during class initialization or when appending to a stream. Example Traceback class NoSubjectEvent ( Event ): encrypted_text : Annotated [ str , Encrypted ( mask_value = \"[TEXT_REDACTED]\" )] plain : str = \"plain\" # Raises when appending to a stream: # event_sourcery.event_store.exceptions.NoSubjectIdFound: No subject id found for event 'NoSubjectEvent' in stream <StreamId> How to fix: Add a field with DataSubject annotation: class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str Custom subject field You can specify a different subject for an encrypted field: class CustomSubjectEvent ( Event ): subject_id : Annotated [ str , DataSubject ] secondary_subject_id : str custom_subject : Annotated [ str , Encrypted ( mask_value = \"[TEXT_REDACTED]\" , subject_field = \"secondary_subject_id\" ), ] Shredding To shred (irreversibly mask) all data for a subject: encryption . shred ( subject_id = \"user-123\" ) # All encrypted fields for this subject will now return their mask_value (e.g. \"[REDACTED]\") How it works Crypto-shredding in this framework is based on three main concepts: Field Markers: Use Python type annotations to mark fields for encryption (with Encrypted ) and to identify the privacy subject (with DataSubject ). Each event must have exactly one DataSubject field. Encrypted fields will be associated with this subject by default. You can specify a different subject for an Encrypted field using the subject_field parameter. Automatic Encryption & Decryption: When events are serialized, fields marked as Encrypted are automatically encrypted using the EncryptionStrategy and EncryptionKeyStorage . When events are deserialized, Encrypted fields are automatically decrypted if the key is available. If the key is shredded, the field will return its mask_value instead of the original data. Shredding: Shredding is performed by deleting the encryption key for a given subject. All Encrypted fields for that subject will be irreversibly masked. Public fields remain accessible and are not affected by shredding. Best Practices Always specify a meaningful mask_value for each Encrypted field. This value will be shown when data is shredded. Ensure every event with Encrypted fields has a DataSubject field. This is required for correct operation and validation. Use custom subject_field only when you need to associate Encrypted data with a different subject than the default. Regularly audit your key storage and shredding logic to ensure compliance with privacy regulations. Test shredding in development to verify that sensitive data is properly masked and cannot be recovered. FAQ What happens if I forget to add a DataSubject field? You will get a NoSubjectIdFound exception when appending the event to a stream. This ensures that every Encrypted field is associated with a privacy subject. See the traceback example above for details. Can I shred only part of the data for a subject? No, shredding deletes the encryption key for the subject, which irreversibly masks all Encrypted fields associated with that subject. If you need more granular control, use separate subjects for different fields. What if I need to change the mask value? You must update the event class definition and re-deploy. The mask value is evaluated at runtime, so no migrations is needed. Just ensure the new mask_value matches the type of the Encrypted field. Is crypto-shredding suitable for all types of data? Crypto-shredding is ideal for privacy-sensitive fields that must be protected or deleted on request. Do not use it for fields that must remain accessible for business or legal reasons. \ud83d\udd0e If you see NoSubjectIdFound in your traceback, check that your event class defines a DataSubject field.","title":"Data privacy"},{"location":"recipes/data_privacy/#data-privacy-crypto-shredding","text":"","title":"\ud83d\udd12 Data Privacy: Crypto-Shredding"},{"location":"recipes/data_privacy/#introduction","text":"Crypto-shredding is a technique for protecting privacy-sensitive data in event-sourced systems. It allows you to irreversibly remove access to encrypted data by deleting encryption keys, ensuring compliance with regulations like GDPR (\"right to be forgotten\"). This approach is especially useful in event sourcing, where data is immutable and cannot be physically deleted from the event log. Our framework makes it easy to mark fields for encryption and shredding using Python type annotations. Once a subject\u2019s data needs to be removed, you can shred their encryption keys, and all encrypted fields will be masked automatically.","title":"Introduction"},{"location":"recipes/data_privacy/#use-cases","text":"GDPR Compliance: Instantly fulfill user requests to erase personal data by shredding their encryption keys. Audit & Compliance: Demonstrate that sensitive data is irreversibly masked, even in backups and logs. Access Control: Limit access to personal data after a user relationship ends, without deleting events. Testing & Debugging: Mask sensitive fields in test environments or logs to prevent accidental leaks.","title":"Use-cases"},{"location":"recipes/data_privacy/#quickstart","text":"Crypto-shredding lets you irreversibly remove access to privacy-sensitive data in events by deleting encryption keys. Mark fields for encryption and shredding using Python type annotations: from typing import Annotated from event_sourcery.event_store.event import Event , Encrypted , DataSubject class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str # unencrypted field Note: mask_value must match the type of the field.","title":"Quickstart"},{"location":"recipes/data_privacy/#usage","text":"Configure backend with encryption: factory = ( SQLAlchemyBackendFactory ( session ) . with_encryption ( key_storage = InMemoryKeyStorage (), strategy = FernetEncryptionStrategy (), ) ) # Usage (automatic through serialization) encryption = Encryption ( strategy = strategy , key_storage = key_storage ) encrypted_data = encryption . encrypt ( user_event , stream_id ) # Returns dict with encrypted fields decrypted_data = encryption . decrypt ( UserRegistered , encrypted_data , stream_id ) # Returns dict with decrypted fields","title":"Usage"},{"location":"recipes/data_privacy/#validation-datasubject-required","text":"Every event with Encrypted fields must have exactly one field marked as DataSubject . If you forget to add it, you'll get a clear error during class initialization or when appending to a stream.","title":"Validation: DataSubject Required"},{"location":"recipes/data_privacy/#example-traceback","text":"class NoSubjectEvent ( Event ): encrypted_text : Annotated [ str , Encrypted ( mask_value = \"[TEXT_REDACTED]\" )] plain : str = \"plain\" # Raises when appending to a stream: # event_sourcery.event_store.exceptions.NoSubjectIdFound: No subject id found for event 'NoSubjectEvent' in stream <StreamId> How to fix: Add a field with DataSubject annotation: class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str","title":"Example Traceback"},{"location":"recipes/data_privacy/#custom-subject-field","text":"You can specify a different subject for an encrypted field: class CustomSubjectEvent ( Event ): subject_id : Annotated [ str , DataSubject ] secondary_subject_id : str custom_subject : Annotated [ str , Encrypted ( mask_value = \"[TEXT_REDACTED]\" , subject_field = \"secondary_subject_id\" ), ]","title":"Custom subject field"},{"location":"recipes/data_privacy/#shredding","text":"To shred (irreversibly mask) all data for a subject: encryption . shred ( subject_id = \"user-123\" ) # All encrypted fields for this subject will now return their mask_value (e.g. \"[REDACTED]\")","title":"Shredding"},{"location":"recipes/data_privacy/#how-it-works","text":"Crypto-shredding in this framework is based on three main concepts: Field Markers: Use Python type annotations to mark fields for encryption (with Encrypted ) and to identify the privacy subject (with DataSubject ). Each event must have exactly one DataSubject field. Encrypted fields will be associated with this subject by default. You can specify a different subject for an Encrypted field using the subject_field parameter. Automatic Encryption & Decryption: When events are serialized, fields marked as Encrypted are automatically encrypted using the EncryptionStrategy and EncryptionKeyStorage . When events are deserialized, Encrypted fields are automatically decrypted if the key is available. If the key is shredded, the field will return its mask_value instead of the original data. Shredding: Shredding is performed by deleting the encryption key for a given subject. All Encrypted fields for that subject will be irreversibly masked. Public fields remain accessible and are not affected by shredding.","title":"How it works"},{"location":"recipes/data_privacy/#best-practices","text":"Always specify a meaningful mask_value for each Encrypted field. This value will be shown when data is shredded. Ensure every event with Encrypted fields has a DataSubject field. This is required for correct operation and validation. Use custom subject_field only when you need to associate Encrypted data with a different subject than the default. Regularly audit your key storage and shredding logic to ensure compliance with privacy regulations. Test shredding in development to verify that sensitive data is properly masked and cannot be recovered.","title":"Best Practices"},{"location":"recipes/data_privacy/#faq","text":"","title":"FAQ"},{"location":"recipes/data_privacy/#what-happens-if-i-forget-to-add-a-datasubject-field","text":"You will get a NoSubjectIdFound exception when appending the event to a stream. This ensures that every Encrypted field is associated with a privacy subject. See the traceback example above for details.","title":"What happens if I forget to add a DataSubject field?"},{"location":"recipes/data_privacy/#can-i-shred-only-part-of-the-data-for-a-subject","text":"No, shredding deletes the encryption key for the subject, which irreversibly masks all Encrypted fields associated with that subject. If you need more granular control, use separate subjects for different fields.","title":"Can I shred only part of the data for a subject?"},{"location":"recipes/data_privacy/#what-if-i-need-to-change-the-mask-value","text":"You must update the event class definition and re-deploy. The mask value is evaluated at runtime, so no migrations is needed. Just ensure the new mask_value matches the type of the Encrypted field.","title":"What if I need to change the mask value?"},{"location":"recipes/data_privacy/#is-crypto-shredding-suitable-for-all-types-of-data","text":"Crypto-shredding is ideal for privacy-sensitive fields that must be protected or deleted on request. Do not use it for fields that must remain accessible for business or legal reasons. \ud83d\udd0e If you see NoSubjectIdFound in your traceback, check that your event class defines a DataSubject field.","title":"Is crypto-shredding suitable for all types of data?"},{"location":"recipes/defining_events/","text":"To define an event, write a class inheriting from Event base class: from event_sourcery.event import Event class InvoicePaid ( Event ): invoice_number : str Base class Event is a pydantic model and so will be every event you define.","title":"Defining events"},{"location":"recipes/event_sourcing/","text":"Building blocks Event Sourcery provides a few building blocks to work with event sourcing. These are Aggregate and Repository base classes. Usage You start from defining your own aggregate inheriting from Aggregate . There are three required attributes that need to be defined: category class-level constant that will be added to all streams from all aggregates of this type __init__ if defined, must not accept any arguments __apply__ method that will change internal state of the aggregate based on the event applied during reading state from the event store from event_sourcery.event import Event from event_sourcery.event_sourcing import Aggregate , Repository class SwitchedOn ( Event ): pass class LightSwitch ( Aggregate ): category = \"light_switch\" # 1 def __init__ ( self ) -> None : # 2 self . _switched_on = False def __apply__ ( self , event : Event ) -> None : # 3 match event : case SwitchedOn (): self . _switched_on = True case _ : raise NotImplementedError ( f \"Unexpected event { type ( event ) } \" ) def switch_on ( self ) -> None : if self . _switched_on : return # no op self . _emit ( SwitchedOn ()) To work with aggregate, you need to create repository. You need an instance of EventStore to do so: repository = Repository [ LightSwitch ]( backend . event_store ) From now on, regardless if you want to work with a given aggregate for the first time or load existing one, you should use repository.aggregate context manager: from event_sourcery import StreamUUID stream_id = StreamUUID ( name = \"light_switch/1\" ) with repository . aggregate ( stream_id , LightSwitch ()) as light_switch : light_switch . switch_on () light_switch . switch_on ()","title":"Event Sourcing"},{"location":"recipes/event_sourcing/#building-blocks","text":"Event Sourcery provides a few building blocks to work with event sourcing. These are Aggregate and Repository base classes.","title":"Building blocks"},{"location":"recipes/event_sourcing/#usage","text":"You start from defining your own aggregate inheriting from Aggregate . There are three required attributes that need to be defined: category class-level constant that will be added to all streams from all aggregates of this type __init__ if defined, must not accept any arguments __apply__ method that will change internal state of the aggregate based on the event applied during reading state from the event store from event_sourcery.event import Event from event_sourcery.event_sourcing import Aggregate , Repository class SwitchedOn ( Event ): pass class LightSwitch ( Aggregate ): category = \"light_switch\" # 1 def __init__ ( self ) -> None : # 2 self . _switched_on = False def __apply__ ( self , event : Event ) -> None : # 3 match event : case SwitchedOn (): self . _switched_on = True case _ : raise NotImplementedError ( f \"Unexpected event { type ( event ) } \" ) def switch_on ( self ) -> None : if self . _switched_on : return # no op self . _emit ( SwitchedOn ()) To work with aggregate, you need to create repository. You need an instance of EventStore to do so: repository = Repository [ LightSwitch ]( backend . event_store ) From now on, regardless if you want to work with a given aggregate for the first time or load existing one, you should use repository.aggregate context manager: from event_sourcery import StreamUUID stream_id = StreamUUID ( name = \"light_switch/1\" ) with repository . aggregate ( stream_id , LightSwitch ()) as light_switch : light_switch . switch_on () light_switch . switch_on ()","title":"Usage"},{"location":"recipes/integrate/","text":"Event Sourcery supports variety of backends and configurations. Integrating it with your project requires following steps: instantiating a corresponding factory class, depending on your storage optional configuration, like enabling additional features building so-called Backend that exposes features of the library SQLAlchemy KurrentDB (formerly EventStoreDB) Django Event Sourcery defines a few models to keep your events and streams in the database. When working with SQLAlchemy , you'll typically use alembic to manage your migrations. Before alembic can detect Event Sourcery models, you need to register them once via configure_models : from event_sourcery_sqlalchemy import configure_models configure_models ( Base ) # Base is your declarative base class Once our models are registered, migrations generated and executed, you can continue. You need an instance of Session to instantiate SQLAlchemyBackend : from event_sourcery_sqlalchemy import SQLAlchemyBackend session = Session () # SQLAlchemy session backend = SQLAlchemyBackend () . configure ( session ) First, you need an instance of kurrentdbclient.KurrentDBClient that represents a connection to EventStoreDB. Then, you can pass it to KurrentDBBackend : from kurrentdbclient import KurrentDBClient from event_sourcery_kurrentdb import KurrentDBBackend client = KurrentDBClient ( uri = \"kurrentdb://localhost:2113?Tls=false\" ) backend = KurrentDBBackend () . configure ( client ) backend . event_store . load_stream ( StreamId ()) # test if connection works Your first step will be adding \"event_sourcery_django\" to the list of INSTALLED_APPS in your settings. Then you can simply create an instance of DjangoBackend : from event_sourcery_django import DjangoBackend backend = DjangoBackend () This can be done once. Then, you can import backend from other parts of code to use it. From backend you can grab EventStore instance: backend . event_store You can now use EventStore to load events from a stream or append new events.","title":"Integrate with your app"},{"location":"recipes/multitenancy/","text":"Event Sourcery implements multitenancy by adding tenant id to all objects it stores. By default, EventStore works in so-called default context, tenant-less. Switching tenant To switch context to specific tenant, one should call scoped_for_tenant method: scoped_backend = sqlite_in_memory_backend . in_tenant_mode ( \"tenant123\" ) Isolation rules There are three rules regarding isolation: streams and events from default, tenant-less context are not visible in any tenant-aware context streams and events from another tenant are not visible when working with tenant-aware context streams and events from any tenant are not visible when working with tenant-less, default context This table summarises visibility rules: visible? tenant-less tenant A tenant B tenant-less yes no no tenant A no yes no tenant B no no yes Multitenancy in other features Outbox and subscriptions Both Outbox and Subscriptions are meant to be used in a system context, for example to implement a projection of events onto a read model. However, you can always get tenant id when working with them. On any Recorded instance there is an attribute called TenantId . For events that were created in a default, tenant-less context, TenantId has value of DEFAULT_TENANT . Value of this constant should not be relied upon and is considered an implementation details. In all places where you wish to check if an event was created in a default context, you should use this constant: from event_sourcery.backend import DEFAULT_TENANT ... for recorded_event in subscription : if recorded_event is None : break elif recorded_event . tenant_id == DEFAULT_TENANT : print ( f \"Got tenant-less event! { recorded_event } \" ) else : ... Event Sourcing In case of Event Sourcing , whenever you construct a Repository make sure you pass a scoped EventStore instance: scoped_backend = sqlite_in_memory_backend . in_tenant_mode ( \"tenant123\" ) repository = Repository [ ExampleAggregate ]( scoped_backend . event_store )","title":"Multitenancy"},{"location":"recipes/multitenancy/#switching-tenant","text":"To switch context to specific tenant, one should call scoped_for_tenant method: scoped_backend = sqlite_in_memory_backend . in_tenant_mode ( \"tenant123\" )","title":"Switching tenant"},{"location":"recipes/multitenancy/#isolation-rules","text":"There are three rules regarding isolation: streams and events from default, tenant-less context are not visible in any tenant-aware context streams and events from another tenant are not visible when working with tenant-aware context streams and events from any tenant are not visible when working with tenant-less, default context This table summarises visibility rules: visible? tenant-less tenant A tenant B tenant-less yes no no tenant A no yes no tenant B no no yes","title":"Isolation rules"},{"location":"recipes/multitenancy/#multitenancy-in-other-features","text":"","title":"Multitenancy in other features"},{"location":"recipes/multitenancy/#outbox-and-subscriptions","text":"Both Outbox and Subscriptions are meant to be used in a system context, for example to implement a projection of events onto a read model. However, you can always get tenant id when working with them. On any Recorded instance there is an attribute called TenantId . For events that were created in a default, tenant-less context, TenantId has value of DEFAULT_TENANT . Value of this constant should not be relied upon and is considered an implementation details. In all places where you wish to check if an event was created in a default context, you should use this constant: from event_sourcery.backend import DEFAULT_TENANT ... for recorded_event in subscription : if recorded_event is None : break elif recorded_event . tenant_id == DEFAULT_TENANT : print ( f \"Got tenant-less event! { recorded_event } \" ) else : ...","title":"Outbox and subscriptions"},{"location":"recipes/multitenancy/#event-sourcing","text":"In case of Event Sourcing , whenever you construct a Repository make sure you pass a scoped EventStore instance: scoped_backend = sqlite_in_memory_backend . in_tenant_mode ( \"tenant123\" ) repository = Repository [ ExampleAggregate ]( scoped_backend . event_store )","title":"Event Sourcing"},{"location":"recipes/outbox/","text":"About the pattern Outbox is a pattern that ensures a message is reliably sent from the system. It provides at-least-once semantics, making sure that every message that CAN be sent to e.g. RabbitMQ, gets there. Tip Use Outbox when you want to send events to a broker (e.g. RabbitMQ, Kafka) or external system for analytics purposes (e.g. Segment.io). Basic usage Configure Event Sourcery To use outbox, you have to add with_outbox method call on factory while setting up Event Sourcery : backend = ( SQLAlchemyBackend () . configure ( session ) . with_outbox () # enable outbox ) Write publishing function To publish messages, you need to write a little bit of glue code that will actually send a message. We need a publishing function that takes Recorded as the only argument and will do the sending. Take RabbitMQ and pika for example: # setting up connection and queue... connection = pika . BlockingConnection ( pika . ConnectionParameters ( \"localhost\" )) channel = connection . channel () channel . queue_declare ( queue = \"events\" ) def publish ( recorded : Recorded ) -> None : as_json = recorded . wrapped_event . event . model_dump_json () channel . basic_publish ( exchange = \"\" , routing_key = \"events\" , body = as_json ) Run outbox Now you can run outbox: backend . outbox . run ( publisher = publish ) This will loop over events and will try to call publishing function for each one of them. Optionally, you can specify a number of events to be processed in a single run: backend . outbox . run ( publisher = publish , limit = 50 ) By default, outbox.run will try to process 100 events. Transactional outbox If you use any of transactional backends (e.g. SQLAlchemy or Django), then every call to outbox.run should be wrapped with a database transaction. with session . begin (): backend . outbox . run ( publisher = publish ) session . commit () Warning Event Sourcery outbox keeps track of messages sent and attempts left. Without commiting a transaction, same messages will be sent over and over again. Running outbox Normally you'd be running outbox in an infinite loop, in a separate process - just like you'd do with a Celery worker: from time import sleep while True : with session . begin (): backend . outbox . run ( publish = publisher ) session . commit () sleep ( 3 ) # wait between runs to avoid hammering the database Optional filterer Warning This feature and/or its API is provisional and will probably change soon. Sometimes you want only specific events to be sent. You can pass an optional filterer argument to with_outbox . It should be a callable (e.g. a function) that accepts an event instance and returns True if an event should be published. False otherwise. backend = ( SQLAlchemyBackend () . configure ( session ) . with_outbox ( filterer = lambda e : \"InvoicePaid\" in e . name ) ) Handling retries Sending each event will be retried up to 3 times.","title":"Publish events - outbox"},{"location":"recipes/outbox/#about-the-pattern","text":"Outbox is a pattern that ensures a message is reliably sent from the system. It provides at-least-once semantics, making sure that every message that CAN be sent to e.g. RabbitMQ, gets there. Tip Use Outbox when you want to send events to a broker (e.g. RabbitMQ, Kafka) or external system for analytics purposes (e.g. Segment.io).","title":"About the pattern"},{"location":"recipes/outbox/#basic-usage","text":"","title":"Basic usage"},{"location":"recipes/outbox/#configure-event-sourcery","text":"To use outbox, you have to add with_outbox method call on factory while setting up Event Sourcery : backend = ( SQLAlchemyBackend () . configure ( session ) . with_outbox () # enable outbox )","title":"Configure Event Sourcery"},{"location":"recipes/outbox/#write-publishing-function","text":"To publish messages, you need to write a little bit of glue code that will actually send a message. We need a publishing function that takes Recorded as the only argument and will do the sending. Take RabbitMQ and pika for example: # setting up connection and queue... connection = pika . BlockingConnection ( pika . ConnectionParameters ( \"localhost\" )) channel = connection . channel () channel . queue_declare ( queue = \"events\" ) def publish ( recorded : Recorded ) -> None : as_json = recorded . wrapped_event . event . model_dump_json () channel . basic_publish ( exchange = \"\" , routing_key = \"events\" , body = as_json )","title":"Write publishing function"},{"location":"recipes/outbox/#run-outbox","text":"Now you can run outbox: backend . outbox . run ( publisher = publish ) This will loop over events and will try to call publishing function for each one of them. Optionally, you can specify a number of events to be processed in a single run: backend . outbox . run ( publisher = publish , limit = 50 ) By default, outbox.run will try to process 100 events.","title":"Run outbox"},{"location":"recipes/outbox/#transactional-outbox","text":"If you use any of transactional backends (e.g. SQLAlchemy or Django), then every call to outbox.run should be wrapped with a database transaction. with session . begin (): backend . outbox . run ( publisher = publish ) session . commit () Warning Event Sourcery outbox keeps track of messages sent and attempts left. Without commiting a transaction, same messages will be sent over and over again.","title":"Transactional outbox"},{"location":"recipes/outbox/#running-outbox","text":"Normally you'd be running outbox in an infinite loop, in a separate process - just like you'd do with a Celery worker: from time import sleep while True : with session . begin (): backend . outbox . run ( publish = publisher ) session . commit () sleep ( 3 ) # wait between runs to avoid hammering the database","title":"Running outbox"},{"location":"recipes/outbox/#optional-filterer","text":"Warning This feature and/or its API is provisional and will probably change soon. Sometimes you want only specific events to be sent. You can pass an optional filterer argument to with_outbox . It should be a callable (e.g. a function) that accepts an event instance and returns True if an event should be published. False otherwise. backend = ( SQLAlchemyBackend () . configure ( session ) . with_outbox ( filterer = lambda e : \"InvoicePaid\" in e . name ) )","title":"Optional filterer"},{"location":"recipes/outbox/#handling-retries","text":"Sending each event will be retried up to 3 times.","title":"Handling retries"},{"location":"recipes/saving_events/","text":"Basic usage Once you have an EventStore instance after integrating with your application and some events defined , you can persist them: invoice_paid = InvoicePaid ( invoice_number = \"1003\" ) stream_id = StreamId ( name = \"invoices/1003\" ) event_store . append ( invoice_paid , stream_id = stream_id ) Events can be later retrieved by using load_stream method: events = event_store . load_stream ( stream_id ) # [ # WrappedEvent( # event=InvoicePaid(invoice_number='1003'), # version=1, # uuid=UUID('831cd32b-02b9-48d2-a67c-28cf7dbb37fa'), # created_at=datetime.datetime(2025, 3, 16, 10, 3, 2, 138667), # context=Context(correlation_id=None, causation_id=None) # ) # ] load_stream returns a list of WrappedEvent objects. They contain a saved event under .event attribute with its metadata in other attributes. Version control All events within a stream by default have assigned version number. This can be used to detect a situation of concurrent writes to the same stream. You have a choice whether you want to check for versions conflict or not. Explicit versioning There is no need to add an expected version when adding some events to the stream for the first time, i.e. creating the stream: stream_id = StreamId ( name = \"invoices/1111\" ) an_event = InvoicePaid ( invoice_number = \"1111\" ) event_store . append ( an_event , stream_id = stream_id ) # no `expected_version` argument given However, when you add events to the stream for the second and subsequent times, you need to pass the expected version explicitly. Otherwise, appending will fail with an exception: another_event = InvoicePaid ( invoice_number = \"1112\" ) # \ud83d\udc47 this would raise an exception # _event_store.append(another_event, stream_id=stream_id) Hence, it is assumed that you will use the events versioning and protection against concurrent writes. Info This is a deliberate design choice, so you must decide explicitly if you need a concurrent writes protection or not. In a typical flow, you'll first load a stream, perform some logic, then try to append new events. You'll then get the latest version from the last event loaded: present_events = event_store . load_stream ( stream_id ) last_version = present_events [ - 1 ] . version # ... some logic another_event = InvoicePaid ( invoice_number = \"1112\" ) event_store . append ( another_event , stream_id = stream_id , expected_version = last_version ) No versioning In case when you don't need protection against concurrent writes, you can disable versioning. NO_VERSIONING must be used consistently for every append to such a stream. stream_id = StreamId ( name = \"invoices/123\" ) an_event = InvoicePaid ( invoice_number = \"1111\" ) event_store . append ( an_event , stream_id = stream_id , expected_version = NO_VERSIONING ) another_event = InvoicePaid ( invoice_number = \"1112\" ) event_store . append ( another_event , stream_id = stream_id , expected_version = NO_VERSIONING ) Info Once a stream has been created with disabled versioning, you cannot enable it. It is also forbidden the other way around. You can always create a new stream and delete the old one.","title":"Save and read events"},{"location":"recipes/saving_events/#basic-usage","text":"Once you have an EventStore instance after integrating with your application and some events defined , you can persist them: invoice_paid = InvoicePaid ( invoice_number = \"1003\" ) stream_id = StreamId ( name = \"invoices/1003\" ) event_store . append ( invoice_paid , stream_id = stream_id ) Events can be later retrieved by using load_stream method: events = event_store . load_stream ( stream_id ) # [ # WrappedEvent( # event=InvoicePaid(invoice_number='1003'), # version=1, # uuid=UUID('831cd32b-02b9-48d2-a67c-28cf7dbb37fa'), # created_at=datetime.datetime(2025, 3, 16, 10, 3, 2, 138667), # context=Context(correlation_id=None, causation_id=None) # ) # ] load_stream returns a list of WrappedEvent objects. They contain a saved event under .event attribute with its metadata in other attributes.","title":"Basic usage"},{"location":"recipes/saving_events/#version-control","text":"All events within a stream by default have assigned version number. This can be used to detect a situation of concurrent writes to the same stream. You have a choice whether you want to check for versions conflict or not.","title":"Version control"},{"location":"recipes/saving_events/#explicit-versioning","text":"There is no need to add an expected version when adding some events to the stream for the first time, i.e. creating the stream: stream_id = StreamId ( name = \"invoices/1111\" ) an_event = InvoicePaid ( invoice_number = \"1111\" ) event_store . append ( an_event , stream_id = stream_id ) # no `expected_version` argument given However, when you add events to the stream for the second and subsequent times, you need to pass the expected version explicitly. Otherwise, appending will fail with an exception: another_event = InvoicePaid ( invoice_number = \"1112\" ) # \ud83d\udc47 this would raise an exception # _event_store.append(another_event, stream_id=stream_id) Hence, it is assumed that you will use the events versioning and protection against concurrent writes. Info This is a deliberate design choice, so you must decide explicitly if you need a concurrent writes protection or not. In a typical flow, you'll first load a stream, perform some logic, then try to append new events. You'll then get the latest version from the last event loaded: present_events = event_store . load_stream ( stream_id ) last_version = present_events [ - 1 ] . version # ... some logic another_event = InvoicePaid ( invoice_number = \"1112\" ) event_store . append ( another_event , stream_id = stream_id , expected_version = last_version )","title":"Explicit versioning"},{"location":"recipes/saving_events/#no-versioning","text":"In case when you don't need protection against concurrent writes, you can disable versioning. NO_VERSIONING must be used consistently for every append to such a stream. stream_id = StreamId ( name = \"invoices/123\" ) an_event = InvoicePaid ( invoice_number = \"1111\" ) event_store . append ( an_event , stream_id = stream_id , expected_version = NO_VERSIONING ) another_event = InvoicePaid ( invoice_number = \"1112\" ) event_store . append ( another_event , stream_id = stream_id , expected_version = NO_VERSIONING ) Info Once a stream has been created with disabled versioning, you cannot enable it. It is also forbidden the other way around. You can always create a new stream and delete the old one.","title":"No versioning"},{"location":"recipes/snapshots/","text":"Snapshot is a frozen state of the stream in a given point. The point of the snapshot is identified by the version. Snapshots are a way to optimize loading a long stream of events. When a snapshot is present, it is always loaded. Then, any events newer then a snapshots are also loaded, if only they are present. Let's say we have a stream with 100 events: class TemperatureChanged ( Event ): reading_in_celsius : int stream_id = StreamId ( name = \"temperature_sensor/1\" ) events = [ TemperatureChanged ( reading_in_celsius = reading ) for reading in range ( 100 )] event_store . append ( * events , stream_id = stream_id ) loaded_events = event_store . load_stream ( stream_id ) print ( len ( loaded_events )) # 100 We could define a snapshot event that will capture all information relevant for us: class TemperatureSensorSnapshot ( Event ): readings_so_far : int last_reading_in_celsius : int Now we can save a snapshot, using dedicated EventStore method: last_version = loaded_events [ - 1 ] . version last_event = loaded_events [ - 1 ] . event snapshot = TemperatureSensorSnapshot ( readings_so_far = 100 , last_reading_in_celsius = last_event . reading_in_celsius ) wrapped_snapshot = WrappedEvent . wrap ( snapshot , last_version ) event_store . save_snapshot ( stream_id , wrapped_snapshot ) Note that version of the snapshot must equal to the version of the last event. Now when you'll try to load the stream you'll notice the method only returns a single event and this will be our snapshot: loaded_events = event_store . load_stream ( stream_id ) print ( len ( loaded_events )) # 1 assert isinstance ( loaded_events [ - 1 ] . event , TemperatureSensorSnapshot ) Warning Long streams are usually a sign of a poor stream design. Snapshots are an optimization that should be used only for a good reason. Use with caution!","title":"Snapshots"},{"location":"recipes/subscriptions/","text":"Subscriptions allow for asynchronous processing of events in another process. Having Backend object after integrating with your application , grab its .subscriber to start building a subscription. Iterating over events one by one Let's say we want to know about all paid invoices and we have an event for that - InvoicePaid . subscription = ( backend . subscriber . start_from ( 0 ) # read from position 0... . to_events ([ InvoicePaid ]) # ...InvoicePaid events... . build_iter ( timelimit = 1 ) # ... iterate events one by one... # ...and wait up to 1 second for a new event ) subscription is an iterable. Thus, it can be used with for loop: for recorded_event in subscription : if recorded_event is None : # no more events for now break # process the event print ( recorded_event . wrapped_event ) print ( recorded_event . position ) print ( recorded_event . tenant_id ) With every iteration we're getting an instance of Recorded or None if there are no new events available. Note subscription is an infinite iterator. Getting None means that you are 'up-to-date' with Event Store but as soon as a new event is appended, it will be returned in another iteration of the loop. Iterating over events in batches When we have to process a bigger amount of events it makes sense to do it in batches instead of processing events one by one. To do it, we need to slightly alter code responsible for building our subscription . Instead of build_iter we call build_batch : batch_subscription = ( backend . subscriber . start_from ( 0 ) . to_events ([ InvoicePaid ]) . build_batch ( size = 10 , timelimit = 1 ) # try getting 10 events at a time ) Then we can pass subscritpion to for loop: for batch in batch_subscription : # process the batch print ( batch ) if len ( batch ) == 0 : # no more events at the moment break In this example, batch is a list of Recorded . Just like in previous case, subscription is an infinite iterator. It will be returning batches of given size as long as there is enough events. If there are fewer 'new' events available, batch subscription will return whatever it can. For example, if you specify batch size of 10 but get a list with 7 events, it means there were only 7 new events and time limit has passed. When batch subscription catches up with event store, it will be returning empty lists. At least until some new events are saved.","title":"React to events - subscriptions"},{"location":"recipes/subscriptions/#iterating-over-events-one-by-one","text":"Let's say we want to know about all paid invoices and we have an event for that - InvoicePaid . subscription = ( backend . subscriber . start_from ( 0 ) # read from position 0... . to_events ([ InvoicePaid ]) # ...InvoicePaid events... . build_iter ( timelimit = 1 ) # ... iterate events one by one... # ...and wait up to 1 second for a new event ) subscription is an iterable. Thus, it can be used with for loop: for recorded_event in subscription : if recorded_event is None : # no more events for now break # process the event print ( recorded_event . wrapped_event ) print ( recorded_event . position ) print ( recorded_event . tenant_id ) With every iteration we're getting an instance of Recorded or None if there are no new events available. Note subscription is an infinite iterator. Getting None means that you are 'up-to-date' with Event Store but as soon as a new event is appended, it will be returned in another iteration of the loop.","title":"Iterating over events one by one"},{"location":"recipes/subscriptions/#iterating-over-events-in-batches","text":"When we have to process a bigger amount of events it makes sense to do it in batches instead of processing events one by one. To do it, we need to slightly alter code responsible for building our subscription . Instead of build_iter we call build_batch : batch_subscription = ( backend . subscriber . start_from ( 0 ) . to_events ([ InvoicePaid ]) . build_batch ( size = 10 , timelimit = 1 ) # try getting 10 events at a time ) Then we can pass subscritpion to for loop: for batch in batch_subscription : # process the batch print ( batch ) if len ( batch ) == 0 : # no more events at the moment break In this example, batch is a list of Recorded . Just like in previous case, subscription is an infinite iterator. It will be returning batches of given size as long as there is enough events. If there are fewer 'new' events available, batch subscription will return whatever it can. For example, if you specify batch size of 10 but get a list with 7 events, it means there were only 7 new events and time limit has passed. When batch subscription catches up with event store, it will be returning empty lists. At least until some new events are saved.","title":"Iterating over events in batches"},{"location":"reference/backends/django/","text":"DjangoBackend Bases: TransactionalBackend Django integration backend for Event Sourcery. Provides a fully configured TransactionalBackend for Django projects, including event store, outbox, and subscription strategies. Supports configuration via the DjangoConfig class. Source code in event_sourcery_django/__init__.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class DjangoBackend ( TransactionalBackend ): \"\"\" Django integration backend for Event Sourcery. Provides a fully configured TransactionalBackend for Django projects, including event store, outbox, and subscription strategies. Supports configuration via the `DjangoConfig` class. \"\"\" def __init__ ( self ) -> None : from event_sourcery_django.event_store import DjangoStorageStrategy from event_sourcery_django.outbox import DjangoOutboxStorageStrategy from event_sourcery_django.subscription import DjangoSubscriptionStrategy super () . __init__ () self [ DjangoConfig ] = not_configured ( \"Configure backend with `.configure(config)`\" ) self [ StorageStrategy ] = lambda c : DjangoStorageStrategy ( c [ Dispatcher ], c . get ( DjangoOutboxStorageStrategy , None ), ) . scoped_for_tenant ( c [ TenantId ]) self [ SubscriptionStrategy ] = lambda c : DjangoSubscriptionStrategy ( gap_retry_interval = c [ DjangoConfig ] . gap_retry_interval ) def configure ( self , config : DjangoConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for outbox and subscription behavior. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: config (DjangoConfig | None): Optional custom configuration. Returns: Self: The configured backend instance (for chaining). \"\"\" self [ DjangoConfig ] = config or DjangoConfig () return self def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : from event_sourcery_django.outbox import DjangoOutboxStorageStrategy self [ OutboxFiltererStrategy ] = filterer # type: ignore[type-abstract] self [ DjangoOutboxStorageStrategy ] = lambda c : DjangoOutboxStorageStrategy ( c [ OutboxFiltererStrategy ], # type: ignore[type-abstract] c [ DjangoConfig ] . outbox_attempts , ) self [ OutboxStorageStrategy ] = lambda c : c [ DjangoOutboxStorageStrategy ] return self configure ( config = None ) Sets the backend configuration for outbox and subscription behavior. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Parameters: Name Type Description Default config DjangoConfig | None Optional custom configuration. None Returns: Name Type Description Self Self The configured backend instance (for chaining). Source code in event_sourcery_django/__init__.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def configure ( self , config : DjangoConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for outbox and subscription behavior. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: config (DjangoConfig | None): Optional custom configuration. Returns: Self: The configured backend instance (for chaining). \"\"\" self [ DjangoConfig ] = config or DjangoConfig () return self DjangoConfig Bases: BaseModel Configuration for DjangoBackend event store integration. Attributes: Name Type Description outbox_attempts PositiveInt Maximum number of outbox delivery attempts per event. gap_retry_interval timedelta Time to wait before retrying a subscription gap. If the subscription detects a gap in event identifiers (e.g., missing event IDs), it assumes there may be an open transaction and the database has already assigned IDs for new events that are not yet committed. This interval determines how long the subscription waits before retrying to fetch events, preventing loss of events that are in the process of being written to the database. Source code in event_sourcery_django/__init__.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class DjangoConfig ( BaseModel ): \"\"\" Configuration for DjangoBackend event store integration. Attributes: outbox_attempts (PositiveInt): Maximum number of outbox delivery attempts per event. gap_retry_interval (timedelta): Time to wait before retrying a subscription gap. If the subscription detects a gap in event identifiers (e.g., missing event IDs), it assumes there may be an open transaction and the database has already assigned IDs for new events that are not yet committed. This interval determines how long the subscription waits before retrying to fetch events, preventing loss of events that are in the process of being written to the database. \"\"\" model_config = ConfigDict ( extra = \"forbid\" , frozen = True ) outbox_attempts : PositiveInt = 3 gap_retry_interval : timedelta = timedelta ( seconds = 0.5 )","title":"Django"},{"location":"reference/backends/django/#djangobackend","text":"Bases: TransactionalBackend Django integration backend for Event Sourcery. Provides a fully configured TransactionalBackend for Django projects, including event store, outbox, and subscription strategies. Supports configuration via the DjangoConfig class. Source code in event_sourcery_django/__init__.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class DjangoBackend ( TransactionalBackend ): \"\"\" Django integration backend for Event Sourcery. Provides a fully configured TransactionalBackend for Django projects, including event store, outbox, and subscription strategies. Supports configuration via the `DjangoConfig` class. \"\"\" def __init__ ( self ) -> None : from event_sourcery_django.event_store import DjangoStorageStrategy from event_sourcery_django.outbox import DjangoOutboxStorageStrategy from event_sourcery_django.subscription import DjangoSubscriptionStrategy super () . __init__ () self [ DjangoConfig ] = not_configured ( \"Configure backend with `.configure(config)`\" ) self [ StorageStrategy ] = lambda c : DjangoStorageStrategy ( c [ Dispatcher ], c . get ( DjangoOutboxStorageStrategy , None ), ) . scoped_for_tenant ( c [ TenantId ]) self [ SubscriptionStrategy ] = lambda c : DjangoSubscriptionStrategy ( gap_retry_interval = c [ DjangoConfig ] . gap_retry_interval ) def configure ( self , config : DjangoConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for outbox and subscription behavior. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: config (DjangoConfig | None): Optional custom configuration. Returns: Self: The configured backend instance (for chaining). \"\"\" self [ DjangoConfig ] = config or DjangoConfig () return self def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : from event_sourcery_django.outbox import DjangoOutboxStorageStrategy self [ OutboxFiltererStrategy ] = filterer # type: ignore[type-abstract] self [ DjangoOutboxStorageStrategy ] = lambda c : DjangoOutboxStorageStrategy ( c [ OutboxFiltererStrategy ], # type: ignore[type-abstract] c [ DjangoConfig ] . outbox_attempts , ) self [ OutboxStorageStrategy ] = lambda c : c [ DjangoOutboxStorageStrategy ] return self","title":"DjangoBackend"},{"location":"reference/backends/django/#event_sourcery_django.DjangoBackend.configure","text":"Sets the backend configuration for outbox and subscription behavior. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Parameters: Name Type Description Default config DjangoConfig | None Optional custom configuration. None Returns: Name Type Description Self Self The configured backend instance (for chaining). Source code in event_sourcery_django/__init__.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def configure ( self , config : DjangoConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for outbox and subscription behavior. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: config (DjangoConfig | None): Optional custom configuration. Returns: Self: The configured backend instance (for chaining). \"\"\" self [ DjangoConfig ] = config or DjangoConfig () return self","title":"configure"},{"location":"reference/backends/django/#djangoconfig","text":"Bases: BaseModel Configuration for DjangoBackend event store integration. Attributes: Name Type Description outbox_attempts PositiveInt Maximum number of outbox delivery attempts per event. gap_retry_interval timedelta Time to wait before retrying a subscription gap. If the subscription detects a gap in event identifiers (e.g., missing event IDs), it assumes there may be an open transaction and the database has already assigned IDs for new events that are not yet committed. This interval determines how long the subscription waits before retrying to fetch events, preventing loss of events that are in the process of being written to the database. Source code in event_sourcery_django/__init__.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class DjangoConfig ( BaseModel ): \"\"\" Configuration for DjangoBackend event store integration. Attributes: outbox_attempts (PositiveInt): Maximum number of outbox delivery attempts per event. gap_retry_interval (timedelta): Time to wait before retrying a subscription gap. If the subscription detects a gap in event identifiers (e.g., missing event IDs), it assumes there may be an open transaction and the database has already assigned IDs for new events that are not yet committed. This interval determines how long the subscription waits before retrying to fetch events, preventing loss of events that are in the process of being written to the database. \"\"\" model_config = ConfigDict ( extra = \"forbid\" , frozen = True ) outbox_attempts : PositiveInt = 3 gap_retry_interval : timedelta = timedelta ( seconds = 0.5 )","title":"DjangoConfig"},{"location":"reference/backends/in_memory/","text":"InMemoryBackend Bases: TransactionalBackend In-memory backend for Event Sourcery. Provides a fully configured backend for in-memory event store. Useful for testing, development, and scenarios where persistence is not required. Ensures multi-tenancy and transactional event handling using in-memory implementations. Source code in event_sourcery/_event_store/in_memory.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 class InMemoryBackend ( TransactionalBackend ): \"\"\" In-memory backend for Event Sourcery. Provides a fully configured backend for in-memory event store. Useful for testing, development, and scenarios where persistence is not required. Ensures multi-tenancy and transactional event handling using in-memory implementations. \"\"\" def __init__ ( self ) -> None : super () . __init__ () self [ InMemoryConfig ] = not_configured ( \"Configure backend with `.configure(config)`\" ) self [ Storage ] = Storage () self [ StorageStrategy ] = lambda c : InMemoryStorageStrategy ( c [ Storage ], c [ Dispatcher ], outbox_strategy = c . get ( InMemoryOutboxStorageStrategy ), ) . scoped_for_tenant ( c [ TenantId ]) self [ SubscriptionStrategy ] = lambda c : InMemorySubscriptionStrategy ( c [ Storage ]) def configure ( self , config : InMemoryConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for outbox behavior. If no config is provided, the default configuration is used. This method must be called before using the backend. Args: config (InMemoryConfig | None): Optional custom configuration. If None, uses default KurrentDBConfig(). Returns: Self: The configured backend instance (for chaining). \"\"\" self [ InMemoryConfig ] = config or InMemoryConfig () return self def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : self [ OutboxFiltererStrategy ] = filterer # type: ignore[type-abstract] self [ InMemoryOutboxStorageStrategy ] = singleton ( lambda c : InMemoryOutboxStorageStrategy ( c [ OutboxFiltererStrategy ], # type: ignore[type-abstract] c [ InMemoryConfig ] . outbox_attempts , ) ) self [ OutboxStorageStrategy ] = lambda c : c [ InMemoryOutboxStorageStrategy ] return self configure ( config = None ) Sets the backend configuration for outbox behavior. If no config is provided, the default configuration is used. This method must be called before using the backend. Parameters: Name Type Description Default config InMemoryConfig | None Optional custom configuration. If None, uses default KurrentDBConfig(). None Returns: Name Type Description Self Self The configured backend instance (for chaining). Source code in event_sourcery/_event_store/in_memory.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 def configure ( self , config : InMemoryConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for outbox behavior. If no config is provided, the default configuration is used. This method must be called before using the backend. Args: config (InMemoryConfig | None): Optional custom configuration. If None, uses default KurrentDBConfig(). Returns: Self: The configured backend instance (for chaining). \"\"\" self [ InMemoryConfig ] = config or InMemoryConfig () return self InMemoryConfig Bases: BaseModel Configuration for InMemoryBackend event store integration. Attributes: Name Type Description outbox_attempts PositiveInt Maximum number of outbox delivery attempts per event before giving up. Source code in event_sourcery/_event_store/in_memory.py 303 304 305 306 307 308 309 310 311 312 313 class InMemoryConfig ( BaseModel ): \"\"\" Configuration for InMemoryBackend event store integration. Attributes: outbox_attempts (PositiveInt): Maximum number of outbox delivery attempts per event before giving up. \"\"\" model_config = ConfigDict ( extra = \"forbid\" , frozen = True ) outbox_attempts : PositiveInt = 3 InMemoryKeyStorage Bases: EncryptionKeyStorageStrategy In-memory implementation of encryption key storage strategy. Stores encryption keys for data subjects in memory. Suitable for testing and development where persistent key storage is not required. Source code in event_sourcery/_event_store/in_memory.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 @dataclass class InMemoryKeyStorage ( EncryptionKeyStorageStrategy ): \"\"\" In-memory implementation of encryption key storage strategy. Stores encryption keys for data subjects in memory. Suitable for testing and development where persistent key storage is not required. \"\"\" _keys : dict [ tuple [ TenantId , str ], bytes ] = field ( default_factory = dict ) _tenant_id : TenantId = DEFAULT_TENANT def get ( self , subject_id : str ) -> bytes | None : return self . _keys . get (( self . _tenant_id , subject_id )) def store ( self , subject_id : str , key : bytes ) -> None : self . _keys [( self . _tenant_id , subject_id )] = key def delete ( self , subject_id : str ) -> None : self . _keys . pop (( self . _tenant_id , subject_id ), None ) def scoped_for_tenant ( self , tenant_id : TenantId ) -> \"InMemoryKeyStorage\" : return InMemoryKeyStorage ( self . _keys , tenant_id )","title":"In-Memory"},{"location":"reference/backends/in_memory/#inmemorybackend","text":"Bases: TransactionalBackend In-memory backend for Event Sourcery. Provides a fully configured backend for in-memory event store. Useful for testing, development, and scenarios where persistence is not required. Ensures multi-tenancy and transactional event handling using in-memory implementations. Source code in event_sourcery/_event_store/in_memory.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 class InMemoryBackend ( TransactionalBackend ): \"\"\" In-memory backend for Event Sourcery. Provides a fully configured backend for in-memory event store. Useful for testing, development, and scenarios where persistence is not required. Ensures multi-tenancy and transactional event handling using in-memory implementations. \"\"\" def __init__ ( self ) -> None : super () . __init__ () self [ InMemoryConfig ] = not_configured ( \"Configure backend with `.configure(config)`\" ) self [ Storage ] = Storage () self [ StorageStrategy ] = lambda c : InMemoryStorageStrategy ( c [ Storage ], c [ Dispatcher ], outbox_strategy = c . get ( InMemoryOutboxStorageStrategy ), ) . scoped_for_tenant ( c [ TenantId ]) self [ SubscriptionStrategy ] = lambda c : InMemorySubscriptionStrategy ( c [ Storage ]) def configure ( self , config : InMemoryConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for outbox behavior. If no config is provided, the default configuration is used. This method must be called before using the backend. Args: config (InMemoryConfig | None): Optional custom configuration. If None, uses default KurrentDBConfig(). Returns: Self: The configured backend instance (for chaining). \"\"\" self [ InMemoryConfig ] = config or InMemoryConfig () return self def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : self [ OutboxFiltererStrategy ] = filterer # type: ignore[type-abstract] self [ InMemoryOutboxStorageStrategy ] = singleton ( lambda c : InMemoryOutboxStorageStrategy ( c [ OutboxFiltererStrategy ], # type: ignore[type-abstract] c [ InMemoryConfig ] . outbox_attempts , ) ) self [ OutboxStorageStrategy ] = lambda c : c [ InMemoryOutboxStorageStrategy ] return self","title":"InMemoryBackend"},{"location":"reference/backends/in_memory/#event_sourcery.backend.InMemoryBackend.configure","text":"Sets the backend configuration for outbox behavior. If no config is provided, the default configuration is used. This method must be called before using the backend. Parameters: Name Type Description Default config InMemoryConfig | None Optional custom configuration. If None, uses default KurrentDBConfig(). None Returns: Name Type Description Self Self The configured backend instance (for chaining). Source code in event_sourcery/_event_store/in_memory.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 def configure ( self , config : InMemoryConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for outbox behavior. If no config is provided, the default configuration is used. This method must be called before using the backend. Args: config (InMemoryConfig | None): Optional custom configuration. If None, uses default KurrentDBConfig(). Returns: Self: The configured backend instance (for chaining). \"\"\" self [ InMemoryConfig ] = config or InMemoryConfig () return self","title":"configure"},{"location":"reference/backends/in_memory/#inmemoryconfig","text":"Bases: BaseModel Configuration for InMemoryBackend event store integration. Attributes: Name Type Description outbox_attempts PositiveInt Maximum number of outbox delivery attempts per event before giving up. Source code in event_sourcery/_event_store/in_memory.py 303 304 305 306 307 308 309 310 311 312 313 class InMemoryConfig ( BaseModel ): \"\"\" Configuration for InMemoryBackend event store integration. Attributes: outbox_attempts (PositiveInt): Maximum number of outbox delivery attempts per event before giving up. \"\"\" model_config = ConfigDict ( extra = \"forbid\" , frozen = True ) outbox_attempts : PositiveInt = 3","title":"InMemoryConfig"},{"location":"reference/backends/in_memory/#inmemorykeystorage","text":"Bases: EncryptionKeyStorageStrategy In-memory implementation of encryption key storage strategy. Stores encryption keys for data subjects in memory. Suitable for testing and development where persistent key storage is not required. Source code in event_sourcery/_event_store/in_memory.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 @dataclass class InMemoryKeyStorage ( EncryptionKeyStorageStrategy ): \"\"\" In-memory implementation of encryption key storage strategy. Stores encryption keys for data subjects in memory. Suitable for testing and development where persistent key storage is not required. \"\"\" _keys : dict [ tuple [ TenantId , str ], bytes ] = field ( default_factory = dict ) _tenant_id : TenantId = DEFAULT_TENANT def get ( self , subject_id : str ) -> bytes | None : return self . _keys . get (( self . _tenant_id , subject_id )) def store ( self , subject_id : str , key : bytes ) -> None : self . _keys [( self . _tenant_id , subject_id )] = key def delete ( self , subject_id : str ) -> None : self . _keys . pop (( self . _tenant_id , subject_id ), None ) def scoped_for_tenant ( self , tenant_id : TenantId ) -> \"InMemoryKeyStorage\" : return InMemoryKeyStorage ( self . _keys , tenant_id )","title":"InMemoryKeyStorage"},{"location":"reference/backends/kurrentdb/","text":"KurrentDBBackend Bases: Backend KurrentDB integration backend for Event Sourcery. Source code in event_sourcery_kurrentdb/__init__.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class KurrentDBBackend ( Backend ): \"\"\" KurrentDB integration backend for Event Sourcery. \"\"\" def __init__ ( self ) -> None : super () . __init__ () self [ KurrentDBClient ] = not_configured ( \"Configure backend with `.configure(kurrentdb_client, config)`\" , ) self [ KurrentDBConfig ] = not_configured ( \"Configure backend with `.configure(kurrentdb_client, config)`\" , ) self [ StorageStrategy ] = lambda c : KurrentDBStorageStrategy ( c [ KurrentDBClient ], c [ KurrentDBConfig ] . timeout , ) . scoped_for_tenant ( c [ TenantId ]) self [ SubscriptionStrategy ] = lambda c : KurrentDBSubscriptionStrategy ( c [ KurrentDBClient ], ) def configure ( self , client : KurrentDBClient , config : KurrentDBConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for KurrentDB client and outbox behavior. Allows you to provide a KurrentDBClient instance and an optional SQLAlchemyConfig. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: client (KurrentDBClient): The KurrentDB client instance to use for backend operations. config (KurrentDBConfig | None): Optional custom configuration. If None, uses default SQLAlchemyConfig(). Returns: Self: The configured backend instance (for chaining). \"\"\" self [ KurrentDBClient ] = client self [ KurrentDBConfig ] = config or KurrentDBConfig () return self def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : self [ OutboxFiltererStrategy ] = filterer # type: ignore[type-abstract] self [ KurrentDBOutboxStorageStrategy ] = lambda c : KurrentDBOutboxStorageStrategy ( c [ KurrentDBClient ], c [ OutboxFiltererStrategy ], # type: ignore[type-abstract] c [ KurrentDBConfig ] . outbox_name , c [ KurrentDBConfig ] . outbox_attempts , c [ KurrentDBConfig ] . timeout , ) self [ OutboxStorageStrategy ] = lambda c : c [ KurrentDBOutboxStorageStrategy ] self [ KurrentDBOutboxStorageStrategy ] . create_subscription () return self configure ( client , config = None ) Sets the backend configuration for KurrentDB client and outbox behavior. Allows you to provide a KurrentDBClient instance and an optional SQLAlchemyConfig. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Parameters: Name Type Description Default client KurrentDBClient The KurrentDB client instance to use for backend operations. required config KurrentDBConfig | None Optional custom configuration. If None, uses default SQLAlchemyConfig(). None Returns: Name Type Description Self Self The configured backend instance (for chaining). Source code in event_sourcery_kurrentdb/__init__.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def configure ( self , client : KurrentDBClient , config : KurrentDBConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for KurrentDB client and outbox behavior. Allows you to provide a KurrentDBClient instance and an optional SQLAlchemyConfig. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: client (KurrentDBClient): The KurrentDB client instance to use for backend operations. config (KurrentDBConfig | None): Optional custom configuration. If None, uses default SQLAlchemyConfig(). Returns: Self: The configured backend instance (for chaining). \"\"\" self [ KurrentDBClient ] = client self [ KurrentDBConfig ] = config or KurrentDBConfig () return self KurrentDBConfig Bases: BaseModel Configuration for KurrentDBBackend event store integration. Attributes: Name Type Description timeout Seconds | None Optional timeout (in seconds) for KurrentDB operations. Controls the maximum time allowed for backend requests. If None, the default client timeout is used. outbox_name str Name of the outbox stream used for reliable event publishing. outbox_attempts PositiveInt Maximum number of outbox delivery attempts per event before giving up. Source code in event_sourcery_kurrentdb/__init__.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class KurrentDBConfig ( BaseModel ): \"\"\" Configuration for KurrentDBBackend event store integration. Attributes: timeout (Seconds | None): Optional timeout (in seconds) for KurrentDB operations. Controls the maximum time allowed for backend requests. If None, the default client timeout is used. outbox_name (str): Name of the outbox stream used for reliable event publishing. outbox_attempts (PositiveInt): Maximum number of outbox delivery attempts per event before giving up. \"\"\" model_config = ConfigDict ( extra = \"forbid\" , frozen = True ) timeout : Seconds | None = None outbox_name : str = \"pyes-outbox\" outbox_attempts : PositiveInt = 3","title":"KurrentDB"},{"location":"reference/backends/kurrentdb/#kurrentdbbackend","text":"Bases: Backend KurrentDB integration backend for Event Sourcery. Source code in event_sourcery_kurrentdb/__init__.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class KurrentDBBackend ( Backend ): \"\"\" KurrentDB integration backend for Event Sourcery. \"\"\" def __init__ ( self ) -> None : super () . __init__ () self [ KurrentDBClient ] = not_configured ( \"Configure backend with `.configure(kurrentdb_client, config)`\" , ) self [ KurrentDBConfig ] = not_configured ( \"Configure backend with `.configure(kurrentdb_client, config)`\" , ) self [ StorageStrategy ] = lambda c : KurrentDBStorageStrategy ( c [ KurrentDBClient ], c [ KurrentDBConfig ] . timeout , ) . scoped_for_tenant ( c [ TenantId ]) self [ SubscriptionStrategy ] = lambda c : KurrentDBSubscriptionStrategy ( c [ KurrentDBClient ], ) def configure ( self , client : KurrentDBClient , config : KurrentDBConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for KurrentDB client and outbox behavior. Allows you to provide a KurrentDBClient instance and an optional SQLAlchemyConfig. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: client (KurrentDBClient): The KurrentDB client instance to use for backend operations. config (KurrentDBConfig | None): Optional custom configuration. If None, uses default SQLAlchemyConfig(). Returns: Self: The configured backend instance (for chaining). \"\"\" self [ KurrentDBClient ] = client self [ KurrentDBConfig ] = config or KurrentDBConfig () return self def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : self [ OutboxFiltererStrategy ] = filterer # type: ignore[type-abstract] self [ KurrentDBOutboxStorageStrategy ] = lambda c : KurrentDBOutboxStorageStrategy ( c [ KurrentDBClient ], c [ OutboxFiltererStrategy ], # type: ignore[type-abstract] c [ KurrentDBConfig ] . outbox_name , c [ KurrentDBConfig ] . outbox_attempts , c [ KurrentDBConfig ] . timeout , ) self [ OutboxStorageStrategy ] = lambda c : c [ KurrentDBOutboxStorageStrategy ] self [ KurrentDBOutboxStorageStrategy ] . create_subscription () return self","title":"KurrentDBBackend"},{"location":"reference/backends/kurrentdb/#event_sourcery_kurrentdb.KurrentDBBackend.configure","text":"Sets the backend configuration for KurrentDB client and outbox behavior. Allows you to provide a KurrentDBClient instance and an optional SQLAlchemyConfig. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Parameters: Name Type Description Default client KurrentDBClient The KurrentDB client instance to use for backend operations. required config KurrentDBConfig | None Optional custom configuration. If None, uses default SQLAlchemyConfig(). None Returns: Name Type Description Self Self The configured backend instance (for chaining). Source code in event_sourcery_kurrentdb/__init__.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def configure ( self , client : KurrentDBClient , config : KurrentDBConfig | None = None ) -> Self : \"\"\" Sets the backend configuration for KurrentDB client and outbox behavior. Allows you to provide a KurrentDBClient instance and an optional SQLAlchemyConfig. If no config is provided, the default configuration is used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: client (KurrentDBClient): The KurrentDB client instance to use for backend operations. config (KurrentDBConfig | None): Optional custom configuration. If None, uses default SQLAlchemyConfig(). Returns: Self: The configured backend instance (for chaining). \"\"\" self [ KurrentDBClient ] = client self [ KurrentDBConfig ] = config or KurrentDBConfig () return self","title":"configure"},{"location":"reference/backends/kurrentdb/#kurrentdbconfig","text":"Bases: BaseModel Configuration for KurrentDBBackend event store integration. Attributes: Name Type Description timeout Seconds | None Optional timeout (in seconds) for KurrentDB operations. Controls the maximum time allowed for backend requests. If None, the default client timeout is used. outbox_name str Name of the outbox stream used for reliable event publishing. outbox_attempts PositiveInt Maximum number of outbox delivery attempts per event before giving up. Source code in event_sourcery_kurrentdb/__init__.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class KurrentDBConfig ( BaseModel ): \"\"\" Configuration for KurrentDBBackend event store integration. Attributes: timeout (Seconds | None): Optional timeout (in seconds) for KurrentDB operations. Controls the maximum time allowed for backend requests. If None, the default client timeout is used. outbox_name (str): Name of the outbox stream used for reliable event publishing. outbox_attempts (PositiveInt): Maximum number of outbox delivery attempts per event before giving up. \"\"\" model_config = ConfigDict ( extra = \"forbid\" , frozen = True ) timeout : Seconds | None = None outbox_name : str = \"pyes-outbox\" outbox_attempts : PositiveInt = 3","title":"KurrentDBConfig"},{"location":"reference/backends/sqlalchemy/","text":"SQLAlchemyBackend Bases: TransactionalBackend SQLAlchemy integration backend for Event Sourcery. Source code in event_sourcery_sqlalchemy/__init__.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class SQLAlchemyBackend ( TransactionalBackend ): \"\"\" SQLAlchemy integration backend for Event Sourcery. \"\"\" UNCONFIGURED_MESSAGE = \"Configure backend with `.configure(session, config)`\" def __init__ ( self ) -> None : super () . __init__ () self [ Models ] = not_configured ( self . UNCONFIGURED_MESSAGE ) self [ Session ] = not_configured ( self . UNCONFIGURED_MESSAGE ) self [ SQLAlchemyConfig ] = not_configured ( self . UNCONFIGURED_MESSAGE ) self [ StorageStrategy ] = lambda c : SqlAlchemyStorageStrategy ( c [ Session ], c [ Dispatcher ], c . get ( SqlAlchemyOutboxStorageStrategy , None ), c [ Models ] . event_model , c [ Models ] . snapshot_model , c [ Models ] . stream_model , ) . scoped_for_tenant ( c [ TenantId ]) self [ SubscriptionStrategy ] = lambda c : SqlAlchemySubscriptionStrategy ( c [ Session ], c [ SQLAlchemyConfig ] . gap_retry_interval , c [ Models ] . event_model , c [ Models ] . stream_model , ) def configure ( self , session : Session , config : SQLAlchemyConfig | None = None , custom_models : Models | None = None , ) -> Self : \"\"\" Sets the backend configuration for SQLAlchemy session, outbox, and models. Allows you to provide a SQLAlchemy Session instance, an optional Config instance, and optional custom ORM models. If no config or models are provided, the default configuration and models are used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: session (Session): The SQLAlchemy session instance to use for backend operations. config (SQLAlchemyConfig | None): Optional custom configuration. If None, uses default Config(). custom_models (Models | None): Optional custom ORM models. If None, uses default models. Returns: Self: The configured backend instance (for chaining). \"\"\" if custom_models is None : custom_models = Models ( event_model = DefaultEvent , stream_model = DefaultStream , snapshot_model = DefaultSnapshot , outbox_entry_model = DefaultOutboxEntry , ) self [ Session ] = session self [ SQLAlchemyConfig ] = config or SQLAlchemyConfig () self [ Models ] = custom_models return self def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : self [ OutboxFiltererStrategy ] = filterer # type: ignore[type-abstract] self [ SqlAlchemyOutboxStorageStrategy ] = ( lambda c : SqlAlchemyOutboxStorageStrategy ( c [ Session ], c [ OutboxFiltererStrategy ], # type: ignore[type-abstract] c [ SQLAlchemyConfig ] . outbox_attempts , c [ Models ] . outbox_entry_model , ) ) self [ OutboxStorageStrategy ] = lambda c : c [ SqlAlchemyOutboxStorageStrategy ] return self configure ( session , config = None , custom_models = None ) Sets the backend configuration for SQLAlchemy session, outbox, and models. Allows you to provide a SQLAlchemy Session instance, an optional Config instance, and optional custom ORM models. If no config or models are provided, the default configuration and models are used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Parameters: Name Type Description Default session Session The SQLAlchemy session instance to use for backend operations. required config SQLAlchemyConfig | None Optional custom configuration. If None, uses default Config(). None custom_models Models | None Optional custom ORM models. If None, uses default models. None Returns: Name Type Description Self Self The configured backend instance (for chaining). Source code in event_sourcery_sqlalchemy/__init__.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def configure ( self , session : Session , config : SQLAlchemyConfig | None = None , custom_models : Models | None = None , ) -> Self : \"\"\" Sets the backend configuration for SQLAlchemy session, outbox, and models. Allows you to provide a SQLAlchemy Session instance, an optional Config instance, and optional custom ORM models. If no config or models are provided, the default configuration and models are used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: session (Session): The SQLAlchemy session instance to use for backend operations. config (SQLAlchemyConfig | None): Optional custom configuration. If None, uses default Config(). custom_models (Models | None): Optional custom ORM models. If None, uses default models. Returns: Self: The configured backend instance (for chaining). \"\"\" if custom_models is None : custom_models = Models ( event_model = DefaultEvent , stream_model = DefaultStream , snapshot_model = DefaultSnapshot , outbox_entry_model = DefaultOutboxEntry , ) self [ Session ] = session self [ SQLAlchemyConfig ] = config or SQLAlchemyConfig () self [ Models ] = custom_models return self SQLAlchemyConfig Bases: BaseModel Configuration for SQLAlchemyBackend event store integration. Attributes: Name Type Description outbox_attempts PositiveInt Maximum number of outbox delivery attempts per event before giving up. gap_retry_interval timedelta Time to wait before retrying a subscription gap. If the subscription detects a gap in event identifiers (e.g., missing event IDs), it assumes there may be an open transaction and the database has already assigned IDs for new events that are not yet committed. This interval determines how long the subscription waits before retrying to fetch events, preventing loss of events that are in the process of being written to the database. Source code in event_sourcery_sqlalchemy/__init__.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class SQLAlchemyConfig ( BaseModel ): \"\"\" Configuration for SQLAlchemyBackend event store integration. Attributes: outbox_attempts (PositiveInt): Maximum number of outbox delivery attempts per event before giving up. gap_retry_interval (timedelta): Time to wait before retrying a subscription gap. If the subscription detects a gap in event identifiers (e.g., missing event IDs), it assumes there may be an open transaction and the database has already assigned IDs for new events that are not yet committed. This interval determines how long the subscription waits before retrying to fetch events, preventing loss of events that are in the process of being written to the database. \"\"\" model_config = ConfigDict ( extra = \"forbid\" , frozen = True ) outbox_attempts : PositiveInt = 3 gap_retry_interval : timedelta = timedelta ( seconds = 0.5 ) Models Container for SQLAlchemy ORM models used by the backend. Allows customization of the event, stream, snapshot, and outbox entry models used by the backend. It allows to have different event store models for different modules in modular monolith applications still having in transactional event dispatching between them. By default, uses the standard models provided by Event Sourcery. Attributes: Name Type Description event_model type [ BaseEvent ] SQLAlchemy model for events. stream_model type [ BaseStream ] SQLAlchemy model for streams. snapshot_model type [ BaseSnapshot ] SQLAlchemy model for snapshots. outbox_entry_model type [ BaseOutboxEntry ] SQLAlchemy model for outbox entries (default: DefaultOutboxEntry). Source code in event_sourcery_sqlalchemy/__init__.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @dataclass ( frozen = True ) class Models : \"\"\" Container for SQLAlchemy ORM models used by the backend. Allows customization of the event, stream, snapshot, and outbox entry models used by the backend. It allows to have different event store models for different modules in modular monolith applications still having in transactional event dispatching between them. By default, uses the standard models provided by Event Sourcery. Attributes: event_model (type[BaseEvent]): SQLAlchemy model for events. stream_model (type[BaseStream]): SQLAlchemy model for streams. snapshot_model (type[BaseSnapshot]): SQLAlchemy model for snapshots. outbox_entry_model (type[BaseOutboxEntry]): SQLAlchemy model for outbox entries (default: DefaultOutboxEntry). \"\"\" event_model : type [ BaseEvent ] stream_model : type [ BaseStream ] snapshot_model : type [ BaseSnapshot ] outbox_entry_model : type [ BaseOutboxEntry ] = DefaultOutboxEntry configure_models Configures SQLAlchemy ORM models for Event Sourcery backend. Sets up mapping information and registers the provided (or default) models with SQLAlchemy's registry. This function allows customization of event, stream, snapshot, outbox entry, and projector cursor models for advanced scenarios, or uses the default models for standard usage. Ensures all models are mapped declaratively and share the same metadata and class registry, enabling flexible schema management and migrations. Parameters: Name Type Description Default base type [ BaseProto ] Base class providing SQLAlchemy MetaData for model registration. required event_model type [ BaseEvent ] Event model class to use. Defaults to DefaultEvent. DefaultEvent stream_model type [ BaseStream ] Stream model class to use. Defaults to DefaultStream. DefaultStream snapshot_model type [ BaseSnapshot ] Snapshot model class to use. Defaults to DefaultSnapshot. DefaultSnapshot outbox_entry_model type [ BaseOutboxEntry ] Outbox entry model class to use. Defaults to DefaultOutboxEntry. DefaultOutboxEntry projector_cursor_model type [ BaseProjectorCursor ] Projector cursor model class to use. Defaults to DefaultProjectorCursor. DefaultProjectorCursor Source code in event_sourcery_sqlalchemy/models/__init__.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def configure_models ( base : type [ BaseProto ], event_model : type [ BaseEvent ] = DefaultEvent , stream_model : type [ BaseStream ] = DefaultStream , snapshot_model : type [ BaseSnapshot ] = DefaultSnapshot , outbox_entry_model : type [ BaseOutboxEntry ] = DefaultOutboxEntry , projector_cursor_model : type [ BaseProjectorCursor ] = DefaultProjectorCursor , ) -> None : \"\"\" Configures SQLAlchemy ORM models for Event Sourcery backend. Sets up mapping information and registers the provided (or default) models with SQLAlchemy's registry. This function allows customization of event, stream, snapshot, outbox entry, and projector cursor models for advanced scenarios, or uses the default models for standard usage. Ensures all models are mapped declaratively and share the same metadata and class registry, enabling flexible schema management and migrations. Args: base (type[BaseProto]): Base class providing SQLAlchemy MetaData for model registration. event_model (type[BaseEvent], optional): Event model class to use. Defaults to DefaultEvent. stream_model (type[BaseStream], optional): Stream model class to use. Defaults to DefaultStream. snapshot_model (type[BaseSnapshot], optional): Snapshot model class to use. Defaults to DefaultSnapshot. outbox_entry_model (type[BaseOutboxEntry], optional): Outbox entry model class to use. Defaults to DefaultOutboxEntry. projector_cursor_model (type[BaseProjectorCursor], optional): Projector cursor model class to use. Defaults to DefaultProjectorCursor. \"\"\" event_model . __set_mapping_information__ ( stream_model ) snapshot_model . __set_mapping_information__ ( stream_model ) stream_model . __set_mapping_information__ ( event_model , snapshot_model ) mapping_registry = registry ( metadata = base . metadata , class_registry = _class_registry ) for model_cls in ( stream_model , event_model , snapshot_model , outbox_entry_model , projector_cursor_model , ): if model_cls in _class_registry . values (): continue mapping_registry . map_declaratively ( model_cls ) BaseEvent Source code in event_sourcery_sqlalchemy/models/base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class BaseEvent : __stream_model__ : type [ \"BaseStream\" ] __tablename__ : str @declared_attr # type: ignore[arg-type] @classmethod def __table_args__ ( cls ) -> tuple [ Index | UniqueConstraint , ... ]: return ( Index ( f \"ix_events_stream_id_version_ { cls . __tablename__ } \" , \"db_stream_id\" , \"version\" , unique = True , ), ) def __init__ ( self , uuid : UUID , created_at : datetime , name : str , data : dict , event_context : dict , version : int | None , ) -> None : self . uuid = uuid self . created_at = created_at self . name = name self . data = data self . event_context = event_context self . version = version @classmethod def __set_mapping_information__ ( cls , stream_model : type [ BaseStream ]) -> None : cls . __stream_model__ = stream_model @declared_attr @classmethod def _db_stream_id ( cls ) -> MappedColumn [ Any ]: return mapped_column ( \"db_stream_id\" , BigInteger () . with_variant ( Integer (), \"sqlite\" ), ForeignKey ( cls . __stream_model__ . id ), nullable = False , index = True , ) @declared_attr @classmethod def stream ( cls ) -> Mapped [ BaseStream ]: return relationship ( cls . __stream_model__ , back_populates = \"events\" ) id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) version = mapped_column ( Integer (), nullable = True ) uuid = mapped_column ( GUID (), index = True , unique = True ) stream_id : AssociationProxy [ StreamId ] = association_proxy ( \"stream\" , \"stream_id\" ) tenant_id : AssociationProxy [ TenantId ] = association_proxy ( \"stream\" , \"tenant_id\" ) name = mapped_column ( String ( 200 ), nullable = False ) data = mapped_column ( JSONB (), nullable = False ) event_context = mapped_column ( JSONB (), nullable = False ) created_at = mapped_column ( DateTime (), nullable = False , index = True ) BaseStream Source code in event_sourcery_sqlalchemy/models/base.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class BaseStream : __event_model__ : type [ \"BaseEvent\" ] __snapshot_model__ : type [ \"BaseSnapshot\" ] @declared_attr # type: ignore[arg-type] @classmethod def __table_args__ ( cls ) -> tuple [ Index | UniqueConstraint , ... ]: return ( UniqueConstraint ( \"uuid\" , \"category\" , \"tenant_id\" ), UniqueConstraint ( \"name\" , \"category\" , \"tenant_id\" ), ) id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) uuid = mapped_column ( GUID (), nullable = False ) name = mapped_column ( String ( 255 ), nullable = True , default = None ) category = mapped_column ( String ( 255 ), nullable = False , default = \"\" ) tenant_id = mapped_column ( String ( 255 ), nullable = False ) version = mapped_column ( BigInteger (), nullable = True ) @classmethod def __set_mapping_information__ ( cls , event_model : type [ \"BaseEvent\" ], snapshot_model : type [ \"BaseSnapshot\" ] ) -> None : cls . __event_model__ = event_model cls . __snapshot_model__ = snapshot_model @declared_attr @classmethod def events ( cls ) -> Mapped [ list [ \"BaseEvent\" ]]: return relationship ( cls . __event_model__ , back_populates = \"stream\" ) @declared_attr @classmethod def snapshots ( cls ) -> Mapped [ list [ \"BaseSnapshot\" ]]: return relationship ( cls . __snapshot_model__ , back_populates = \"stream\" ) @hybrid_property def stream_id ( self ) -> StreamId : return StreamId ( self . uuid , self . name , category = self . category or None ) @stream_id . inplace . comparator @classmethod def _stream_id_comparator ( cls ) -> StreamIdComparator : return StreamIdComparator ( cls . uuid , cls . name , cls . category ) BaseOutboxEntry Source code in event_sourcery_sqlalchemy/models/base.py 210 211 212 213 214 215 216 class BaseOutboxEntry : id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) created_at = mapped_column ( DateTime (), nullable = False , index = True ) data = mapped_column ( JSONB (), nullable = False ) stream_name = mapped_column ( String ( 255 ), nullable = True ) position = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" )) tries_left = mapped_column ( Integer (), nullable = False ) BaseProjectorCursor Source code in event_sourcery_sqlalchemy/models/base.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class BaseProjectorCursor : __tablename__ : str @declared_attr # type: ignore[arg-type] @classmethod def __table_args__ ( cls ) -> tuple [ Index | UniqueConstraint , ... ]: return ( Index ( f \"ix_name_stream_id_ { cls . __tablename__ } \" , \"name\" , \"stream_id\" , \"category\" , unique = True , ), ) id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) name = mapped_column ( String ( 255 ), nullable = False ) stream_id = mapped_column ( GUID (), nullable = False , index = True ) category = mapped_column ( String ( 255 ), nullable = True ) version = mapped_column ( BigInteger (), nullable = False ) BaseSnapshot Source code in event_sourcery_sqlalchemy/models/base.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 class BaseSnapshot : __stream_model__ : type [ \"BaseStream\" ] def __init__ ( self , uuid : UUID , created_at : datetime , name : str , data : dict , event_context : dict , version : int | None , ) -> None : self . uuid = uuid self . created_at = created_at self . name = name self . data = data self . event_context = event_context self . version = version @classmethod def __set_mapping_information__ ( cls , stream_model : type [ BaseStream ]) -> None : cls . __stream_model__ = stream_model @declared_attr @classmethod def _db_stream_id ( cls ) -> MappedColumn [ Any ]: return mapped_column ( \"db_stream_id\" , BigInteger () . with_variant ( Integer (), \"sqlite\" ), ForeignKey ( cls . __stream_model__ . id ), nullable = False , index = True , ) @declared_attr @classmethod def stream ( cls ) -> Mapped [ BaseStream ]: return relationship ( cls . __stream_model__ , back_populates = \"snapshots\" ) uuid = mapped_column ( GUID , primary_key = True ) version = mapped_column ( Integer (), nullable = False ) stream_id : AssociationProxy [ StreamId ] = association_proxy ( \"stream\" , \"stream_id\" ) name = mapped_column ( String ( 50 ), nullable = False ) data = mapped_column ( JSONB (), nullable = False ) event_context = mapped_column ( JSONB (), nullable = False ) created_at = mapped_column ( DateTime (), nullable = False )","title":"SQLAlchemy"},{"location":"reference/backends/sqlalchemy/#sqlalchemybackend","text":"Bases: TransactionalBackend SQLAlchemy integration backend for Event Sourcery. Source code in event_sourcery_sqlalchemy/__init__.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class SQLAlchemyBackend ( TransactionalBackend ): \"\"\" SQLAlchemy integration backend for Event Sourcery. \"\"\" UNCONFIGURED_MESSAGE = \"Configure backend with `.configure(session, config)`\" def __init__ ( self ) -> None : super () . __init__ () self [ Models ] = not_configured ( self . UNCONFIGURED_MESSAGE ) self [ Session ] = not_configured ( self . UNCONFIGURED_MESSAGE ) self [ SQLAlchemyConfig ] = not_configured ( self . UNCONFIGURED_MESSAGE ) self [ StorageStrategy ] = lambda c : SqlAlchemyStorageStrategy ( c [ Session ], c [ Dispatcher ], c . get ( SqlAlchemyOutboxStorageStrategy , None ), c [ Models ] . event_model , c [ Models ] . snapshot_model , c [ Models ] . stream_model , ) . scoped_for_tenant ( c [ TenantId ]) self [ SubscriptionStrategy ] = lambda c : SqlAlchemySubscriptionStrategy ( c [ Session ], c [ SQLAlchemyConfig ] . gap_retry_interval , c [ Models ] . event_model , c [ Models ] . stream_model , ) def configure ( self , session : Session , config : SQLAlchemyConfig | None = None , custom_models : Models | None = None , ) -> Self : \"\"\" Sets the backend configuration for SQLAlchemy session, outbox, and models. Allows you to provide a SQLAlchemy Session instance, an optional Config instance, and optional custom ORM models. If no config or models are provided, the default configuration and models are used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: session (Session): The SQLAlchemy session instance to use for backend operations. config (SQLAlchemyConfig | None): Optional custom configuration. If None, uses default Config(). custom_models (Models | None): Optional custom ORM models. If None, uses default models. Returns: Self: The configured backend instance (for chaining). \"\"\" if custom_models is None : custom_models = Models ( event_model = DefaultEvent , stream_model = DefaultStream , snapshot_model = DefaultSnapshot , outbox_entry_model = DefaultOutboxEntry , ) self [ Session ] = session self [ SQLAlchemyConfig ] = config or SQLAlchemyConfig () self [ Models ] = custom_models return self def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : self [ OutboxFiltererStrategy ] = filterer # type: ignore[type-abstract] self [ SqlAlchemyOutboxStorageStrategy ] = ( lambda c : SqlAlchemyOutboxStorageStrategy ( c [ Session ], c [ OutboxFiltererStrategy ], # type: ignore[type-abstract] c [ SQLAlchemyConfig ] . outbox_attempts , c [ Models ] . outbox_entry_model , ) ) self [ OutboxStorageStrategy ] = lambda c : c [ SqlAlchemyOutboxStorageStrategy ] return self","title":"SQLAlchemyBackend"},{"location":"reference/backends/sqlalchemy/#event_sourcery_sqlalchemy.SQLAlchemyBackend.configure","text":"Sets the backend configuration for SQLAlchemy session, outbox, and models. Allows you to provide a SQLAlchemy Session instance, an optional Config instance, and optional custom ORM models. If no config or models are provided, the default configuration and models are used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Parameters: Name Type Description Default session Session The SQLAlchemy session instance to use for backend operations. required config SQLAlchemyConfig | None Optional custom configuration. If None, uses default Config(). None custom_models Models | None Optional custom ORM models. If None, uses default models. None Returns: Name Type Description Self Self The configured backend instance (for chaining). Source code in event_sourcery_sqlalchemy/__init__.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def configure ( self , session : Session , config : SQLAlchemyConfig | None = None , custom_models : Models | None = None , ) -> Self : \"\"\" Sets the backend configuration for SQLAlchemy session, outbox, and models. Allows you to provide a SQLAlchemy Session instance, an optional Config instance, and optional custom ORM models. If no config or models are provided, the default configuration and models are used. This method must be called before using the backend in production to ensure correct event publishing and subscription reliability. Args: session (Session): The SQLAlchemy session instance to use for backend operations. config (SQLAlchemyConfig | None): Optional custom configuration. If None, uses default Config(). custom_models (Models | None): Optional custom ORM models. If None, uses default models. Returns: Self: The configured backend instance (for chaining). \"\"\" if custom_models is None : custom_models = Models ( event_model = DefaultEvent , stream_model = DefaultStream , snapshot_model = DefaultSnapshot , outbox_entry_model = DefaultOutboxEntry , ) self [ Session ] = session self [ SQLAlchemyConfig ] = config or SQLAlchemyConfig () self [ Models ] = custom_models return self","title":"configure"},{"location":"reference/backends/sqlalchemy/#sqlalchemyconfig","text":"Bases: BaseModel Configuration for SQLAlchemyBackend event store integration. Attributes: Name Type Description outbox_attempts PositiveInt Maximum number of outbox delivery attempts per event before giving up. gap_retry_interval timedelta Time to wait before retrying a subscription gap. If the subscription detects a gap in event identifiers (e.g., missing event IDs), it assumes there may be an open transaction and the database has already assigned IDs for new events that are not yet committed. This interval determines how long the subscription waits before retrying to fetch events, preventing loss of events that are in the process of being written to the database. Source code in event_sourcery_sqlalchemy/__init__.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class SQLAlchemyConfig ( BaseModel ): \"\"\" Configuration for SQLAlchemyBackend event store integration. Attributes: outbox_attempts (PositiveInt): Maximum number of outbox delivery attempts per event before giving up. gap_retry_interval (timedelta): Time to wait before retrying a subscription gap. If the subscription detects a gap in event identifiers (e.g., missing event IDs), it assumes there may be an open transaction and the database has already assigned IDs for new events that are not yet committed. This interval determines how long the subscription waits before retrying to fetch events, preventing loss of events that are in the process of being written to the database. \"\"\" model_config = ConfigDict ( extra = \"forbid\" , frozen = True ) outbox_attempts : PositiveInt = 3 gap_retry_interval : timedelta = timedelta ( seconds = 0.5 )","title":"SQLAlchemyConfig"},{"location":"reference/backends/sqlalchemy/#models","text":"Container for SQLAlchemy ORM models used by the backend. Allows customization of the event, stream, snapshot, and outbox entry models used by the backend. It allows to have different event store models for different modules in modular monolith applications still having in transactional event dispatching between them. By default, uses the standard models provided by Event Sourcery. Attributes: Name Type Description event_model type [ BaseEvent ] SQLAlchemy model for events. stream_model type [ BaseStream ] SQLAlchemy model for streams. snapshot_model type [ BaseSnapshot ] SQLAlchemy model for snapshots. outbox_entry_model type [ BaseOutboxEntry ] SQLAlchemy model for outbox entries (default: DefaultOutboxEntry). Source code in event_sourcery_sqlalchemy/__init__.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @dataclass ( frozen = True ) class Models : \"\"\" Container for SQLAlchemy ORM models used by the backend. Allows customization of the event, stream, snapshot, and outbox entry models used by the backend. It allows to have different event store models for different modules in modular monolith applications still having in transactional event dispatching between them. By default, uses the standard models provided by Event Sourcery. Attributes: event_model (type[BaseEvent]): SQLAlchemy model for events. stream_model (type[BaseStream]): SQLAlchemy model for streams. snapshot_model (type[BaseSnapshot]): SQLAlchemy model for snapshots. outbox_entry_model (type[BaseOutboxEntry]): SQLAlchemy model for outbox entries (default: DefaultOutboxEntry). \"\"\" event_model : type [ BaseEvent ] stream_model : type [ BaseStream ] snapshot_model : type [ BaseSnapshot ] outbox_entry_model : type [ BaseOutboxEntry ] = DefaultOutboxEntry","title":"Models"},{"location":"reference/backends/sqlalchemy/#configure_models","text":"Configures SQLAlchemy ORM models for Event Sourcery backend. Sets up mapping information and registers the provided (or default) models with SQLAlchemy's registry. This function allows customization of event, stream, snapshot, outbox entry, and projector cursor models for advanced scenarios, or uses the default models for standard usage. Ensures all models are mapped declaratively and share the same metadata and class registry, enabling flexible schema management and migrations. Parameters: Name Type Description Default base type [ BaseProto ] Base class providing SQLAlchemy MetaData for model registration. required event_model type [ BaseEvent ] Event model class to use. Defaults to DefaultEvent. DefaultEvent stream_model type [ BaseStream ] Stream model class to use. Defaults to DefaultStream. DefaultStream snapshot_model type [ BaseSnapshot ] Snapshot model class to use. Defaults to DefaultSnapshot. DefaultSnapshot outbox_entry_model type [ BaseOutboxEntry ] Outbox entry model class to use. Defaults to DefaultOutboxEntry. DefaultOutboxEntry projector_cursor_model type [ BaseProjectorCursor ] Projector cursor model class to use. Defaults to DefaultProjectorCursor. DefaultProjectorCursor Source code in event_sourcery_sqlalchemy/models/__init__.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def configure_models ( base : type [ BaseProto ], event_model : type [ BaseEvent ] = DefaultEvent , stream_model : type [ BaseStream ] = DefaultStream , snapshot_model : type [ BaseSnapshot ] = DefaultSnapshot , outbox_entry_model : type [ BaseOutboxEntry ] = DefaultOutboxEntry , projector_cursor_model : type [ BaseProjectorCursor ] = DefaultProjectorCursor , ) -> None : \"\"\" Configures SQLAlchemy ORM models for Event Sourcery backend. Sets up mapping information and registers the provided (or default) models with SQLAlchemy's registry. This function allows customization of event, stream, snapshot, outbox entry, and projector cursor models for advanced scenarios, or uses the default models for standard usage. Ensures all models are mapped declaratively and share the same metadata and class registry, enabling flexible schema management and migrations. Args: base (type[BaseProto]): Base class providing SQLAlchemy MetaData for model registration. event_model (type[BaseEvent], optional): Event model class to use. Defaults to DefaultEvent. stream_model (type[BaseStream], optional): Stream model class to use. Defaults to DefaultStream. snapshot_model (type[BaseSnapshot], optional): Snapshot model class to use. Defaults to DefaultSnapshot. outbox_entry_model (type[BaseOutboxEntry], optional): Outbox entry model class to use. Defaults to DefaultOutboxEntry. projector_cursor_model (type[BaseProjectorCursor], optional): Projector cursor model class to use. Defaults to DefaultProjectorCursor. \"\"\" event_model . __set_mapping_information__ ( stream_model ) snapshot_model . __set_mapping_information__ ( stream_model ) stream_model . __set_mapping_information__ ( event_model , snapshot_model ) mapping_registry = registry ( metadata = base . metadata , class_registry = _class_registry ) for model_cls in ( stream_model , event_model , snapshot_model , outbox_entry_model , projector_cursor_model , ): if model_cls in _class_registry . values (): continue mapping_registry . map_declaratively ( model_cls )","title":"configure_models"},{"location":"reference/backends/sqlalchemy/#baseevent","text":"Source code in event_sourcery_sqlalchemy/models/base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class BaseEvent : __stream_model__ : type [ \"BaseStream\" ] __tablename__ : str @declared_attr # type: ignore[arg-type] @classmethod def __table_args__ ( cls ) -> tuple [ Index | UniqueConstraint , ... ]: return ( Index ( f \"ix_events_stream_id_version_ { cls . __tablename__ } \" , \"db_stream_id\" , \"version\" , unique = True , ), ) def __init__ ( self , uuid : UUID , created_at : datetime , name : str , data : dict , event_context : dict , version : int | None , ) -> None : self . uuid = uuid self . created_at = created_at self . name = name self . data = data self . event_context = event_context self . version = version @classmethod def __set_mapping_information__ ( cls , stream_model : type [ BaseStream ]) -> None : cls . __stream_model__ = stream_model @declared_attr @classmethod def _db_stream_id ( cls ) -> MappedColumn [ Any ]: return mapped_column ( \"db_stream_id\" , BigInteger () . with_variant ( Integer (), \"sqlite\" ), ForeignKey ( cls . __stream_model__ . id ), nullable = False , index = True , ) @declared_attr @classmethod def stream ( cls ) -> Mapped [ BaseStream ]: return relationship ( cls . __stream_model__ , back_populates = \"events\" ) id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) version = mapped_column ( Integer (), nullable = True ) uuid = mapped_column ( GUID (), index = True , unique = True ) stream_id : AssociationProxy [ StreamId ] = association_proxy ( \"stream\" , \"stream_id\" ) tenant_id : AssociationProxy [ TenantId ] = association_proxy ( \"stream\" , \"tenant_id\" ) name = mapped_column ( String ( 200 ), nullable = False ) data = mapped_column ( JSONB (), nullable = False ) event_context = mapped_column ( JSONB (), nullable = False ) created_at = mapped_column ( DateTime (), nullable = False , index = True )","title":"BaseEvent"},{"location":"reference/backends/sqlalchemy/#basestream","text":"Source code in event_sourcery_sqlalchemy/models/base.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class BaseStream : __event_model__ : type [ \"BaseEvent\" ] __snapshot_model__ : type [ \"BaseSnapshot\" ] @declared_attr # type: ignore[arg-type] @classmethod def __table_args__ ( cls ) -> tuple [ Index | UniqueConstraint , ... ]: return ( UniqueConstraint ( \"uuid\" , \"category\" , \"tenant_id\" ), UniqueConstraint ( \"name\" , \"category\" , \"tenant_id\" ), ) id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) uuid = mapped_column ( GUID (), nullable = False ) name = mapped_column ( String ( 255 ), nullable = True , default = None ) category = mapped_column ( String ( 255 ), nullable = False , default = \"\" ) tenant_id = mapped_column ( String ( 255 ), nullable = False ) version = mapped_column ( BigInteger (), nullable = True ) @classmethod def __set_mapping_information__ ( cls , event_model : type [ \"BaseEvent\" ], snapshot_model : type [ \"BaseSnapshot\" ] ) -> None : cls . __event_model__ = event_model cls . __snapshot_model__ = snapshot_model @declared_attr @classmethod def events ( cls ) -> Mapped [ list [ \"BaseEvent\" ]]: return relationship ( cls . __event_model__ , back_populates = \"stream\" ) @declared_attr @classmethod def snapshots ( cls ) -> Mapped [ list [ \"BaseSnapshot\" ]]: return relationship ( cls . __snapshot_model__ , back_populates = \"stream\" ) @hybrid_property def stream_id ( self ) -> StreamId : return StreamId ( self . uuid , self . name , category = self . category or None ) @stream_id . inplace . comparator @classmethod def _stream_id_comparator ( cls ) -> StreamIdComparator : return StreamIdComparator ( cls . uuid , cls . name , cls . category )","title":"BaseStream"},{"location":"reference/backends/sqlalchemy/#baseoutboxentry","text":"Source code in event_sourcery_sqlalchemy/models/base.py 210 211 212 213 214 215 216 class BaseOutboxEntry : id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) created_at = mapped_column ( DateTime (), nullable = False , index = True ) data = mapped_column ( JSONB (), nullable = False ) stream_name = mapped_column ( String ( 255 ), nullable = True ) position = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" )) tries_left = mapped_column ( Integer (), nullable = False )","title":"BaseOutboxEntry"},{"location":"reference/backends/sqlalchemy/#baseprojectorcursor","text":"Source code in event_sourcery_sqlalchemy/models/base.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class BaseProjectorCursor : __tablename__ : str @declared_attr # type: ignore[arg-type] @classmethod def __table_args__ ( cls ) -> tuple [ Index | UniqueConstraint , ... ]: return ( Index ( f \"ix_name_stream_id_ { cls . __tablename__ } \" , \"name\" , \"stream_id\" , \"category\" , unique = True , ), ) id = mapped_column ( BigInteger () . with_variant ( Integer (), \"sqlite\" ), primary_key = True ) name = mapped_column ( String ( 255 ), nullable = False ) stream_id = mapped_column ( GUID (), nullable = False , index = True ) category = mapped_column ( String ( 255 ), nullable = True ) version = mapped_column ( BigInteger (), nullable = False )","title":"BaseProjectorCursor"},{"location":"reference/backends/sqlalchemy/#basesnapshot","text":"Source code in event_sourcery_sqlalchemy/models/base.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 class BaseSnapshot : __stream_model__ : type [ \"BaseStream\" ] def __init__ ( self , uuid : UUID , created_at : datetime , name : str , data : dict , event_context : dict , version : int | None , ) -> None : self . uuid = uuid self . created_at = created_at self . name = name self . data = data self . event_context = event_context self . version = version @classmethod def __set_mapping_information__ ( cls , stream_model : type [ BaseStream ]) -> None : cls . __stream_model__ = stream_model @declared_attr @classmethod def _db_stream_id ( cls ) -> MappedColumn [ Any ]: return mapped_column ( \"db_stream_id\" , BigInteger () . with_variant ( Integer (), \"sqlite\" ), ForeignKey ( cls . __stream_model__ . id ), nullable = False , index = True , ) @declared_attr @classmethod def stream ( cls ) -> Mapped [ BaseStream ]: return relationship ( cls . __stream_model__ , back_populates = \"snapshots\" ) uuid = mapped_column ( GUID , primary_key = True ) version = mapped_column ( Integer (), nullable = False ) stream_id : AssociationProxy [ StreamId ] = association_proxy ( \"stream\" , \"stream_id\" ) name = mapped_column ( String ( 50 ), nullable = False ) data = mapped_column ( JSONB (), nullable = False ) event_context = mapped_column ( JSONB (), nullable = False ) created_at = mapped_column ( DateTime (), nullable = False )","title":"BaseSnapshot"},{"location":"reference/event_sourcing/Aggregate/","text":"Base class for event-sourced aggregates. Aggregates encapsulate domain logic and state changes as a sequence of events. They are responsible for applying events to mutate their state and for emitting new events that represent business-relevant changes. The Aggregate base class provides mechanisms for tracking unpersisted changes and for applying events in a consistent way. Attributes: Name Type Description category ClassVar [ str ] StreamCategory for the aggregate type (group streams). _changes list [ Event ] List of yet not persisted events. Source code in event_sourcery/event_sourcing/aggregate.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class Aggregate : \"\"\" Base class for event-sourced aggregates. Aggregates encapsulate domain logic and state changes as a sequence of events. They are responsible for applying events to mutate their state and for emitting new events that represent business-relevant changes. The Aggregate base class provides mechanisms for tracking unpersisted changes and for applying events in a consistent way. Attributes: category (ClassVar[str]): StreamCategory for the aggregate type (group streams). _changes (list[Event]): List of yet not persisted events. \"\"\" category : ClassVar [ str ] _changes : list [ Event ] @contextmanager def __persisting_changes__ ( self ) -> Iterator [ Iterator [ Event ]]: \"\"\" Context manager for accessing and clearing unpersisted events. Yields an iterator over all events that have been applied but not yet persisted. After exiting the context, the list of unpersisted events is cleared. Returns: Iterator[Iterator[Event]]: Iterator over unpersisted events. \"\"\" yield iter ( getattr ( self , \"_changes\" , [])) self . _changes = [] def __apply__ ( self , event : Event ) -> None : \"\"\" Applies a single event to mutate the aggregate's state. This method must be implemented by subclasses to define how each event type changes the aggregate's state. Args: event (Event): The event to apply. \"\"\" raise NotImplementedError def _emit ( self , event : Event ) -> None : \"\"\" Applies and tracks a new event as a pending change. Calls __apply__ to mutate the aggregate's state and appends the event to the list of unpersisted changes. Args: event (Event): The event to emit and track. \"\"\" if not hasattr ( self , \"_changes\" ): self . _changes = [] self . __apply__ ( event ) self . _changes . append ( event ) __apply__ ( event ) Applies a single event to mutate the aggregate's state. This method must be implemented by subclasses to define how each event type changes the aggregate's state. Parameters: Name Type Description Default event Event The event to apply. required Source code in event_sourcery/event_sourcing/aggregate.py 41 42 43 44 45 46 47 48 49 50 51 def __apply__ ( self , event : Event ) -> None : \"\"\" Applies a single event to mutate the aggregate's state. This method must be implemented by subclasses to define how each event type changes the aggregate's state. Args: event (Event): The event to apply. \"\"\" raise NotImplementedError __persisting_changes__ () Context manager for accessing and clearing unpersisted events. Yields an iterator over all events that have been applied but not yet persisted. After exiting the context, the list of unpersisted events is cleared. Returns: Type Description Iterator [ Iterator [ Event ]] Iterator[Iterator[Event]]: Iterator over unpersisted events. Source code in event_sourcery/event_sourcing/aggregate.py 27 28 29 30 31 32 33 34 35 36 37 38 39 @contextmanager def __persisting_changes__ ( self ) -> Iterator [ Iterator [ Event ]]: \"\"\" Context manager for accessing and clearing unpersisted events. Yields an iterator over all events that have been applied but not yet persisted. After exiting the context, the list of unpersisted events is cleared. Returns: Iterator[Iterator[Event]]: Iterator over unpersisted events. \"\"\" yield iter ( getattr ( self , \"_changes\" , [])) self . _changes = []","title":"Aggregate"},{"location":"reference/event_sourcing/Aggregate/#event_sourcery.event_sourcing.Aggregate.__apply__","text":"Applies a single event to mutate the aggregate's state. This method must be implemented by subclasses to define how each event type changes the aggregate's state. Parameters: Name Type Description Default event Event The event to apply. required Source code in event_sourcery/event_sourcing/aggregate.py 41 42 43 44 45 46 47 48 49 50 51 def __apply__ ( self , event : Event ) -> None : \"\"\" Applies a single event to mutate the aggregate's state. This method must be implemented by subclasses to define how each event type changes the aggregate's state. Args: event (Event): The event to apply. \"\"\" raise NotImplementedError","title":"__apply__"},{"location":"reference/event_sourcing/Aggregate/#event_sourcery.event_sourcing.Aggregate.__persisting_changes__","text":"Context manager for accessing and clearing unpersisted events. Yields an iterator over all events that have been applied but not yet persisted. After exiting the context, the list of unpersisted events is cleared. Returns: Type Description Iterator [ Iterator [ Event ]] Iterator[Iterator[Event]]: Iterator over unpersisted events. Source code in event_sourcery/event_sourcing/aggregate.py 27 28 29 30 31 32 33 34 35 36 37 38 39 @contextmanager def __persisting_changes__ ( self ) -> Iterator [ Iterator [ Event ]]: \"\"\" Context manager for accessing and clearing unpersisted events. Yields an iterator over all events that have been applied but not yet persisted. After exiting the context, the list of unpersisted events is cleared. Returns: Iterator[Iterator[Event]]: Iterator over unpersisted events. \"\"\" yield iter ( getattr ( self , \"_changes\" , [])) self . _changes = []","title":"__persisting_changes__"},{"location":"reference/event_sourcing/Repository/","text":"Bases: Generic [ TAggregate ] Repository for event-sourced aggregates. Provides loading and persisting of aggregates using an event store. Handles event replay to reconstruct aggregate state and persists new events emitted by the aggregate. Source code in event_sourcery/event_sourcing/repository.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class Repository ( Generic [ TAggregate ]): \"\"\" Repository for event-sourced aggregates. Provides loading and persisting of aggregates using an event store. Handles event replay to reconstruct aggregate state and persists new events emitted by the aggregate. \"\"\" def __init__ ( self , event_store : EventStore ) -> None : self . _event_store = event_store @contextmanager def aggregate ( self , uuid : StreamUUID , aggregate : TAggregate , ) -> Iterator [ TAggregate ]: \"\"\" Context manager for loading an aggregate instance. Loads the aggregate's event stream, replays events to reconstruct its state, yields the aggregate for use, and persists any new events emitted during the context. Args: uuid (StreamUUID): The unique identifier of the aggregate's stream. aggregate (TAggregate): The aggregate initial instance to load state into. Yields: TAggregate: The loaded and ready-to-use aggregate instance. \"\"\" stream_id = StreamId ( uuid = uuid , name = uuid . name , category = aggregate . category ) old_version = self . _load ( stream_id , aggregate ) yield aggregate self . _save ( aggregate , old_version , stream_id ) def _load ( self , stream_id : StreamId , aggregate : TAggregate ) -> int : stream = self . _event_store . load_stream ( stream_id ) last_version = 0 for envelope in stream : aggregate . __apply__ ( envelope . event ) last_version = cast ( int , envelope . version ) return last_version def _save ( self , aggregate : TAggregate , old_version : int , stream_id : StreamId , ) -> None : with aggregate . __persisting_changes__ () as pending : start_from = old_version + 1 events = [ WrappedEvent . wrap ( event , version ) for version , event in enumerate ( pending , start = start_from ) ] if not events : return self . _event_store . append ( * events , stream_id = stream_id , expected_version = old_version , ) aggregate ( uuid , aggregate ) Context manager for loading an aggregate instance. Loads the aggregate's event stream, replays events to reconstruct its state, yields the aggregate for use, and persists any new events emitted during the context. Parameters: Name Type Description Default uuid StreamUUID The unique identifier of the aggregate's stream. required aggregate TAggregate The aggregate initial instance to load state into. required Yields: Name Type Description TAggregate TAggregate The loaded and ready-to-use aggregate instance. Source code in event_sourcery/event_sourcing/repository.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @contextmanager def aggregate ( self , uuid : StreamUUID , aggregate : TAggregate , ) -> Iterator [ TAggregate ]: \"\"\" Context manager for loading an aggregate instance. Loads the aggregate's event stream, replays events to reconstruct its state, yields the aggregate for use, and persists any new events emitted during the context. Args: uuid (StreamUUID): The unique identifier of the aggregate's stream. aggregate (TAggregate): The aggregate initial instance to load state into. Yields: TAggregate: The loaded and ready-to-use aggregate instance. \"\"\" stream_id = StreamId ( uuid = uuid , name = uuid . name , category = aggregate . category ) old_version = self . _load ( stream_id , aggregate ) yield aggregate self . _save ( aggregate , old_version , stream_id )","title":"Repository"},{"location":"reference/event_sourcing/Repository/#event_sourcery.event_sourcing.Repository.aggregate","text":"Context manager for loading an aggregate instance. Loads the aggregate's event stream, replays events to reconstruct its state, yields the aggregate for use, and persists any new events emitted during the context. Parameters: Name Type Description Default uuid StreamUUID The unique identifier of the aggregate's stream. required aggregate TAggregate The aggregate initial instance to load state into. required Yields: Name Type Description TAggregate TAggregate The loaded and ready-to-use aggregate instance. Source code in event_sourcery/event_sourcing/repository.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @contextmanager def aggregate ( self , uuid : StreamUUID , aggregate : TAggregate , ) -> Iterator [ TAggregate ]: \"\"\" Context manager for loading an aggregate instance. Loads the aggregate's event stream, replays events to reconstruct its state, yields the aggregate for use, and persists any new events emitted during the context. Args: uuid (StreamUUID): The unique identifier of the aggregate's stream. aggregate (TAggregate): The aggregate initial instance to load state into. Yields: TAggregate: The loaded and ready-to-use aggregate instance. \"\"\" stream_id = StreamId ( uuid = uuid , name = uuid . name , category = aggregate . category ) old_version = self . _load ( stream_id , aggregate ) yield aggregate self . _save ( aggregate , old_version , stream_id )","title":"aggregate"},{"location":"reference/event_store/DEFAULT_TENANT/","text":"No tenant is represented by an DEFAULT_TENANT variable.","title":"DEFAULT_TENANT"},{"location":"reference/event_store/EventStore/","text":"API for working with events. Source code in event_sourcery/_event_store/event_store.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 class EventStore : \"\"\"API for working with events.\"\"\" def __init__ ( self , storage_strategy : StorageStrategy , serde : Serde ) -> None : self . _storage_strategy = storage_strategy self . _serde = serde def load_stream ( self , stream_id : StreamId , start : int | None = None , stop : int | None = None , ) -> Sequence [ WrappedEvent ]: \"\"\"Loads events from a given stream. Examples: >>> event_store.load_stream(stream_id=StreamId(name=\"not_existing_stream\")) [] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\")) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\"), start=2, stop=3) [WrappedEvent(..., version=2)] Args: stream_id: The stream identifier to load events from. start: The stream version to start loading from (including). stop: The stream version to stop loading at (excluding). Returns: A sequence of events or empty list if the stream doesn't exist. \"\"\" events = self . _storage_strategy . fetch_events ( stream_id , start = start , stop = stop ) return self . _serde . deserialize_many ( events ) @singledispatchmethod def append ( self , first : WrappedEvent , * events : WrappedEvent , stream_id : StreamId , expected_version : int | Versioning = 0 , ) -> None : \"\"\"Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store.append(WrappedEvent(...), stream_id=StreamId()) None >>> event_store.append(WrappedEvent(...), stream_id=StreamId(), expected_version=1) None Args: first: The first event to append (WrappedEvent or Event). *events: The rest of the events to append (same type as first argument). stream_id: The stream identifier to append events to. expected_version: The expected version of the stream Returns: None \"\"\" self . _append ( stream_id = stream_id , events = ( first , * events ), expected_version = expected_version , ) @append . register def _append_events ( self , * events : Event , stream_id : StreamId , expected_version : int | Versioning = 0 , ) -> None : wrapped_events = self . _wrap_events ( expected_version , events ) self . append ( * wrapped_events , stream_id = stream_id , expected_version = expected_version , ) @singledispatchmethod def _wrap_events ( self , expected_version : int , events : Sequence [ Event ], ) -> Sequence [ WrappedEvent ]: return [ WrappedEvent . wrap ( event = event , version = version ) for version , event in enumerate ( events , start = expected_version + 1 ) ] @_wrap_events . register def _wrap_events_versioning ( self , expected_version : Versioning , events : Sequence [ Event ] ) -> Sequence [ WrappedEvent ]: return [ WrappedEvent . wrap ( event = event , version = None ) for event in events ] def _append ( self , stream_id : StreamId , events : Sequence [ WrappedEvent ], expected_version : int | Versioning , ) -> None : new_version = events [ - 1 ] . version versioning : Versioning if expected_version is not NO_VERSIONING : versioning = ExplicitVersioning ( expected_version = cast ( int , expected_version ), initial_version = cast ( int , new_version ), ) else : versioning = NO_VERSIONING self . _storage_strategy . insert_events ( stream_id = stream_id , versioning = versioning , events = self . _serde . serialize_many ( events , stream_id ), ) def delete_stream ( self , stream_id : StreamId ) -> None : \"\"\"Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store.delete_stream(StreamId()) None >>> event_store.delete_stream(StreamId(name=\"not_existing_stream\")) None Args: stream_id: The stream identifier to delete. Returns: None \"\"\" self . _storage_strategy . delete_stream ( stream_id ) def save_snapshot ( self , stream_id : StreamId , snapshot : WrappedEvent ) -> None : \"\"\"Saves a snapshot of the stream. Examples: >>> event_store.save_snapshot(StreamId(), WrappedEvent(...)) None >>> event_store.save_snapshot(StreamId(name=\"not_existing_stream\"), WrappedEvent(...)) None Args: stream_id: The stream identifier to save the snapshot. snapshot: The snapshot to save. Returns: None \"\"\" serialized = self . _serde . serialize ( event = snapshot , stream_id = stream_id ) self . _storage_strategy . save_snapshot ( serialized ) @property def position ( self ) -> Position | None : \"\"\"Returns the current position of the event store. Examples: >>> event_store.position None # nothing was saved yet >>> event_store.position Position(15) # Some events were saved \"\"\" return self . _storage_strategy . current_position position : Position | None property Returns the current position of the event store. Examples: >>> event_store . position None # nothing was saved yet >>> event_store . position Position(15) # Some events were saved append ( first , * events , stream_id , expected_version = 0 ) Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store . append ( WrappedEvent ( ... ), stream_id = StreamId ()) None >>> event_store . append ( WrappedEvent ( ... ), stream_id = StreamId (), expected_version = 1 ) None Parameters: Name Type Description Default first WrappedEvent The first event to append (WrappedEvent or Event). required *events WrappedEvent The rest of the events to append (same type as first argument). () stream_id StreamId The stream identifier to append events to. required expected_version int | Versioning The expected version of the stream 0 Returns: Type Description None None Source code in event_sourcery/_event_store/event_store.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 @singledispatchmethod def append ( self , first : WrappedEvent , * events : WrappedEvent , stream_id : StreamId , expected_version : int | Versioning = 0 , ) -> None : \"\"\"Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store.append(WrappedEvent(...), stream_id=StreamId()) None >>> event_store.append(WrappedEvent(...), stream_id=StreamId(), expected_version=1) None Args: first: The first event to append (WrappedEvent or Event). *events: The rest of the events to append (same type as first argument). stream_id: The stream identifier to append events to. expected_version: The expected version of the stream Returns: None \"\"\" self . _append ( stream_id = stream_id , events = ( first , * events ), expected_version = expected_version , ) delete_stream ( stream_id ) Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store . delete_stream ( StreamId ()) None >>> event_store . delete_stream ( StreamId ( name = \"not_existing_stream\" )) None Parameters: Name Type Description Default stream_id StreamId The stream identifier to delete. required Returns: Type Description None None Source code in event_sourcery/_event_store/event_store.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def delete_stream ( self , stream_id : StreamId ) -> None : \"\"\"Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store.delete_stream(StreamId()) None >>> event_store.delete_stream(StreamId(name=\"not_existing_stream\")) None Args: stream_id: The stream identifier to delete. Returns: None \"\"\" self . _storage_strategy . delete_stream ( stream_id ) load_stream ( stream_id , start = None , stop = None ) Loads events from a given stream. Examples: >>> event_store . load_stream ( stream_id = StreamId ( name = \"not_existing_stream\" )) [] >>> event_store . load_stream ( stream_id = StreamId ( name = \"existing_stream\" )) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store . load_stream ( stream_id = StreamId ( name = \"existing_stream\" ), start = 2 , stop = 3 ) [WrappedEvent(..., version=2)] Parameters: Name Type Description Default stream_id StreamId The stream identifier to load events from. required start int | None The stream version to start loading from (including). None stop int | None The stream version to stop loading at (excluding). None Returns: Type Description Sequence [ WrappedEvent ] A sequence of events or empty list if the stream doesn't exist. Source code in event_sourcery/_event_store/event_store.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def load_stream ( self , stream_id : StreamId , start : int | None = None , stop : int | None = None , ) -> Sequence [ WrappedEvent ]: \"\"\"Loads events from a given stream. Examples: >>> event_store.load_stream(stream_id=StreamId(name=\"not_existing_stream\")) [] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\")) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\"), start=2, stop=3) [WrappedEvent(..., version=2)] Args: stream_id: The stream identifier to load events from. start: The stream version to start loading from (including). stop: The stream version to stop loading at (excluding). Returns: A sequence of events or empty list if the stream doesn't exist. \"\"\" events = self . _storage_strategy . fetch_events ( stream_id , start = start , stop = stop ) return self . _serde . deserialize_many ( events ) save_snapshot ( stream_id , snapshot ) Saves a snapshot of the stream. Examples: >>> event_store . save_snapshot ( StreamId (), WrappedEvent ( ... )) None >>> event_store . save_snapshot ( StreamId ( name = \"not_existing_stream\" ), WrappedEvent ( ... )) None Parameters: Name Type Description Default stream_id StreamId The stream identifier to save the snapshot. required snapshot WrappedEvent The snapshot to save. required Returns: Type Description None None Source code in event_sourcery/_event_store/event_store.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def save_snapshot ( self , stream_id : StreamId , snapshot : WrappedEvent ) -> None : \"\"\"Saves a snapshot of the stream. Examples: >>> event_store.save_snapshot(StreamId(), WrappedEvent(...)) None >>> event_store.save_snapshot(StreamId(name=\"not_existing_stream\"), WrappedEvent(...)) None Args: stream_id: The stream identifier to save the snapshot. snapshot: The snapshot to save. Returns: None \"\"\" serialized = self . _serde . serialize ( event = snapshot , stream_id = stream_id ) self . _storage_strategy . save_snapshot ( serialized )","title":"EventStore"},{"location":"reference/event_store/EventStore/#event_sourcery.EventStore.position","text":"Returns the current position of the event store. Examples: >>> event_store . position None # nothing was saved yet >>> event_store . position Position(15) # Some events were saved","title":"position"},{"location":"reference/event_store/EventStore/#event_sourcery.EventStore.append","text":"Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store . append ( WrappedEvent ( ... ), stream_id = StreamId ()) None >>> event_store . append ( WrappedEvent ( ... ), stream_id = StreamId (), expected_version = 1 ) None Parameters: Name Type Description Default first WrappedEvent The first event to append (WrappedEvent or Event). required *events WrappedEvent The rest of the events to append (same type as first argument). () stream_id StreamId The stream identifier to append events to. required expected_version int | Versioning The expected version of the stream 0 Returns: Type Description None None Source code in event_sourcery/_event_store/event_store.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 @singledispatchmethod def append ( self , first : WrappedEvent , * events : WrappedEvent , stream_id : StreamId , expected_version : int | Versioning = 0 , ) -> None : \"\"\"Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store.append(WrappedEvent(...), stream_id=StreamId()) None >>> event_store.append(WrappedEvent(...), stream_id=StreamId(), expected_version=1) None Args: first: The first event to append (WrappedEvent or Event). *events: The rest of the events to append (same type as first argument). stream_id: The stream identifier to append events to. expected_version: The expected version of the stream Returns: None \"\"\" self . _append ( stream_id = stream_id , events = ( first , * events ), expected_version = expected_version , )","title":"append"},{"location":"reference/event_store/EventStore/#event_sourcery.EventStore.delete_stream","text":"Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store . delete_stream ( StreamId ()) None >>> event_store . delete_stream ( StreamId ( name = \"not_existing_stream\" )) None Parameters: Name Type Description Default stream_id StreamId The stream identifier to delete. required Returns: Type Description None None Source code in event_sourcery/_event_store/event_store.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def delete_stream ( self , stream_id : StreamId ) -> None : \"\"\"Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store.delete_stream(StreamId()) None >>> event_store.delete_stream(StreamId(name=\"not_existing_stream\")) None Args: stream_id: The stream identifier to delete. Returns: None \"\"\" self . _storage_strategy . delete_stream ( stream_id )","title":"delete_stream"},{"location":"reference/event_store/EventStore/#event_sourcery.EventStore.load_stream","text":"Loads events from a given stream. Examples: >>> event_store . load_stream ( stream_id = StreamId ( name = \"not_existing_stream\" )) [] >>> event_store . load_stream ( stream_id = StreamId ( name = \"existing_stream\" )) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store . load_stream ( stream_id = StreamId ( name = \"existing_stream\" ), start = 2 , stop = 3 ) [WrappedEvent(..., version=2)] Parameters: Name Type Description Default stream_id StreamId The stream identifier to load events from. required start int | None The stream version to start loading from (including). None stop int | None The stream version to stop loading at (excluding). None Returns: Type Description Sequence [ WrappedEvent ] A sequence of events or empty list if the stream doesn't exist. Source code in event_sourcery/_event_store/event_store.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def load_stream ( self , stream_id : StreamId , start : int | None = None , stop : int | None = None , ) -> Sequence [ WrappedEvent ]: \"\"\"Loads events from a given stream. Examples: >>> event_store.load_stream(stream_id=StreamId(name=\"not_existing_stream\")) [] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\")) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\"), start=2, stop=3) [WrappedEvent(..., version=2)] Args: stream_id: The stream identifier to load events from. start: The stream version to start loading from (including). stop: The stream version to stop loading at (excluding). Returns: A sequence of events or empty list if the stream doesn't exist. \"\"\" events = self . _storage_strategy . fetch_events ( stream_id , start = start , stop = stop ) return self . _serde . deserialize_many ( events )","title":"load_stream"},{"location":"reference/event_store/EventStore/#event_sourcery.EventStore.save_snapshot","text":"Saves a snapshot of the stream. Examples: >>> event_store . save_snapshot ( StreamId (), WrappedEvent ( ... )) None >>> event_store . save_snapshot ( StreamId ( name = \"not_existing_stream\" ), WrappedEvent ( ... )) None Parameters: Name Type Description Default stream_id StreamId The stream identifier to save the snapshot. required snapshot WrappedEvent The snapshot to save. required Returns: Type Description None None Source code in event_sourcery/_event_store/event_store.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def save_snapshot ( self , stream_id : StreamId , snapshot : WrappedEvent ) -> None : \"\"\"Saves a snapshot of the stream. Examples: >>> event_store.save_snapshot(StreamId(), WrappedEvent(...)) None >>> event_store.save_snapshot(StreamId(name=\"not_existing_stream\"), WrappedEvent(...)) None Args: stream_id: The stream identifier to save the snapshot. snapshot: The snapshot to save. Returns: None \"\"\" serialized = self . _serde . serialize ( event = snapshot , stream_id = stream_id ) self . _storage_strategy . save_snapshot ( serialized )","title":"save_snapshot"},{"location":"reference/event_store/NO_VERSIONING/","text":"","title":"NO_VERSIONING"},{"location":"reference/event_store/StreamCategory/","text":"Represents a logical grouping or classification of event streams. StreamCategory is typically used to organize streams by aggregate type, business domain, or other criteria. It allows for efficient querying, filtering, and subscription to related streams as a group. In most implementations, it is simply an alias for str , but its semantic meaning is important for event sourcing patterns such as category-based subscriptions.","title":"StreamCategory"},{"location":"reference/event_store/StreamId/","text":"Bases: StreamUUID Identifier for an event stream, with optional category support. Extends StreamUUID by adding a category attribute, allowing grouping of streams (e.g., by aggregate type or business domain). Used as the primary identifier for event streams in the event store, supporting both UUID-based and name-based addressing. Attributes: Name Type Description category StreamCategory | None Optional category for grouping streams. Source code in event_sourcery/_event_store/stream_id.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 @dataclass ( frozen = True , repr = False , eq = False ) class StreamId ( StreamUUID ): \"\"\" Identifier for an event stream, with optional category support. Extends StreamUUID by adding a category attribute, allowing grouping of streams (e.g., by aggregate type or business domain). Used as the primary identifier for event streams in the event store, supporting both UUID-based and name-based addressing. Attributes: category (StreamCategory | None): Optional category for grouping streams. \"\"\" category : StreamCategory | None = None def __repr__ ( self ) -> str : return ( f \" { type ( self ) . __name__ } \" f \"(hex= { self !s} , name= { self . name } , category= { self . category } )\" ) def __eq__ ( self , other : Any ) -> bool : if isinstance ( other , StreamId ): return super () . __eq__ ( other ) and self . category == other . category return NotImplemented def __hash__ ( self ) -> int : return hash (( self . category , super () . __hash__ ()))","title":"StreamId"},{"location":"reference/event_store/StreamUUID/","text":"Bases: UUID Universally unique identifier for event streams. Extends the standard UUID to support deterministic generation from stream names using a fixed namespace. Allows referencing streams by UUID or by name, ensuring consistent mapping between names and UUIDs. Used as a base for stream identification. Attributes: Name Type Description NAMESPACE UUID Namespace used for deterministic name-based UUID generation. uuid UUID | None Create from explicit UUID for the stream. name str | None Create from name for deterministic UUID generation. from_hex str | None Create from hex string to construct the UUID. Source code in event_sourcery/_event_store/stream_id.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @dataclass ( frozen = True , repr = False , eq = False ) class StreamUUID ( UUID ): \"\"\" Universally unique identifier for event streams. Extends the standard UUID to support deterministic generation from stream names using a fixed namespace. Allows referencing streams by UUID or by name, ensuring consistent mapping between names and UUIDs. Used as a base for stream identification. Attributes: NAMESPACE (UUID): Namespace used for deterministic name-based UUID generation. uuid (UUID | None): Create from explicit UUID for the stream. name (str | None): Create from name for deterministic UUID generation. from_hex (str | None): Create from hex string to construct the UUID. \"\"\" NAMESPACE = UUID ( \"3a24a3ee-d33d-4266-93ab-7d8e256a6d44\" ) uuid : InitVar [ UUID | None ] = None name : str | None = None from_hex : InitVar [ str | None ] = None def __post_init__ ( self , uuid : UUID | None , from_hex : str | None ) -> None : if uuid is not None : super () . __init__ ( bytes = uuid . bytes ) elif self . name is not None : super () . __init__ ( bytes = self . _from_name ( self . name ) . bytes ) elif from_hex is not None : super () . __init__ ( hex = from_hex ) else : super () . __init__ ( bytes = uuid4 () . bytes ) if self . name and ( expected := self . _from_name ( self . name )) != self : raise IncompatibleUuidAndName ( self , expected , self . name ) def _from_name ( self , name : str ) -> UUID : return uuid5 ( self . NAMESPACE , name ) def __repr__ ( self ) -> str : return f \" { type ( self ) . __name__ } (hex= { self !s} , name= { self . name } )\"","title":"StreamUUID"},{"location":"reference/event_store/TenantId/","text":"String-based type for tenant identifiers in multi-tenant. Used to distinguish tenants in Event Sourcery backends.","title":"TenantId"},{"location":"reference/event_store/exceptions/","text":"ConcurrentStreamWriteError Bases: EventStoreException Source code in event_sourcery/exceptions.py 9 10 class ConcurrentStreamWriteError ( EventStoreException ): pass AnotherStreamWithThisNameButOtherIdExists Bases: EventStoreException Source code in event_sourcery/exceptions.py 13 14 class AnotherStreamWithThisNameButOtherIdExists ( EventStoreException ): pass IllegalCategoryName Bases: EventStoreException Source code in event_sourcery/exceptions.py 17 18 class IllegalCategoryName ( EventStoreException ): pass IllegalTenantId Bases: EventStoreException Source code in event_sourcery/exceptions.py 21 22 class IllegalTenantId ( EventStoreException ): pass ExpectedVersionUsedOnVersionlessStream Bases: VersioningMismatch Source code in event_sourcery/exceptions.py 29 30 class ExpectedVersionUsedOnVersionlessStream ( VersioningMismatch ): pass NoExpectedVersionGivenOnVersionedStream Bases: VersioningMismatch Source code in event_sourcery/exceptions.py 33 34 class NoExpectedVersionGivenOnVersionedStream ( VersioningMismatch ): pass IncompatibleUuidAndName Bases: EventStoreException Source code in event_sourcery/exceptions.py 37 38 39 40 41 @dataclass class IncompatibleUuidAndName ( EventStoreException ): received : UUID expected : UUID name : str ClassModuleUnavailable Bases: Exception Source code in event_sourcery/exceptions.py 44 45 class ClassModuleUnavailable ( Exception ): pass DuplicatedEvent Bases: Exception Source code in event_sourcery/exceptions.py 48 49 class DuplicatedEvent ( Exception ): pass KeyNotFoundError Bases: PrivacyError Source code in event_sourcery/exceptions.py 56 57 58 @dataclass class KeyNotFoundError ( PrivacyError ): subject_id : str NoSubjectIdFound Bases: PrivacyError Source code in event_sourcery/exceptions.py 61 62 63 @dataclass class NoSubjectIdFound ( PrivacyError ): stream_id : UUID NoProviderConfigured Bases: Exception Source code in event_sourcery/exceptions.py 66 67 class NoProviderConfigured ( Exception ): pass","title":"exceptions"},{"location":"reference/event_store/exceptions/#concurrentstreamwriteerror","text":"Bases: EventStoreException Source code in event_sourcery/exceptions.py 9 10 class ConcurrentStreamWriteError ( EventStoreException ): pass","title":"ConcurrentStreamWriteError"},{"location":"reference/event_store/exceptions/#anotherstreamwiththisnamebutotheridexists","text":"Bases: EventStoreException Source code in event_sourcery/exceptions.py 13 14 class AnotherStreamWithThisNameButOtherIdExists ( EventStoreException ): pass","title":"AnotherStreamWithThisNameButOtherIdExists"},{"location":"reference/event_store/exceptions/#illegalcategoryname","text":"Bases: EventStoreException Source code in event_sourcery/exceptions.py 17 18 class IllegalCategoryName ( EventStoreException ): pass","title":"IllegalCategoryName"},{"location":"reference/event_store/exceptions/#illegaltenantid","text":"Bases: EventStoreException Source code in event_sourcery/exceptions.py 21 22 class IllegalTenantId ( EventStoreException ): pass","title":"IllegalTenantId"},{"location":"reference/event_store/exceptions/#expectedversionusedonversionlessstream","text":"Bases: VersioningMismatch Source code in event_sourcery/exceptions.py 29 30 class ExpectedVersionUsedOnVersionlessStream ( VersioningMismatch ): pass","title":"ExpectedVersionUsedOnVersionlessStream"},{"location":"reference/event_store/exceptions/#noexpectedversiongivenonversionedstream","text":"Bases: VersioningMismatch Source code in event_sourcery/exceptions.py 33 34 class NoExpectedVersionGivenOnVersionedStream ( VersioningMismatch ): pass","title":"NoExpectedVersionGivenOnVersionedStream"},{"location":"reference/event_store/exceptions/#incompatibleuuidandname","text":"Bases: EventStoreException Source code in event_sourcery/exceptions.py 37 38 39 40 41 @dataclass class IncompatibleUuidAndName ( EventStoreException ): received : UUID expected : UUID name : str","title":"IncompatibleUuidAndName"},{"location":"reference/event_store/exceptions/#classmoduleunavailable","text":"Bases: Exception Source code in event_sourcery/exceptions.py 44 45 class ClassModuleUnavailable ( Exception ): pass","title":"ClassModuleUnavailable"},{"location":"reference/event_store/exceptions/#duplicatedevent","text":"Bases: Exception Source code in event_sourcery/exceptions.py 48 49 class DuplicatedEvent ( Exception ): pass","title":"DuplicatedEvent"},{"location":"reference/event_store/exceptions/#keynotfounderror","text":"Bases: PrivacyError Source code in event_sourcery/exceptions.py 56 57 58 @dataclass class KeyNotFoundError ( PrivacyError ): subject_id : str","title":"KeyNotFoundError"},{"location":"reference/event_store/exceptions/#nosubjectidfound","text":"Bases: PrivacyError Source code in event_sourcery/exceptions.py 61 62 63 @dataclass class NoSubjectIdFound ( PrivacyError ): stream_id : UUID","title":"NoSubjectIdFound"},{"location":"reference/event_store/exceptions/#noproviderconfigured","text":"Bases: Exception Source code in event_sourcery/exceptions.py 66 67 class NoProviderConfigured ( Exception ): pass","title":"NoProviderConfigured"},{"location":"reference/event_store/backend/Backend/","text":"Bases: _Container Dependency Injection container for Event Sourcery components. Dict-like object resolving object instance during access by type. Providers are passed setting the type as key and provider as a value. By default, basic implementations are registered or an exception is raised if backend configuration (e.g., storage strategy) is required. Core components are exposed by properties. Source code in event_sourcery/_event_store/backend.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 class Backend ( _Container ): \"\"\" Dependency Injection container for Event Sourcery components. Dict-like object resolving object instance during access by type. Providers are passed setting the type as key and provider as a value. By default, basic implementations are registered or an exception is raised if backend configuration (e.g., storage strategy) is required. Core components are exposed by properties. \"\"\" def __init__ ( self ) -> None : super () . __init__ () self [ TenantId ] = DEFAULT_TENANT self [ EventRegistry ] = EventRegistry () self [ EncryptionStrategy ] = NoEncryptionStrategy () self [ EncryptionKeyStorageStrategy ] = ( lambda c : NoKeyStorageStrategy () . scoped_for_tenant ( c [ TenantId ]) ) self [ Encryption ] = lambda c : Encryption ( registry = c [ EventRegistry ], strategy = c [ EncryptionStrategy ], key_storage = c [ EncryptionKeyStorageStrategy ], ) self [ Serde ] = lambda c : Serde ( registry = c [ EventRegistry ], encryption = c [ Encryption ], ) self [ StorageStrategy ] = not_configured ( \"Use one of pyES backends: SQLAlchemy, Django or KurrentDB\" , ) self [ EventStore ] = lambda c : EventStore ( storage_strategy = c [ StorageStrategy ], serde = c [ Serde ], ) self [ Outbox ] = lambda c : Outbox ( strategy = c [ OutboxStorageStrategy ], serde = c [ Serde ], ) self [ OutboxStorageStrategy ] = lambda _ : NoOutboxStorageStrategy () self [ SubscriptionStrategy ] = not_configured ( \"Use one of pyES backends: SQLAlchemy, Django or KurrentDB\" , ) self [ PositionPhase ] = lambda c : SubscriptionBuilder ( c [ Serde ], c [ SubscriptionStrategy ], ) @property def event_store ( self ) -> EventStore : \"\"\" Returns the current instance of `EventStore`. \"\"\" return self [ EventStore ] @property def outbox ( self ) -> Outbox : \"\"\" Returns the current instance of `Outbox`. \"\"\" return self [ Outbox ] @property def subscriber ( self ) -> PositionPhase : \"\"\" Returns the current instance of `SubscriptionBuilder` (as `PositionPhase`). \"\"\" return self [ PositionPhase ] def in_tenant_mode ( self , tenant_id : TenantId ) -> Self : \"\"\" Returns a copy of the backend with the specified tenant ID set. \"\"\" in_tenant_mode = self . copy () in_tenant_mode [ TenantId ] = tenant_id return in_tenant_mode def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : \"\"\" Configure the outbox with a custom filter. \"\"\" raise NotImplementedError () def with_encryption ( self , strategy : EncryptionStrategy , key_storage : EncryptionKeyStorageStrategy , ) -> Self : \"\"\" Configures event encryption with the provided strategy and key storage. \"\"\" self [ EncryptionStrategy ] = strategy self [ EncryptionKeyStorageStrategy ] = lambda c : key_storage . scoped_for_tenant ( c [ TenantId ], ) return self event_store : EventStore property Returns the current instance of EventStore . outbox : Outbox property Returns the current instance of Outbox . subscriber : PositionPhase property Returns the current instance of SubscriptionBuilder (as PositionPhase ). in_tenant_mode ( tenant_id ) Returns a copy of the backend with the specified tenant ID set. Source code in event_sourcery/_event_store/backend.py 181 182 183 184 185 186 187 def in_tenant_mode ( self , tenant_id : TenantId ) -> Self : \"\"\" Returns a copy of the backend with the specified tenant ID set. \"\"\" in_tenant_mode = self . copy () in_tenant_mode [ TenantId ] = tenant_id return in_tenant_mode with_encryption ( strategy , key_storage ) Configures event encryption with the provided strategy and key storage. Source code in event_sourcery/_event_store/backend.py 195 196 197 198 199 200 201 202 203 204 205 206 207 def with_encryption ( self , strategy : EncryptionStrategy , key_storage : EncryptionKeyStorageStrategy , ) -> Self : \"\"\" Configures event encryption with the provided strategy and key storage. \"\"\" self [ EncryptionStrategy ] = strategy self [ EncryptionKeyStorageStrategy ] = lambda c : key_storage . scoped_for_tenant ( c [ TenantId ], ) return self with_outbox ( filterer = no_filter ) Configure the outbox with a custom filter. Source code in event_sourcery/_event_store/backend.py 189 190 191 192 193 def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : \"\"\" Configure the outbox with a custom filter. \"\"\" raise NotImplementedError ()","title":"Backend"},{"location":"reference/event_store/backend/Backend/#event_sourcery.backend.Backend.event_store","text":"Returns the current instance of EventStore .","title":"event_store"},{"location":"reference/event_store/backend/Backend/#event_sourcery.backend.Backend.outbox","text":"Returns the current instance of Outbox .","title":"outbox"},{"location":"reference/event_store/backend/Backend/#event_sourcery.backend.Backend.subscriber","text":"Returns the current instance of SubscriptionBuilder (as PositionPhase ).","title":"subscriber"},{"location":"reference/event_store/backend/Backend/#event_sourcery.backend.Backend.in_tenant_mode","text":"Returns a copy of the backend with the specified tenant ID set. Source code in event_sourcery/_event_store/backend.py 181 182 183 184 185 186 187 def in_tenant_mode ( self , tenant_id : TenantId ) -> Self : \"\"\" Returns a copy of the backend with the specified tenant ID set. \"\"\" in_tenant_mode = self . copy () in_tenant_mode [ TenantId ] = tenant_id return in_tenant_mode","title":"in_tenant_mode"},{"location":"reference/event_store/backend/Backend/#event_sourcery.backend.Backend.with_encryption","text":"Configures event encryption with the provided strategy and key storage. Source code in event_sourcery/_event_store/backend.py 195 196 197 198 199 200 201 202 203 204 205 206 207 def with_encryption ( self , strategy : EncryptionStrategy , key_storage : EncryptionKeyStorageStrategy , ) -> Self : \"\"\" Configures event encryption with the provided strategy and key storage. \"\"\" self [ EncryptionStrategy ] = strategy self [ EncryptionKeyStorageStrategy ] = lambda c : key_storage . scoped_for_tenant ( c [ TenantId ], ) return self","title":"with_encryption"},{"location":"reference/event_store/backend/Backend/#event_sourcery.backend.Backend.with_outbox","text":"Configure the outbox with a custom filter. Source code in event_sourcery/_event_store/backend.py 189 190 191 192 193 def with_outbox ( self , filterer : OutboxFiltererStrategy = no_filter ) -> Self : \"\"\" Configure the outbox with a custom filter. \"\"\" raise NotImplementedError ()","title":"with_outbox"},{"location":"reference/event_store/backend/TransactionalBackend/","text":"Bases: Backend Backend variant that provides transactional event handling support. Registers additional services for managing listeners and dispatchers to enable in-transaction event processing. Properties in_transaction (Listeners): The current state of in transaction Listeners. Source code in event_sourcery/_event_store/backend.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 class TransactionalBackend ( Backend ): \"\"\" Backend variant that provides transactional event handling support. Registers additional services for managing listeners and dispatchers to enable in-transaction event processing. Properties: in_transaction (Listeners): The current state of in transaction Listeners. \"\"\" def __init__ ( self ) -> None : super () . __init__ () self [ Listeners ] = singleton ( lambda _ : Listeners ()) self [ Dispatcher ] = lambda c : Dispatcher ( c [ Serde ], c [ Listeners ]) @property def in_transaction ( self ) -> Listeners : \"\"\" Returns the current instance of `Listeners` for transactional event handling. \"\"\" return cast ( Listeners , self [ Listeners ]) in_transaction : Listeners property Returns the current instance of Listeners for transactional event handling.","title":"TransactionalBackend"},{"location":"reference/event_store/backend/TransactionalBackend/#event_sourcery.backend.TransactionalBackend.in_transaction","text":"Returns the current instance of Listeners for transactional event handling.","title":"in_transaction"},{"location":"reference/event_store/backend/not_configured/","text":"Used as a placeholder for providers that require configuration. Parameters: Name Type Description Default error_message str str. The error message to raise with the exception. required Source code in event_sourcery/_event_store/backend.py 71 72 73 74 75 76 77 78 79 80 81 82 def not_configured ( error_message : str ) -> _Provider [ T ]: \"\"\" Used as a placeholder for providers that require configuration. Args: error_message (str): str. The error message to raise with the exception. \"\"\" def _raise ( container : \"_Container\" ) -> NoReturn : raise NoProviderConfigured ( error_message ) # pragma: no cover return _raise","title":"not_configured"},{"location":"reference/event_store/backend/singleton/","text":"Decorator that ensures the provider returns the same instance (singleton) for every call within the container's lifecycle. Parameters: Name Type Description Default provider _Provider [ T ] The provider function to wrap. required Returns: Type Description _Provider [ T ] _Provider[T]: A provider that always returns the same instance. Source code in event_sourcery/_event_store/backend.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def singleton ( provider : _Provider [ T ]) -> _Provider [ T ]: \"\"\" Decorator that ensures the provider returns the same instance (singleton) for every call within the container's lifecycle. Args: provider (_Provider[T]): The provider function to wrap. Returns: _Provider[T]: A provider that always returns the same instance. \"\"\" result : T | None = None @wraps ( provider ) def _wrapper ( container : \"_Container\" ) -> T : nonlocal result if result is not None : return result result = provider ( container ) return result return _wrapper","title":"singleton"},{"location":"reference/event_store/encryption/DataSubject/","text":"Marker class for identifying the data subject in event models. Used as a field annotation to indicate which attribute of an event model represents the subject whose data is being encrypted. This allows the encryption system to resolve the subject identifier for key management and crypto-shredding.","title":"DataSubject"},{"location":"reference/event_store/encryption/Encrypted/","text":"Field annotation for marking event fields that should be encrypted. This annotation is used in combination with DataSubject to implement crypto-shredding, allowing for GDPR-compliant data removal in event-sourced systems. Parameters: Name Type Description Default mask_value Any Value to display when the data is shredded (deleted). Must match the field type. required subject_field str | None Optional. Name of the field containing the subject ID for this encrypted field. If not provided, the field marked with DataSubject will be used. None Example class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] # Custom subject field other_data : Annotated [ str , Encrypted ( mask_value = \"[HIDDEN]\" , subject_field = \"alternative_id\" , ), ] Note Every event with encrypted fields must have exactly one DataSubject field. The mask_value type must match the field type. After shredding, the field will always return its mask_value. Source code in event_sourcery/_event_store/event/dto.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 @dataclass ( frozen = True ) class Encrypted : \"\"\"Field annotation for marking event fields that should be encrypted. This annotation is used in combination with DataSubject to implement crypto-shredding, allowing for GDPR-compliant data removal in event-sourced systems. Args: mask_value: Value to display when the data is shredded (deleted). Must match the field type. subject_field: Optional. Name of the field containing the subject ID for this encrypted field. If not provided, the field marked with DataSubject will be used. Example: ```python class UserRegistered(Event): user_id: Annotated[str, DataSubject] email: Annotated[str, Encrypted(mask_value=\"[REDACTED]\")] # Custom subject field other_data: Annotated[ str, Encrypted( mask_value=\"[HIDDEN]\", subject_field=\"alternative_id\", ), ] ``` Note: - Every event with encrypted fields must have exactly one DataSubject field. - The mask_value type must match the field type. - After shredding, the field will always return its mask_value. \"\"\" mask_value : Any subject_field : str | None = None","title":"Encrypted"},{"location":"reference/event_store/encryption/Encryption/","text":"Integrates encryption logic and key management. Uses the KeyStorageStrategy and EncryptionStrategy interfaces. Responsible for: - Retrieving the key for a given subject_id - Encrypting and decrypting data - Masking data when the key is not available (crypto-shredding) - Processing event encryption/decryption with field annotations Source code in event_sourcery/_event_store/event/encryption.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @dataclass class Encryption : \"\"\" Integrates encryption logic and key management. Uses the KeyStorageStrategy and EncryptionStrategy interfaces. Responsible for: - Retrieving the key for a given subject_id - Encrypting and decrypting data - Masking data when the key is not available (crypto-shredding) - Processing event encryption/decryption with field annotations \"\"\" registry : EventRegistry strategy : EncryptionStrategy key_storage : EncryptionKeyStorageStrategy def encrypt ( self , event : BaseModel , stream_id : StreamId ) -> dict [ str , Any ]: \"\"\" Encrypts all fields of the event marked as encrypted. Args: event (BaseModel): The event instance to encrypt. stream_id (StreamId): The stream identifier used for subject resolution. Returns: dict[str, Any]: The event data with encrypted fields. Raises: NoSubjectIdFound: If the subject id cannot be determined for encryption. KeyNotFoundError: If the encryption key for a subject is missing. \"\"\" event_type = type ( event ) data = NestedDict ( event . model_dump ( mode = \"json\" )) for field_name in self . registry . encrypted_fields ( of = event_type ) . keys (): subject_field = self . registry . subject_filed ( field_name , of = event_type ) subject_id = ( subject_field and getattr ( event , subject_field ) ) or stream_id . name if subject_id is None : raise NoSubjectIdFound ( stream_id ) data [ field_name ] = self . _encrypt_value ( data [ field_name ], subject_id ) return data . data def decrypt ( self , event_type : type [ BaseModel ], raw : dict [ str , Any ], stream_id : StreamId , ) -> dict [ str , Any ]: \"\"\" Decrypts all fields of the event marked as encrypted in the registry. Args: event_type (type[BaseModel]): The event class type. raw (dict[str, Any]): The raw event data with encrypted fields. stream_id (StreamId): The stream identifier used for subject resolution. Returns: dict[str, Any]: The event data with decrypted fields (or masked if no key). \"\"\" data = NestedDict ( raw ) encrypted_fields = self . registry . encrypted_fields ( of = event_type ) for field_name , encrypted_config in encrypted_fields . items (): subject_field = self . registry . subject_filed ( field_name , of = event_type ) subject_id = data [ subject_field ] if subject_field else stream_id . name or \"\" data [ field_name ] = self . _decrypt_value ( data [ field_name ], subject_id , encrypted_config . mask_value , ) return data . data def _decrypt_value ( self , value : str , subject_id : str , mask_value : Any ) -> Any : key = self . key_storage . get ( subject_id ) if key is None : return mask_value decrypted = self . strategy . decrypt ( value , key ) try : return json . loads ( decrypted ) except ( json . JSONDecodeError , TypeError ): return decrypted def _encrypt_value ( self , value : Any , subject_id : str ) -> str : key = self . key_storage . get ( subject_id ) if key is None : raise KeyNotFoundError ( subject_id ) match value : case str (): serialized = value case _ : serialized = json . dumps ( value ) return self . strategy . encrypt ( serialized , key ) def shred ( self , subject_id : str ) -> None : \"\"\" Deletes the encryption key for the given subject, effectively making all encrypted data for that subject unrecoverable (crypto-shredding). Args: subject_id (str): The subject identifier whose key should be deleted. \"\"\" self . key_storage . delete ( subject_id ) decrypt ( event_type , raw , stream_id ) Decrypts all fields of the event marked as encrypted in the registry. Parameters: Name Type Description Default event_type type [ BaseModel ] The event class type. required raw dict [ str , Any ] The raw event data with encrypted fields. required stream_id StreamId The stream identifier used for subject resolution. required Returns: Type Description dict [ str , Any ] dict[str, Any]: The event data with decrypted fields (or masked if no key). Source code in event_sourcery/_event_store/event/encryption.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def decrypt ( self , event_type : type [ BaseModel ], raw : dict [ str , Any ], stream_id : StreamId , ) -> dict [ str , Any ]: \"\"\" Decrypts all fields of the event marked as encrypted in the registry. Args: event_type (type[BaseModel]): The event class type. raw (dict[str, Any]): The raw event data with encrypted fields. stream_id (StreamId): The stream identifier used for subject resolution. Returns: dict[str, Any]: The event data with decrypted fields (or masked if no key). \"\"\" data = NestedDict ( raw ) encrypted_fields = self . registry . encrypted_fields ( of = event_type ) for field_name , encrypted_config in encrypted_fields . items (): subject_field = self . registry . subject_filed ( field_name , of = event_type ) subject_id = data [ subject_field ] if subject_field else stream_id . name or \"\" data [ field_name ] = self . _decrypt_value ( data [ field_name ], subject_id , encrypted_config . mask_value , ) return data . data encrypt ( event , stream_id ) Encrypts all fields of the event marked as encrypted. Parameters: Name Type Description Default event BaseModel The event instance to encrypt. required stream_id StreamId The stream identifier used for subject resolution. required Returns: Type Description dict [ str , Any ] dict[str, Any]: The event data with encrypted fields. Raises: Type Description NoSubjectIdFound If the subject id cannot be determined for encryption. KeyNotFoundError If the encryption key for a subject is missing. Source code in event_sourcery/_event_store/event/encryption.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def encrypt ( self , event : BaseModel , stream_id : StreamId ) -> dict [ str , Any ]: \"\"\" Encrypts all fields of the event marked as encrypted. Args: event (BaseModel): The event instance to encrypt. stream_id (StreamId): The stream identifier used for subject resolution. Returns: dict[str, Any]: The event data with encrypted fields. Raises: NoSubjectIdFound: If the subject id cannot be determined for encryption. KeyNotFoundError: If the encryption key for a subject is missing. \"\"\" event_type = type ( event ) data = NestedDict ( event . model_dump ( mode = \"json\" )) for field_name in self . registry . encrypted_fields ( of = event_type ) . keys (): subject_field = self . registry . subject_filed ( field_name , of = event_type ) subject_id = ( subject_field and getattr ( event , subject_field ) ) or stream_id . name if subject_id is None : raise NoSubjectIdFound ( stream_id ) data [ field_name ] = self . _encrypt_value ( data [ field_name ], subject_id ) return data . data shred ( subject_id ) Deletes the encryption key for the given subject, effectively making all encrypted data for that subject unrecoverable (crypto-shredding). Parameters: Name Type Description Default subject_id str The subject identifier whose key should be deleted. required Source code in event_sourcery/_event_store/event/encryption.py 238 239 240 241 242 243 244 245 246 def shred ( self , subject_id : str ) -> None : \"\"\" Deletes the encryption key for the given subject, effectively making all encrypted data for that subject unrecoverable (crypto-shredding). Args: subject_id (str): The subject identifier whose key should be deleted. \"\"\" self . key_storage . delete ( subject_id )","title":"Encryption"},{"location":"reference/event_store/encryption/Encryption/#event_sourcery.encryption.Encryption.decrypt","text":"Decrypts all fields of the event marked as encrypted in the registry. Parameters: Name Type Description Default event_type type [ BaseModel ] The event class type. required raw dict [ str , Any ] The raw event data with encrypted fields. required stream_id StreamId The stream identifier used for subject resolution. required Returns: Type Description dict [ str , Any ] dict[str, Any]: The event data with decrypted fields (or masked if no key). Source code in event_sourcery/_event_store/event/encryption.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def decrypt ( self , event_type : type [ BaseModel ], raw : dict [ str , Any ], stream_id : StreamId , ) -> dict [ str , Any ]: \"\"\" Decrypts all fields of the event marked as encrypted in the registry. Args: event_type (type[BaseModel]): The event class type. raw (dict[str, Any]): The raw event data with encrypted fields. stream_id (StreamId): The stream identifier used for subject resolution. Returns: dict[str, Any]: The event data with decrypted fields (or masked if no key). \"\"\" data = NestedDict ( raw ) encrypted_fields = self . registry . encrypted_fields ( of = event_type ) for field_name , encrypted_config in encrypted_fields . items (): subject_field = self . registry . subject_filed ( field_name , of = event_type ) subject_id = data [ subject_field ] if subject_field else stream_id . name or \"\" data [ field_name ] = self . _decrypt_value ( data [ field_name ], subject_id , encrypted_config . mask_value , ) return data . data","title":"decrypt"},{"location":"reference/event_store/encryption/Encryption/#event_sourcery.encryption.Encryption.encrypt","text":"Encrypts all fields of the event marked as encrypted. Parameters: Name Type Description Default event BaseModel The event instance to encrypt. required stream_id StreamId The stream identifier used for subject resolution. required Returns: Type Description dict [ str , Any ] dict[str, Any]: The event data with encrypted fields. Raises: Type Description NoSubjectIdFound If the subject id cannot be determined for encryption. KeyNotFoundError If the encryption key for a subject is missing. Source code in event_sourcery/_event_store/event/encryption.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def encrypt ( self , event : BaseModel , stream_id : StreamId ) -> dict [ str , Any ]: \"\"\" Encrypts all fields of the event marked as encrypted. Args: event (BaseModel): The event instance to encrypt. stream_id (StreamId): The stream identifier used for subject resolution. Returns: dict[str, Any]: The event data with encrypted fields. Raises: NoSubjectIdFound: If the subject id cannot be determined for encryption. KeyNotFoundError: If the encryption key for a subject is missing. \"\"\" event_type = type ( event ) data = NestedDict ( event . model_dump ( mode = \"json\" )) for field_name in self . registry . encrypted_fields ( of = event_type ) . keys (): subject_field = self . registry . subject_filed ( field_name , of = event_type ) subject_id = ( subject_field and getattr ( event , subject_field ) ) or stream_id . name if subject_id is None : raise NoSubjectIdFound ( stream_id ) data [ field_name ] = self . _encrypt_value ( data [ field_name ], subject_id ) return data . data","title":"encrypt"},{"location":"reference/event_store/encryption/Encryption/#event_sourcery.encryption.Encryption.shred","text":"Deletes the encryption key for the given subject, effectively making all encrypted data for that subject unrecoverable (crypto-shredding). Parameters: Name Type Description Default subject_id str The subject identifier whose key should be deleted. required Source code in event_sourcery/_event_store/event/encryption.py 238 239 240 241 242 243 244 245 246 def shred ( self , subject_id : str ) -> None : \"\"\" Deletes the encryption key for the given subject, effectively making all encrypted data for that subject unrecoverable (crypto-shredding). Args: subject_id (str): The subject identifier whose key should be deleted. \"\"\" self . key_storage . delete ( subject_id )","title":"shred"},{"location":"reference/event_store/encryption/NoEncryptionStrategy/","text":"Bases: EncryptionStrategy Source code in event_sourcery/_event_store/event/encryption.py 104 105 106 107 108 109 class NoEncryptionStrategy ( EncryptionStrategy ): def encrypt ( self , data : Any , key : bytes ) -> str : raise NotImplementedError ( \"NoEncryptionStrategy does not support encryption.\" ) def decrypt ( self , data : str , key : bytes ) -> Any : raise NotImplementedError ( \"NoEncryptionStrategy does not support decryption.\" )","title":"NoEncryptionStrategy"},{"location":"reference/event_store/encryption/NoKeyStorageStrategy/","text":"Bases: EncryptionKeyStorageStrategy Source code in event_sourcery/_event_store/event/encryption.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class NoKeyStorageStrategy ( EncryptionKeyStorageStrategy ): def get ( self , subject_id : str ) -> bytes | None : raise NotImplementedError ( \"NoKeyStorageStrategy does not support key retrieval.\" ) def store ( self , subject_id : str , key : bytes ) -> None : raise NotImplementedError ( \"NoKeyStorageStrategy does not support key storage.\" ) def delete ( self , subject_id : str ) -> None : raise NotImplementedError ( \"NoKeyStorageStrategy does not support key deletion.\" ) def scoped_for_tenant ( self , tenant_id : TenantId ) -> Self : return self","title":"NoKeyStorageStrategy"},{"location":"reference/event_store/event/Context/","text":"Bases: BaseModel Extensible context object for event metadata. Designed to carry correlation and causation identifiers, and can be extended with additional metadata fields as needed. This allows for flexible propagation of context information throughout event processing and handling pipelines. Attributes: Name Type Description correlation_id UUID | None Identifier for correlating related events. causation_id UUID | None Identifier for the cause of the event. Additional fields can be added dynamically due to extra=\"allow\" . Source code in event_sourcery/_event_store/event/dto.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class Context ( BaseModel , extra = \"allow\" ): \"\"\" Extensible context object for event metadata. Designed to carry correlation and causation identifiers, and can be extended with additional metadata fields as needed. This allows for flexible propagation of context information throughout event processing and handling pipelines. Attributes: correlation_id (UUID | None): Identifier for correlating related events. causation_id (UUID | None): Identifier for the cause of the event. Additional fields can be added dynamically due to `extra=\"allow\"`. \"\"\" correlation_id : UUID | None = None causation_id : UUID | None = None","title":"Context"},{"location":"reference/event_store/event/Entry/","text":"Container for a single event and its stream identifier. Used as a base DTO for representing an event (with all metadata) and the stream it belongs to. Serves as a base for more specialized event store DTOs (e.g. Recorded). Attributes: Name Type Description wrapped_event WrappedEvent [ Event ] The event instance with metadata. stream_id StreamId The identifier of the stream to which the event belongs. Source code in event_sourcery/_event_store/event/dto.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 @dataclasses . dataclass ( frozen = True ) class Entry : \"\"\" Container for a single event and its stream identifier. Used as a base DTO for representing an event (with all metadata) and the stream it belongs to. Serves as a base for more specialized event store DTOs (e.g. Recorded). Attributes: wrapped_event (WrappedEvent[Event]): The event instance with metadata. stream_id (StreamId): The identifier of the stream to which the event belongs. \"\"\" wrapped_event : WrappedEvent [ Event ] stream_id : StreamId","title":"Entry"},{"location":"reference/event_store/event/Event/","text":"Bases: BaseModel Base class for all events. Example usage: class OrderCancelled(Event): order_id: OrderId Source code in event_sourcery/_event_store/event/dto.py 64 65 66 67 68 69 70 71 72 class Event ( BaseModel , extra = \"forbid\" ): \"\"\"Base class for all events. Example usage: ``` class OrderCancelled(Event): order_id: OrderId ``` \"\"\"","title":"Event"},{"location":"reference/event_store/event/Position/","text":"Represents the position (offset) of an event within an event stream or the event store. Used to track the order of events and support event replay, pagination, and stream navigation. Typically implemented as an integer value incremented with each new event.","title":"Position"},{"location":"reference/event_store/event/RawEvent/","text":"Bases: BaseModel Data Transfer Object representing a raw event that will be stored. Contains all event metadata and payload in a flat structure. Used for low-level event store operations, serialization, and transport. Attributes: Name Type Description uuid UUID Unique identifier of the event. stream_id StreamId Identifier of the stream to which the event belongs. created_at datetime Timestamp when the event was created. name str Name/type of the event. data dict [ str , Any ] Event payload (fields and values). context dict [ str , Any ] Event custom metadata. version int | None Version of the event in the stream (if applicable). Source code in event_sourcery/_event_store/event/dto.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @dataclasses . dataclass ( frozen = True ) class RawEvent ( BaseModel ): \"\"\" Data Transfer Object representing a raw event that will be stored. Contains all event metadata and payload in a flat structure. Used for low-level event store operations, serialization, and transport. Attributes: uuid (UUID): Unique identifier of the event. stream_id (StreamId): Identifier of the stream to which the event belongs. created_at (datetime): Timestamp when the event was created. name (str): Name/type of the event. data (dict[str, Any]): Event payload (fields and values). context (dict[str, Any]): Event custom metadata. version (int | None): Version of the event in the stream (if applicable). \"\"\" uuid : UUID stream_id : StreamId created_at : datetime name : str data : dict [ str , Any ] context : dict [ str , Any ] version : int | None = None","title":"RawEvent"},{"location":"reference/event_store/event/Recorded/","text":"Bases: Entry DTO containing events saved in the event store. It contains position of the event in the event store and tenant id. Source code in event_sourcery/_event_store/event/dto.py 151 152 153 154 155 156 157 158 159 @dataclasses . dataclass ( frozen = True ) class Recorded ( Entry ): \"\"\"DTO containing events saved in the event store. It contains position of the event in the event store and tenant id. \"\"\" position : Position tenant_id : TenantId = DEFAULT_TENANT","title":"Recorded"},{"location":"reference/event_store/event/RecordedRaw/","text":"Bases: BaseModel DTO representing a raw event received from the event store. Used for low-level event store operations. Wraps a RawEvent with additional metadata (position and tenant id). Attributes: Name Type Description entry RawEvent The raw event data and metadata. position Position Position of the event in the event store. tenant_id TenantId Tenant identifier (for multi-tenant scenarios). Source code in event_sourcery/_event_store/event/dto.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @dataclasses . dataclass ( frozen = True ) class RecordedRaw ( BaseModel ): \"\"\" DTO representing a raw event received from the event store. Used for low-level event store operations. Wraps a RawEvent with additional metadata (position and tenant id). Attributes: entry (RawEvent): The raw event data and metadata. position (Position): Position of the event in the event store. tenant_id (TenantId): Tenant identifier (for multi-tenant scenarios). \"\"\" entry : RawEvent position : Position tenant_id : TenantId = DEFAULT_TENANT","title":"RecordedRaw"},{"location":"reference/event_store/event/WrappedEvent/","text":"Bases: Generic [ TEvent ] Wrapper for events with all relevant metadata. Returned from EventStore when loading events from a stream. Attributes: Name Type Description event TEvent The event instance. version int | None Version of the event in the stream (if applicable). uuid UUID Unique identifier of the event instance. created_at datetime Timestamp when the event was created. context Context Context object containing additional metadata. Example usage: class OrderCancelled(Event): order_id: OrderId event = OrderCancelled(order_id=OrderId(\"#123\")) wrapped_event = WrappedEvent.wrap(event, version=1) Source code in event_sourcery/_event_store/event/dto.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @dataclasses . dataclass () class WrappedEvent ( Generic [ TEvent ]): \"\"\" Wrapper for events with all relevant metadata. Returned from EventStore when loading events from a stream. Attributes: event (TEvent): The event instance. version (int | None): Version of the event in the stream (if applicable). uuid (UUID): Unique identifier of the event instance. created_at (datetime): Timestamp when the event was created. context (Context): Context object containing additional metadata. Example usage: ``` class OrderCancelled(Event): order_id: OrderId event = OrderCancelled(order_id=OrderId(\"#123\")) wrapped_event = WrappedEvent.wrap(event, version=1) ``` \"\"\" event : TEvent version : int | None uuid : UUID = dataclasses . field ( default_factory = uuid4 ) created_at : datetime = dataclasses . field ( default_factory = lambda : datetime . now ( timezone . utc ) . replace ( tzinfo = None ) ) context : Context = dataclasses . field ( default_factory = Context ) @classmethod def wrap ( cls , event : TEvent , version : int | None ) -> \"WrappedEvent[TEvent]\" : return WrappedEvent [ TEvent ]( event = event , version = version )","title":"WrappedEvent"},{"location":"reference/event_store/in_transaction/Dispatcher/","text":"Dispatches events to registered listeners. Used for in-transaction (in-process) event processing. Source code in event_sourcery/_event_store/subscription/in_transaction.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class Dispatcher : \"\"\" Dispatches events to registered listeners. Used for in-transaction (in-process) event processing. \"\"\" def __init__ ( self , serde : Serde , listeners : Listeners ) -> None : self . _serde = serde self . _listeners = listeners def dispatch ( self , * raws : RecordedRaw ) -> None : \"\"\" Dispatches one or more raw event records to all registered listeners. Args: *raws (RecordedRaw): One or more events to dispatch. \"\"\" for raw in raws : record = self . _serde . deserialize_record ( raw ) event_type = record . wrapped_event . event . __class__ for listener in self . _listeners [ event_type ]: listener ( record . wrapped_event , record . stream_id , record . tenant_id , record . position , ) dispatch ( * raws ) Dispatches one or more raw event records to all registered listeners. Parameters: Name Type Description Default *raws RecordedRaw One or more events to dispatch. () Source code in event_sourcery/_event_store/subscription/in_transaction.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def dispatch ( self , * raws : RecordedRaw ) -> None : \"\"\" Dispatches one or more raw event records to all registered listeners. Args: *raws (RecordedRaw): One or more events to dispatch. \"\"\" for raw in raws : record = self . _serde . deserialize_record ( raw ) event_type = record . wrapped_event . event . __class__ for listener in self . _listeners [ event_type ]: listener ( record . wrapped_event , record . stream_id , record . tenant_id , record . position , )","title":"Dispatcher"},{"location":"reference/event_store/in_transaction/Dispatcher/#event_sourcery.in_transaction.Dispatcher.dispatch","text":"Dispatches one or more raw event records to all registered listeners. Parameters: Name Type Description Default *raws RecordedRaw One or more events to dispatch. () Source code in event_sourcery/_event_store/subscription/in_transaction.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def dispatch ( self , * raws : RecordedRaw ) -> None : \"\"\" Dispatches one or more raw event records to all registered listeners. Args: *raws (RecordedRaw): One or more events to dispatch. \"\"\" for raw in raws : record = self . _serde . deserialize_record ( raw ) event_type = record . wrapped_event . event . __class__ for listener in self . _listeners [ event_type ]: listener ( record . wrapped_event , record . stream_id , record . tenant_id , record . position , )","title":"dispatch"},{"location":"reference/event_store/in_transaction/Listener/","text":"Bases: Protocol Protocol for event listeners. Source code in event_sourcery/_event_store/subscription/in_transaction.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class Listener ( Protocol ): \"\"\" Protocol for event listeners. \"\"\" def __call__ ( self , wrapped_event : WrappedEvent , stream_id : StreamId , tenant_id : TenantId , position : Position , ) -> None : \"\"\" Used to process events as they are dispatched. Args: wrapped_event (WrappedEvent): The event instance with metadata. stream_id (StreamId): The stream identifier. tenant_id (TenantId): The tenant identifier. position (Position): The position of the event in the stream. \"\"\" ... __call__ ( wrapped_event , stream_id , tenant_id , position ) Used to process events as they are dispatched. Parameters: Name Type Description Default wrapped_event WrappedEvent The event instance with metadata. required stream_id StreamId The stream identifier. required tenant_id TenantId The tenant identifier. required position Position The position of the event in the stream. required Source code in event_sourcery/_event_store/subscription/in_transaction.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __call__ ( self , wrapped_event : WrappedEvent , stream_id : StreamId , tenant_id : TenantId , position : Position , ) -> None : \"\"\" Used to process events as they are dispatched. Args: wrapped_event (WrappedEvent): The event instance with metadata. stream_id (StreamId): The stream identifier. tenant_id (TenantId): The tenant identifier. position (Position): The position of the event in the stream. \"\"\" ...","title":"Listener"},{"location":"reference/event_store/in_transaction/Listener/#event_sourcery.in_transaction.Listener.__call__","text":"Used to process events as they are dispatched. Parameters: Name Type Description Default wrapped_event WrappedEvent The event instance with metadata. required stream_id StreamId The stream identifier. required tenant_id TenantId The tenant identifier. required position Position The position of the event in the stream. required Source code in event_sourcery/_event_store/subscription/in_transaction.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __call__ ( self , wrapped_event : WrappedEvent , stream_id : StreamId , tenant_id : TenantId , position : Position , ) -> None : \"\"\" Used to process events as they are dispatched. Args: wrapped_event (WrappedEvent): The event instance with metadata. stream_id (StreamId): The stream identifier. tenant_id (TenantId): The tenant identifier. position (Position): The position of the event in the stream. \"\"\" ...","title":"__call__"},{"location":"reference/event_store/in_transaction/Listeners/","text":"Registry for event listeners by event type. Maintains a mapping from event types to sets of listeners. Allows registering listeners for specific event types and retrieving all listeners for a given event (including base types). Source code in event_sourcery/_event_store/subscription/in_transaction.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class Listeners : \"\"\" Registry for event listeners by event type. Maintains a mapping from event types to sets of listeners. Allows registering listeners for specific event types and retrieving all listeners for a given event (including base types). \"\"\" def __init__ ( self ) -> None : self . _listeners : dict [ type [ Event ], set [ Listener ]] = {} def __getitem__ ( self , event_type : type [ Event ]) -> Iterator [ Listener ]: \"\"\" Returns an iterator over all listeners registered for the given event type or its base types. Args: event_type (type[Event]): The event type to look up listeners for. Returns: Iterator[Listener]: An iterator over matching listeners. \"\"\" return chain ( * ( listeners for registered_to , listeners in self . _listeners . items () if issubclass ( event_type , registered_to ) ) ) def register ( self , listener : Listener , to : type [ Event ]) -> None : \"\"\" Registers a listener for a specific event type. Args: listener (Listener): The listener to register. to (type[Event]): The event type to register the listener for. \"\"\" if to not in self . _listeners : self . _listeners [ to ] = set () self . _listeners [ to ] . add ( listener ) def remove ( self , listener : Listener , to : type [ Event ]) -> None : \"\"\" Removes a listener from a specific event type. Args: listener (Listener): The listener to remove. to (type[Event]): The event type to remove the listener from. \"\"\" if to in self . _listeners and listener in self . _listeners [ to ]: self . _listeners [ to ] . remove ( listener ) __getitem__ ( event_type ) Returns an iterator over all listeners registered for the given event type or its base types. Parameters: Name Type Description Default event_type type [ Event ] The event type to look up listeners for. required Returns: Type Description Iterator [ Listener ] Iterator[Listener]: An iterator over matching listeners. Source code in event_sourcery/_event_store/subscription/in_transaction.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __getitem__ ( self , event_type : type [ Event ]) -> Iterator [ Listener ]: \"\"\" Returns an iterator over all listeners registered for the given event type or its base types. Args: event_type (type[Event]): The event type to look up listeners for. Returns: Iterator[Listener]: An iterator over matching listeners. \"\"\" return chain ( * ( listeners for registered_to , listeners in self . _listeners . items () if issubclass ( event_type , registered_to ) ) ) register ( listener , to ) Registers a listener for a specific event type. Parameters: Name Type Description Default listener Listener The listener to register. required to type [ Event ] The event type to register the listener for. required Source code in event_sourcery/_event_store/subscription/in_transaction.py 82 83 84 85 86 87 88 89 90 91 92 def register ( self , listener : Listener , to : type [ Event ]) -> None : \"\"\" Registers a listener for a specific event type. Args: listener (Listener): The listener to register. to (type[Event]): The event type to register the listener for. \"\"\" if to not in self . _listeners : self . _listeners [ to ] = set () self . _listeners [ to ] . add ( listener ) remove ( listener , to ) Removes a listener from a specific event type. Parameters: Name Type Description Default listener Listener The listener to remove. required to type [ Event ] The event type to remove the listener from. required Source code in event_sourcery/_event_store/subscription/in_transaction.py 94 95 96 97 98 99 100 101 102 103 def remove ( self , listener : Listener , to : type [ Event ]) -> None : \"\"\" Removes a listener from a specific event type. Args: listener (Listener): The listener to remove. to (type[Event]): The event type to remove the listener from. \"\"\" if to in self . _listeners and listener in self . _listeners [ to ]: self . _listeners [ to ] . remove ( listener )","title":"Listeners"},{"location":"reference/event_store/in_transaction/Listeners/#event_sourcery.in_transaction.Listeners.__getitem__","text":"Returns an iterator over all listeners registered for the given event type or its base types. Parameters: Name Type Description Default event_type type [ Event ] The event type to look up listeners for. required Returns: Type Description Iterator [ Listener ] Iterator[Listener]: An iterator over matching listeners. Source code in event_sourcery/_event_store/subscription/in_transaction.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __getitem__ ( self , event_type : type [ Event ]) -> Iterator [ Listener ]: \"\"\" Returns an iterator over all listeners registered for the given event type or its base types. Args: event_type (type[Event]): The event type to look up listeners for. Returns: Iterator[Listener]: An iterator over matching listeners. \"\"\" return chain ( * ( listeners for registered_to , listeners in self . _listeners . items () if issubclass ( event_type , registered_to ) ) )","title":"__getitem__"},{"location":"reference/event_store/in_transaction/Listeners/#event_sourcery.in_transaction.Listeners.register","text":"Registers a listener for a specific event type. Parameters: Name Type Description Default listener Listener The listener to register. required to type [ Event ] The event type to register the listener for. required Source code in event_sourcery/_event_store/subscription/in_transaction.py 82 83 84 85 86 87 88 89 90 91 92 def register ( self , listener : Listener , to : type [ Event ]) -> None : \"\"\" Registers a listener for a specific event type. Args: listener (Listener): The listener to register. to (type[Event]): The event type to register the listener for. \"\"\" if to not in self . _listeners : self . _listeners [ to ] = set () self . _listeners [ to ] . add ( listener )","title":"register"},{"location":"reference/event_store/in_transaction/Listeners/#event_sourcery.in_transaction.Listeners.remove","text":"Removes a listener from a specific event type. Parameters: Name Type Description Default listener Listener The listener to remove. required to type [ Event ] The event type to remove the listener from. required Source code in event_sourcery/_event_store/subscription/in_transaction.py 94 95 96 97 98 99 100 101 102 103 def remove ( self , listener : Listener , to : type [ Event ]) -> None : \"\"\" Removes a listener from a specific event type. Args: listener (Listener): The listener to remove. to (type[Event]): The event type to remove the listener from. \"\"\" if to in self . _listeners and listener in self . _listeners [ to ]: self . _listeners [ to ] . remove ( listener )","title":"remove"},{"location":"reference/event_store/interfaces/EncryptionKeyStorageStrategy/","text":"Interface for key management strategies used in event encryption. Implementations are responsible for storing, retrieving, and deleting encryption keys for specific data subjects. Supports multi-tenancy via scoped_for_tenant. Used by the Encryption service for key management and crypto-shredding. Source code in event_sourcery/_event_store/event/encryption.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class EncryptionKeyStorageStrategy : \"\"\" Interface for key management strategies used in event encryption. Implementations are responsible for storing, retrieving, and deleting encryption keys for specific data subjects. Supports multi-tenancy via scoped_for_tenant. Used by the Encryption service for key management and crypto-shredding. \"\"\" def get ( self , subject_id : str ) -> bytes | None : \"\"\" Retrieves the encryption key for the given subject identifier. Args: subject_id (str): The subject identifier. Returns: bytes | None: The encryption key, or None if not found. \"\"\" raise NotImplementedError () def store ( self , subject_id : str , key : bytes ) -> None : \"\"\" Stores the encryption key for the given subject identifier. Args: subject_id (str): The subject identifier. key (bytes): The encryption key to store. \"\"\" raise NotImplementedError () def delete ( self , subject_id : str ) -> None : \"\"\" Deletes the encryption key for the given subject identifier. Args: subject_id (str): The subject identifier whose key should be deleted. \"\"\" raise NotImplementedError () def scoped_for_tenant ( self , tenant_id : TenantId ) -> Self : \"\"\" Returns a key storage strategy instance scoped for the given tenant. Args: tenant_id (TenantId): The tenant identifier. Returns: Self: The key storage strategy instance for the tenant. \"\"\" raise NotImplementedError () delete ( subject_id ) Deletes the encryption key for the given subject identifier. Parameters: Name Type Description Default subject_id str The subject identifier whose key should be deleted. required Source code in event_sourcery/_event_store/event/encryption.py 82 83 84 85 86 87 88 89 def delete ( self , subject_id : str ) -> None : \"\"\" Deletes the encryption key for the given subject identifier. Args: subject_id (str): The subject identifier whose key should be deleted. \"\"\" raise NotImplementedError () get ( subject_id ) Retrieves the encryption key for the given subject identifier. Parameters: Name Type Description Default subject_id str The subject identifier. required Returns: Type Description bytes | None bytes | None: The encryption key, or None if not found. Source code in event_sourcery/_event_store/event/encryption.py 60 61 62 63 64 65 66 67 68 69 70 def get ( self , subject_id : str ) -> bytes | None : \"\"\" Retrieves the encryption key for the given subject identifier. Args: subject_id (str): The subject identifier. Returns: bytes | None: The encryption key, or None if not found. \"\"\" raise NotImplementedError () scoped_for_tenant ( tenant_id ) Returns a key storage strategy instance scoped for the given tenant. Parameters: Name Type Description Default tenant_id TenantId The tenant identifier. required Returns: Name Type Description Self Self The key storage strategy instance for the tenant. Source code in event_sourcery/_event_store/event/encryption.py 91 92 93 94 95 96 97 98 99 100 101 def scoped_for_tenant ( self , tenant_id : TenantId ) -> Self : \"\"\" Returns a key storage strategy instance scoped for the given tenant. Args: tenant_id (TenantId): The tenant identifier. Returns: Self: The key storage strategy instance for the tenant. \"\"\" raise NotImplementedError () store ( subject_id , key ) Stores the encryption key for the given subject identifier. Parameters: Name Type Description Default subject_id str The subject identifier. required key bytes The encryption key to store. required Source code in event_sourcery/_event_store/event/encryption.py 72 73 74 75 76 77 78 79 80 def store ( self , subject_id : str , key : bytes ) -> None : \"\"\" Stores the encryption key for the given subject identifier. Args: subject_id (str): The subject identifier. key (bytes): The encryption key to store. \"\"\" raise NotImplementedError ()","title":"EncryptionKeyStorageStrategy"},{"location":"reference/event_store/interfaces/EncryptionKeyStorageStrategy/#event_sourcery.interfaces.EncryptionKeyStorageStrategy.delete","text":"Deletes the encryption key for the given subject identifier. Parameters: Name Type Description Default subject_id str The subject identifier whose key should be deleted. required Source code in event_sourcery/_event_store/event/encryption.py 82 83 84 85 86 87 88 89 def delete ( self , subject_id : str ) -> None : \"\"\" Deletes the encryption key for the given subject identifier. Args: subject_id (str): The subject identifier whose key should be deleted. \"\"\" raise NotImplementedError ()","title":"delete"},{"location":"reference/event_store/interfaces/EncryptionKeyStorageStrategy/#event_sourcery.interfaces.EncryptionKeyStorageStrategy.get","text":"Retrieves the encryption key for the given subject identifier. Parameters: Name Type Description Default subject_id str The subject identifier. required Returns: Type Description bytes | None bytes | None: The encryption key, or None if not found. Source code in event_sourcery/_event_store/event/encryption.py 60 61 62 63 64 65 66 67 68 69 70 def get ( self , subject_id : str ) -> bytes | None : \"\"\" Retrieves the encryption key for the given subject identifier. Args: subject_id (str): The subject identifier. Returns: bytes | None: The encryption key, or None if not found. \"\"\" raise NotImplementedError ()","title":"get"},{"location":"reference/event_store/interfaces/EncryptionKeyStorageStrategy/#event_sourcery.interfaces.EncryptionKeyStorageStrategy.scoped_for_tenant","text":"Returns a key storage strategy instance scoped for the given tenant. Parameters: Name Type Description Default tenant_id TenantId The tenant identifier. required Returns: Name Type Description Self Self The key storage strategy instance for the tenant. Source code in event_sourcery/_event_store/event/encryption.py 91 92 93 94 95 96 97 98 99 100 101 def scoped_for_tenant ( self , tenant_id : TenantId ) -> Self : \"\"\" Returns a key storage strategy instance scoped for the given tenant. Args: tenant_id (TenantId): The tenant identifier. Returns: Self: The key storage strategy instance for the tenant. \"\"\" raise NotImplementedError ()","title":"scoped_for_tenant"},{"location":"reference/event_store/interfaces/EncryptionKeyStorageStrategy/#event_sourcery.interfaces.EncryptionKeyStorageStrategy.store","text":"Stores the encryption key for the given subject identifier. Parameters: Name Type Description Default subject_id str The subject identifier. required key bytes The encryption key to store. required Source code in event_sourcery/_event_store/event/encryption.py 72 73 74 75 76 77 78 79 80 def store ( self , subject_id : str , key : bytes ) -> None : \"\"\" Stores the encryption key for the given subject identifier. Args: subject_id (str): The subject identifier. key (bytes): The encryption key to store. \"\"\" raise NotImplementedError ()","title":"store"},{"location":"reference/event_store/interfaces/EncryptionStrategy/","text":"Interface for encryption algorithms used to secure event data. Implementations should provide methods for encrypting and decrypting data using a given key. Used by the Encryption service to process event fields marked for encryption. Source code in event_sourcery/_event_store/event/encryption.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class EncryptionStrategy : \"\"\" Interface for encryption algorithms used to secure event data. Implementations should provide methods for encrypting and decrypting data using a given key. Used by the Encryption service to process event fields marked for encryption. \"\"\" def encrypt ( self , data : Any , key : bytes ) -> str : \"\"\" Encrypts the given data using the provided key. Args: data (Any): The data to encrypt. key (bytes): The encryption key. Returns: str: The encrypted data as a string. \"\"\" raise NotImplementedError () def decrypt ( self , data : str , key : bytes ) -> Any : \"\"\" Decrypts the given data using the provided key. Args: data (str): The encrypted data as a string. key (bytes): The encryption key. Returns: Any: The decrypted data. \"\"\" raise NotImplementedError () decrypt ( data , key ) Decrypts the given data using the provided key. Parameters: Name Type Description Default data str The encrypted data as a string. required key bytes The encryption key. required Returns: Name Type Description Any Any The decrypted data. Source code in event_sourcery/_event_store/event/encryption.py 37 38 39 40 41 42 43 44 45 46 47 48 def decrypt ( self , data : str , key : bytes ) -> Any : \"\"\" Decrypts the given data using the provided key. Args: data (str): The encrypted data as a string. key (bytes): The encryption key. Returns: Any: The decrypted data. \"\"\" raise NotImplementedError () encrypt ( data , key ) Encrypts the given data using the provided key. Parameters: Name Type Description Default data Any The data to encrypt. required key bytes The encryption key. required Returns: Name Type Description str str The encrypted data as a string. Source code in event_sourcery/_event_store/event/encryption.py 24 25 26 27 28 29 30 31 32 33 34 35 def encrypt ( self , data : Any , key : bytes ) -> str : \"\"\" Encrypts the given data using the provided key. Args: data (Any): The data to encrypt. key (bytes): The encryption key. Returns: str: The encrypted data as a string. \"\"\" raise NotImplementedError ()","title":"EncryptionStrategy"},{"location":"reference/event_store/interfaces/EncryptionStrategy/#event_sourcery.interfaces.EncryptionStrategy.decrypt","text":"Decrypts the given data using the provided key. Parameters: Name Type Description Default data str The encrypted data as a string. required key bytes The encryption key. required Returns: Name Type Description Any Any The decrypted data. Source code in event_sourcery/_event_store/event/encryption.py 37 38 39 40 41 42 43 44 45 46 47 48 def decrypt ( self , data : str , key : bytes ) -> Any : \"\"\" Decrypts the given data using the provided key. Args: data (str): The encrypted data as a string. key (bytes): The encryption key. Returns: Any: The decrypted data. \"\"\" raise NotImplementedError ()","title":"decrypt"},{"location":"reference/event_store/interfaces/EncryptionStrategy/#event_sourcery.interfaces.EncryptionStrategy.encrypt","text":"Encrypts the given data using the provided key. Parameters: Name Type Description Default data Any The data to encrypt. required key bytes The encryption key. required Returns: Name Type Description str str The encrypted data as a string. Source code in event_sourcery/_event_store/event/encryption.py 24 25 26 27 28 29 30 31 32 33 34 35 def encrypt ( self , data : Any , key : bytes ) -> str : \"\"\" Encrypts the given data using the provided key. Args: data (Any): The data to encrypt. key (bytes): The encryption key. Returns: str: The encrypted data as a string. \"\"\" raise NotImplementedError ()","title":"encrypt"},{"location":"reference/event_store/interfaces/OutboxFiltererStrategy/","text":"Bases: Protocol Protocol for outbox entry filtering strategies. Used to determine whether a Event should be included in the outbox processing. Implementations should return True to include the event, or False to skip it. Source code in event_sourcery/_event_store/outbox.py 13 14 15 16 17 18 19 20 21 22 @runtime_checkable class OutboxFiltererStrategy ( Protocol ): \"\"\" Protocol for outbox entry filtering strategies. Used to determine whether a Event should be included in the outbox processing. Implementations should return True to include the event, or False to skip it. \"\"\" def __call__ ( self , entry : RawEvent ) -> bool : ...","title":"OutboxFiltererStrategy"},{"location":"reference/event_store/interfaces/OutboxStorageStrategy/","text":"Interface for backend outbox storage implementation. Source code in event_sourcery/_event_store/outbox.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class OutboxStorageStrategy : \"\"\" Interface for backend outbox storage implementation. \"\"\" def outbox_entries ( self , limit : int ) -> Iterator [ AbstractContextManager [ RecordedRaw ]]: \"\"\" Returns an iterator over context managers for outbox entries to be published. The context manager ensures transactional processing. If the event is processed without exception, it is removed from the outbox. If an exception occurs, the event remains in the outbox for retry. Args: limit (int): The maximum number of entries to return. Returns: Iterator[AbstractContextManager[RecordedRaw]]: Context managers to wrap record processing \"\"\" raise NotImplementedError () outbox_entries ( limit ) Returns an iterator over context managers for outbox entries to be published. The context manager ensures transactional processing. If the event is processed without exception, it is removed from the outbox. If an exception occurs, the event remains in the outbox for retry. Parameters: Name Type Description Default limit int The maximum number of entries to return. required Returns: Type Description Iterator [ AbstractContextManager [ RecordedRaw ]] Iterator[AbstractContextManager[RecordedRaw]]: Context managers to wrap record processing Source code in event_sourcery/_event_store/outbox.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def outbox_entries ( self , limit : int ) -> Iterator [ AbstractContextManager [ RecordedRaw ]]: \"\"\" Returns an iterator over context managers for outbox entries to be published. The context manager ensures transactional processing. If the event is processed without exception, it is removed from the outbox. If an exception occurs, the event remains in the outbox for retry. Args: limit (int): The maximum number of entries to return. Returns: Iterator[AbstractContextManager[RecordedRaw]]: Context managers to wrap record processing \"\"\" raise NotImplementedError ()","title":"OutboxStorageStrategy"},{"location":"reference/event_store/interfaces/OutboxStorageStrategy/#event_sourcery.interfaces.OutboxStorageStrategy.outbox_entries","text":"Returns an iterator over context managers for outbox entries to be published. The context manager ensures transactional processing. If the event is processed without exception, it is removed from the outbox. If an exception occurs, the event remains in the outbox for retry. Parameters: Name Type Description Default limit int The maximum number of entries to return. required Returns: Type Description Iterator [ AbstractContextManager [ RecordedRaw ]] Iterator[AbstractContextManager[RecordedRaw]]: Context managers to wrap record processing Source code in event_sourcery/_event_store/outbox.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def outbox_entries ( self , limit : int ) -> Iterator [ AbstractContextManager [ RecordedRaw ]]: \"\"\" Returns an iterator over context managers for outbox entries to be published. The context manager ensures transactional processing. If the event is processed without exception, it is removed from the outbox. If an exception occurs, the event remains in the outbox for retry. Args: limit (int): The maximum number of entries to return. Returns: Iterator[AbstractContextManager[RecordedRaw]]: Context managers to wrap record processing \"\"\" raise NotImplementedError ()","title":"outbox_entries"},{"location":"reference/event_store/interfaces/StorageStrategy/","text":"Interface for event store backends. Defines the contract for low-level operations on event streams, such as fetching, inserting, and deleting events. Concrete implementations provide storage logic for specific backends. Source code in event_sourcery/_event_store/event_store.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class StorageStrategy : \"\"\" Interface for event store backends. Defines the contract for low-level operations on event streams, such as fetching, inserting, and deleting events. Concrete implementations provide storage logic for specific backends. \"\"\" def fetch_events ( self , stream_id : StreamId , start : int | None = None , stop : int | None = None , ) -> list [ RawEvent ]: \"\"\" Fetches events from a stream in the given range. Args: stream_id (StreamId): The stream identifier to fetch events from. start (int | None): From version (inclusive), or None for the beginning. stop (int | None): Stop before version (exclusive), or None for the end. Returns: list[RawEvent]: List of raw events in the specified range. \"\"\" raise NotImplementedError () def insert_events ( self , stream_id : StreamId , versioning : Versioning , events : list [ RawEvent ], ) -> None : \"\"\" Inserts events into a stream with using versioning strategy. Args: stream_id (StreamId): The stream identifier to insert events into. versioning (Versioning): Versioning strategy for optimistic locking. events (list[RawEvent]): List of raw events to insert. \"\"\" raise NotImplementedError () def save_snapshot ( self , snapshot : RawEvent ) -> None : \"\"\" Saves a snapshot of the stream. Stream will be fetched from newest snapshot. Args: snapshot (RawEvent): The snapshot event to save. \"\"\" raise NotImplementedError () def delete_stream ( self , stream_id : StreamId ) -> None : \"\"\" Deletes a stream and all its events. Args: stream_id (StreamId): The stream identifier to delete. \"\"\" raise NotImplementedError () @property def current_position ( self ) -> Position | None : \"\"\" Returns the current position (offset) in the event store, if supported. \"\"\" raise NotImplementedError () def scoped_for_tenant ( self , tenant_id : str ) -> Self : \"\"\" Returns a backend instance scoped for the given tenant. Args: tenant_id (str): The tenant identifier. Returns: Self: The backend instance for the tenant. \"\"\" raise NotImplementedError () current_position : Position | None property Returns the current position (offset) in the event store, if supported. delete_stream ( stream_id ) Deletes a stream and all its events. Parameters: Name Type Description Default stream_id StreamId The stream identifier to delete. required Source code in event_sourcery/_event_store/event_store.py 75 76 77 78 79 80 81 82 def delete_stream ( self , stream_id : StreamId ) -> None : \"\"\" Deletes a stream and all its events. Args: stream_id (StreamId): The stream identifier to delete. \"\"\" raise NotImplementedError () fetch_events ( stream_id , start = None , stop = None ) Fetches events from a stream in the given range. Parameters: Name Type Description Default stream_id StreamId The stream identifier to fetch events from. required start int | None From version (inclusive), or None for the beginning. None stop int | None Stop before version (exclusive), or None for the end. None Returns: Type Description list [ RawEvent ] list[RawEvent]: List of raw events in the specified range. Source code in event_sourcery/_event_store/event_store.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def fetch_events ( self , stream_id : StreamId , start : int | None = None , stop : int | None = None , ) -> list [ RawEvent ]: \"\"\" Fetches events from a stream in the given range. Args: stream_id (StreamId): The stream identifier to fetch events from. start (int | None): From version (inclusive), or None for the beginning. stop (int | None): Stop before version (exclusive), or None for the end. Returns: list[RawEvent]: List of raw events in the specified range. \"\"\" raise NotImplementedError () insert_events ( stream_id , versioning , events ) Inserts events into a stream with using versioning strategy. Parameters: Name Type Description Default stream_id StreamId The stream identifier to insert events into. required versioning Versioning Versioning strategy for optimistic locking. required events list [ RawEvent ] List of raw events to insert. required Source code in event_sourcery/_event_store/event_store.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def insert_events ( self , stream_id : StreamId , versioning : Versioning , events : list [ RawEvent ], ) -> None : \"\"\" Inserts events into a stream with using versioning strategy. Args: stream_id (StreamId): The stream identifier to insert events into. versioning (Versioning): Versioning strategy for optimistic locking. events (list[RawEvent]): List of raw events to insert. \"\"\" raise NotImplementedError () save_snapshot ( snapshot ) Saves a snapshot of the stream. Stream will be fetched from newest snapshot. Parameters: Name Type Description Default snapshot RawEvent The snapshot event to save. required Source code in event_sourcery/_event_store/event_store.py 66 67 68 69 70 71 72 73 def save_snapshot ( self , snapshot : RawEvent ) -> None : \"\"\" Saves a snapshot of the stream. Stream will be fetched from newest snapshot. Args: snapshot (RawEvent): The snapshot event to save. \"\"\" raise NotImplementedError () scoped_for_tenant ( tenant_id ) Returns a backend instance scoped for the given tenant. Parameters: Name Type Description Default tenant_id str The tenant identifier. required Returns: Name Type Description Self Self The backend instance for the tenant. Source code in event_sourcery/_event_store/event_store.py 91 92 93 94 95 96 97 98 99 100 101 def scoped_for_tenant ( self , tenant_id : str ) -> Self : \"\"\" Returns a backend instance scoped for the given tenant. Args: tenant_id (str): The tenant identifier. Returns: Self: The backend instance for the tenant. \"\"\" raise NotImplementedError ()","title":"StorageStrategy"},{"location":"reference/event_store/interfaces/StorageStrategy/#event_sourcery.interfaces.StorageStrategy.current_position","text":"Returns the current position (offset) in the event store, if supported.","title":"current_position"},{"location":"reference/event_store/interfaces/StorageStrategy/#event_sourcery.interfaces.StorageStrategy.delete_stream","text":"Deletes a stream and all its events. Parameters: Name Type Description Default stream_id StreamId The stream identifier to delete. required Source code in event_sourcery/_event_store/event_store.py 75 76 77 78 79 80 81 82 def delete_stream ( self , stream_id : StreamId ) -> None : \"\"\" Deletes a stream and all its events. Args: stream_id (StreamId): The stream identifier to delete. \"\"\" raise NotImplementedError ()","title":"delete_stream"},{"location":"reference/event_store/interfaces/StorageStrategy/#event_sourcery.interfaces.StorageStrategy.fetch_events","text":"Fetches events from a stream in the given range. Parameters: Name Type Description Default stream_id StreamId The stream identifier to fetch events from. required start int | None From version (inclusive), or None for the beginning. None stop int | None Stop before version (exclusive), or None for the end. None Returns: Type Description list [ RawEvent ] list[RawEvent]: List of raw events in the specified range. Source code in event_sourcery/_event_store/event_store.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def fetch_events ( self , stream_id : StreamId , start : int | None = None , stop : int | None = None , ) -> list [ RawEvent ]: \"\"\" Fetches events from a stream in the given range. Args: stream_id (StreamId): The stream identifier to fetch events from. start (int | None): From version (inclusive), or None for the beginning. stop (int | None): Stop before version (exclusive), or None for the end. Returns: list[RawEvent]: List of raw events in the specified range. \"\"\" raise NotImplementedError ()","title":"fetch_events"},{"location":"reference/event_store/interfaces/StorageStrategy/#event_sourcery.interfaces.StorageStrategy.insert_events","text":"Inserts events into a stream with using versioning strategy. Parameters: Name Type Description Default stream_id StreamId The stream identifier to insert events into. required versioning Versioning Versioning strategy for optimistic locking. required events list [ RawEvent ] List of raw events to insert. required Source code in event_sourcery/_event_store/event_store.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def insert_events ( self , stream_id : StreamId , versioning : Versioning , events : list [ RawEvent ], ) -> None : \"\"\" Inserts events into a stream with using versioning strategy. Args: stream_id (StreamId): The stream identifier to insert events into. versioning (Versioning): Versioning strategy for optimistic locking. events (list[RawEvent]): List of raw events to insert. \"\"\" raise NotImplementedError ()","title":"insert_events"},{"location":"reference/event_store/interfaces/StorageStrategy/#event_sourcery.interfaces.StorageStrategy.save_snapshot","text":"Saves a snapshot of the stream. Stream will be fetched from newest snapshot. Parameters: Name Type Description Default snapshot RawEvent The snapshot event to save. required Source code in event_sourcery/_event_store/event_store.py 66 67 68 69 70 71 72 73 def save_snapshot ( self , snapshot : RawEvent ) -> None : \"\"\" Saves a snapshot of the stream. Stream will be fetched from newest snapshot. Args: snapshot (RawEvent): The snapshot event to save. \"\"\" raise NotImplementedError ()","title":"save_snapshot"},{"location":"reference/event_store/interfaces/StorageStrategy/#event_sourcery.interfaces.StorageStrategy.scoped_for_tenant","text":"Returns a backend instance scoped for the given tenant. Parameters: Name Type Description Default tenant_id str The tenant identifier. required Returns: Name Type Description Self Self The backend instance for the tenant. Source code in event_sourcery/_event_store/event_store.py 91 92 93 94 95 96 97 98 99 100 101 def scoped_for_tenant ( self , tenant_id : str ) -> Self : \"\"\" Returns a backend instance scoped for the given tenant. Args: tenant_id (str): The tenant identifier. Returns: Self: The backend instance for the tenant. \"\"\" raise NotImplementedError ()","title":"scoped_for_tenant"},{"location":"reference/event_store/interfaces/SubscriptionStrategy/","text":"Interface for event store backend subscription. Defines the contract for subscribing to event streams in various ways. Source code in event_sourcery/_event_store/subscription/interfaces.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class SubscriptionStrategy : \"\"\" Interface for event store backend subscription. Defines the contract for subscribing to event streams in various ways. \"\"\" def subscribe_to_all ( self , start_from : Position , batch_size : int , timelimit : timedelta , ) -> Iterator [ list [ RecordedRaw ]]: \"\"\" Subscribes to all events in the event store, starting from a given position. Args: start_from (Position): The position to start reading events from. batch_size (int): The maximum number of events to return in each batch. timelimit (timedelta): The maximum time to spend reading one batch. Returns: Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. \"\"\" raise NotImplementedError () def subscribe_to_category ( self , start_from : Position , batch_size : int , timelimit : timedelta , category : str , ) -> Iterator [ list [ RecordedRaw ]]: \"\"\" Subscribes to all events in a given category of streams, starting from a given position. Args: start_from (Position): The position to start reading events from. batch_size (int): The maximum number of events to return in each batch. timelimit (timedelta): The maximum time to spend reading one batch. category (str): The category of streams to subscribe to. Returns: Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. \"\"\" raise NotImplementedError () def subscribe_to_events ( self , start_from : Position , batch_size : int , timelimit : timedelta , events : list [ str ], ) -> Iterator [ list [ RecordedRaw ]]: \"\"\" Subscribes to all events of the given event types, starting from a position. Args: start_from (Position): The position to start reading events from. batch_size (int): The maximum number of events to return in each batch. timelimit (timedelta): The maximum time to spend reading one batch. events (list[str]): The list of event type names to subscribe to. Returns: Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. \"\"\" raise NotImplementedError () subscribe_to_all ( start_from , batch_size , timelimit ) Subscribes to all events in the event store, starting from a given position. Parameters: Name Type Description Default start_from Position The position to start reading events from. required batch_size int The maximum number of events to return in each batch. required timelimit timedelta The maximum time to spend reading one batch. required Returns: Type Description Iterator [ list [ RecordedRaw ]] Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. Source code in event_sourcery/_event_store/subscription/interfaces.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def subscribe_to_all ( self , start_from : Position , batch_size : int , timelimit : timedelta , ) -> Iterator [ list [ RecordedRaw ]]: \"\"\" Subscribes to all events in the event store, starting from a given position. Args: start_from (Position): The position to start reading events from. batch_size (int): The maximum number of events to return in each batch. timelimit (timedelta): The maximum time to spend reading one batch. Returns: Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. \"\"\" raise NotImplementedError () subscribe_to_category ( start_from , batch_size , timelimit , category ) Subscribes to all events in a given category of streams, starting from a given position. Parameters: Name Type Description Default start_from Position The position to start reading events from. required batch_size int The maximum number of events to return in each batch. required timelimit timedelta The maximum time to spend reading one batch. required category str The category of streams to subscribe to. required Returns: Type Description Iterator [ list [ RecordedRaw ]] Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. Source code in event_sourcery/_event_store/subscription/interfaces.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def subscribe_to_category ( self , start_from : Position , batch_size : int , timelimit : timedelta , category : str , ) -> Iterator [ list [ RecordedRaw ]]: \"\"\" Subscribes to all events in a given category of streams, starting from a given position. Args: start_from (Position): The position to start reading events from. batch_size (int): The maximum number of events to return in each batch. timelimit (timedelta): The maximum time to spend reading one batch. category (str): The category of streams to subscribe to. Returns: Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. \"\"\" raise NotImplementedError () subscribe_to_events ( start_from , batch_size , timelimit , events ) Subscribes to all events of the given event types, starting from a position. Parameters: Name Type Description Default start_from Position The position to start reading events from. required batch_size int The maximum number of events to return in each batch. required timelimit timedelta The maximum time to spend reading one batch. required events list [ str ] The list of event type names to subscribe to. required Returns: Type Description Iterator [ list [ RecordedRaw ]] Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. Source code in event_sourcery/_event_store/subscription/interfaces.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def subscribe_to_events ( self , start_from : Position , batch_size : int , timelimit : timedelta , events : list [ str ], ) -> Iterator [ list [ RecordedRaw ]]: \"\"\" Subscribes to all events of the given event types, starting from a position. Args: start_from (Position): The position to start reading events from. batch_size (int): The maximum number of events to return in each batch. timelimit (timedelta): The maximum time to spend reading one batch. events (list[str]): The list of event type names to subscribe to. Returns: Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. \"\"\" raise NotImplementedError ()","title":"SubscriptionStrategy"},{"location":"reference/event_store/interfaces/SubscriptionStrategy/#event_sourcery.interfaces.SubscriptionStrategy.subscribe_to_all","text":"Subscribes to all events in the event store, starting from a given position. Parameters: Name Type Description Default start_from Position The position to start reading events from. required batch_size int The maximum number of events to return in each batch. required timelimit timedelta The maximum time to spend reading one batch. required Returns: Type Description Iterator [ list [ RecordedRaw ]] Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. Source code in event_sourcery/_event_store/subscription/interfaces.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def subscribe_to_all ( self , start_from : Position , batch_size : int , timelimit : timedelta , ) -> Iterator [ list [ RecordedRaw ]]: \"\"\" Subscribes to all events in the event store, starting from a given position. Args: start_from (Position): The position to start reading events from. batch_size (int): The maximum number of events to return in each batch. timelimit (timedelta): The maximum time to spend reading one batch. Returns: Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. \"\"\" raise NotImplementedError ()","title":"subscribe_to_all"},{"location":"reference/event_store/interfaces/SubscriptionStrategy/#event_sourcery.interfaces.SubscriptionStrategy.subscribe_to_category","text":"Subscribes to all events in a given category of streams, starting from a given position. Parameters: Name Type Description Default start_from Position The position to start reading events from. required batch_size int The maximum number of events to return in each batch. required timelimit timedelta The maximum time to spend reading one batch. required category str The category of streams to subscribe to. required Returns: Type Description Iterator [ list [ RecordedRaw ]] Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. Source code in event_sourcery/_event_store/subscription/interfaces.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def subscribe_to_category ( self , start_from : Position , batch_size : int , timelimit : timedelta , category : str , ) -> Iterator [ list [ RecordedRaw ]]: \"\"\" Subscribes to all events in a given category of streams, starting from a given position. Args: start_from (Position): The position to start reading events from. batch_size (int): The maximum number of events to return in each batch. timelimit (timedelta): The maximum time to spend reading one batch. category (str): The category of streams to subscribe to. Returns: Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. \"\"\" raise NotImplementedError ()","title":"subscribe_to_category"},{"location":"reference/event_store/interfaces/SubscriptionStrategy/#event_sourcery.interfaces.SubscriptionStrategy.subscribe_to_events","text":"Subscribes to all events of the given event types, starting from a position. Parameters: Name Type Description Default start_from Position The position to start reading events from. required batch_size int The maximum number of events to return in each batch. required timelimit timedelta The maximum time to spend reading one batch. required events list [ str ] The list of event type names to subscribe to. required Returns: Type Description Iterator [ list [ RecordedRaw ]] Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. Source code in event_sourcery/_event_store/subscription/interfaces.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def subscribe_to_events ( self , start_from : Position , batch_size : int , timelimit : timedelta , events : list [ str ], ) -> Iterator [ list [ RecordedRaw ]]: \"\"\" Subscribes to all events of the given event types, starting from a position. Args: start_from (Position): The position to start reading events from. batch_size (int): The maximum number of events to return in each batch. timelimit (timedelta): The maximum time to spend reading one batch. events (list[str]): The list of event type names to subscribe to. Returns: Iterator[list[RecordedRaw]]: An iterator over batches of recorded events. \"\"\" raise NotImplementedError ()","title":"subscribe_to_events"},{"location":"reference/event_store/outbox/NoOutboxStorageStrategy/","text":"Bases: OutboxStorageStrategy Source code in event_sourcery/_event_store/outbox.py 102 103 104 105 106 class NoOutboxStorageStrategy ( OutboxStorageStrategy ): def outbox_entries ( self , limit : int ) -> Iterator [ AbstractContextManager [ RecordedRaw ]]: return iter ([])","title":"NoOutboxStorageStrategy"},{"location":"reference/event_store/outbox/Outbox/","text":"Outbox pattern implementation for reliable event publishing. Uses a storage strategy to fetch outbox entries. Ensures that events are only removed from the outbox if they are processed successfully (i.e., no exception is raised during publishing). If an exception occurs, the event remains in the outbox for retry, providing at-least-once delivery semantics. Parameters: Name Type Description Default strategy OutboxStorageStrategy The backend strategy for outbox storage and entry management. required serde Serde The serializer/deserializer for event records. required Source code in event_sourcery/_event_store/outbox.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Outbox : \"\"\" Outbox pattern implementation for reliable event publishing. Uses a storage strategy to fetch outbox entries. Ensures that events are only removed from the outbox if they are processed successfully (i.e., no exception is raised during publishing). If an exception occurs, the event remains in the outbox for retry, providing at-least-once delivery semantics. Args: strategy (OutboxStorageStrategy): The backend strategy for outbox storage and entry management. serde (Serde): The serializer/deserializer for event records. \"\"\" def __init__ ( self , strategy : OutboxStorageStrategy , serde : Serde ) -> None : self . _strategy = strategy self . _serde = serde def run ( self , publisher : Callable [[ Recorded ], None ], limit : int = 100 , ) -> None : \"\"\" Processes and publishes outbox entries using the provided publisher function. Fetches entries from the outbox (up to the given limit), and passes each to the publisher. If the publisher raises an exception, the event remains in the outbox for retry. If processing succeeds, the event is removed from the outbox. Args: publisher (Callable[[Recorded], None]): Function to publish a single event. limit (int, optional): Maximum number of entries to process in one run. Defaults to 100. \"\"\" stream = self . _strategy . outbox_entries ( limit = limit ) for entry in stream : with entry as raw_record : event = self . _serde . deserialize ( raw_record . entry ) record = Recorded ( wrapped_event = event , stream_id = raw_record . entry . stream_id , position = raw_record . position , tenant_id = raw_record . tenant_id , ) publisher ( record ) run ( publisher , limit = 100 ) Processes and publishes outbox entries using the provided publisher function. Fetches entries from the outbox (up to the given limit), and passes each to the publisher. If the publisher raises an exception, the event remains in the outbox for retry. If processing succeeds, the event is removed from the outbox. Parameters: Name Type Description Default publisher Callable [[ Recorded ], None] Function to publish a single event. required limit int Maximum number of entries to process in one run. Defaults to 100. 100 Source code in event_sourcery/_event_store/outbox.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def run ( self , publisher : Callable [[ Recorded ], None ], limit : int = 100 , ) -> None : \"\"\" Processes and publishes outbox entries using the provided publisher function. Fetches entries from the outbox (up to the given limit), and passes each to the publisher. If the publisher raises an exception, the event remains in the outbox for retry. If processing succeeds, the event is removed from the outbox. Args: publisher (Callable[[Recorded], None]): Function to publish a single event. limit (int, optional): Maximum number of entries to process in one run. Defaults to 100. \"\"\" stream = self . _strategy . outbox_entries ( limit = limit ) for entry in stream : with entry as raw_record : event = self . _serde . deserialize ( raw_record . entry ) record = Recorded ( wrapped_event = event , stream_id = raw_record . entry . stream_id , position = raw_record . position , tenant_id = raw_record . tenant_id , ) publisher ( record )","title":"Outbox"},{"location":"reference/event_store/outbox/Outbox/#event_sourcery.outbox.Outbox.run","text":"Processes and publishes outbox entries using the provided publisher function. Fetches entries from the outbox (up to the given limit), and passes each to the publisher. If the publisher raises an exception, the event remains in the outbox for retry. If processing succeeds, the event is removed from the outbox. Parameters: Name Type Description Default publisher Callable [[ Recorded ], None] Function to publish a single event. required limit int Maximum number of entries to process in one run. Defaults to 100. 100 Source code in event_sourcery/_event_store/outbox.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def run ( self , publisher : Callable [[ Recorded ], None ], limit : int = 100 , ) -> None : \"\"\" Processes and publishes outbox entries using the provided publisher function. Fetches entries from the outbox (up to the given limit), and passes each to the publisher. If the publisher raises an exception, the event remains in the outbox for retry. If processing succeeds, the event is removed from the outbox. Args: publisher (Callable[[Recorded], None]): Function to publish a single event. limit (int, optional): Maximum number of entries to process in one run. Defaults to 100. \"\"\" stream = self . _strategy . outbox_entries ( limit = limit ) for entry in stream : with entry as raw_record : event = self . _serde . deserialize ( raw_record . entry ) record = Recorded ( wrapped_event = event , stream_id = raw_record . entry . stream_id , position = raw_record . position , tenant_id = raw_record . tenant_id , ) publisher ( record )","title":"run"},{"location":"reference/event_store/outbox/no_filter/","text":"Source code in event_sourcery/_event_store/outbox.py 98 99 def no_filter ( entry : RawEvent ) -> bool : return True","title":"no_filter"},{"location":"reference/event_store/subscription/BuildPhase/","text":"Source code in event_sourcery/_event_store/subscription/interfaces.py 85 86 87 88 89 90 91 92 93 94 class BuildPhase : def build_iter ( self , timelimit : Seconds | timedelta ) -> Iterator [ Recorded | None ]: raise NotImplementedError () def build_batch ( self , size : int , timelimit : Seconds | timedelta , ) -> Iterator [ list [ Recorded ]]: raise NotImplementedError ()","title":"BuildPhase"},{"location":"reference/event_store/subscription/FilterPhase/","text":"Bases: BuildPhase Source code in event_sourcery/_event_store/subscription/interfaces.py 97 98 99 100 101 102 class FilterPhase ( BuildPhase ): def to_category ( self , category : StreamCategory ) -> BuildPhase : raise NotImplementedError () def to_events ( self , events : list [ type [ Event ]]) -> BuildPhase : raise NotImplementedError ()","title":"FilterPhase"},{"location":"reference/event_store/subscription/PositionPhase/","text":"Source code in event_sourcery/_event_store/subscription/interfaces.py 105 106 107 class PositionPhase : def start_from ( self , position : Position ) -> FilterPhase : raise NotImplementedError ()","title":"PositionPhase"},{"location":"reference/event_store/subscription/SubscriptionBuilder/","text":"Bases: PositionPhase , FilterPhase , BuildPhase Source code in event_sourcery/_event_store/subscription/builder.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @dataclass ( repr = False ) class SubscriptionBuilder ( PositionPhase , FilterPhase , BuildPhase ): _serde : Serde _strategy : SubscriptionStrategy _position : Position = field ( init = False , default = sys . maxsize ) _build : Callable [ ... , Iterator [ list [ RecordedRaw ]]] = field ( init = False ) def __post_init__ ( self ) -> None : self . _build = partial ( self . _strategy . subscribe_to_all ) def start_from ( self , position : Position ) -> FilterPhase : self . _build = partial ( self . _build , start_from = position ) self . _position = position return self def to_category ( self , category : StreamCategory ) -> BuildPhase : self . _build = partial ( self . _strategy . subscribe_to_category , start_from = self . _position , category = category , ) return self def to_events ( self , events : list [ type [ Event ]]) -> BuildPhase : self . _build = partial ( self . _strategy . subscribe_to_events , start_from = self . _position , events = [ self . _serde . registry . name_for_type ( event ) for event in events ], ) return self @staticmethod def _to_timedelta ( timelimit : Seconds | timedelta ) -> timedelta : seconds = ( timelimit if isinstance ( timelimit , timedelta ) else timedelta ( seconds = timelimit ) ) if seconds . total_seconds () < 0.1 : raise ValueError ( f \"Timebox must be at least 100 milliseconds. Received: \" f \" { seconds . total_seconds () : .02f } \" , ) return seconds def build_iter ( self , timelimit : Seconds | timedelta ) -> Iterator [ Recorded | None ]: timelimit = self . _to_timedelta ( timelimit ) return self . _single_event_unpack ( self . _build ( batch_size = 1 , timelimit = timelimit )) def _single_event_unpack ( self , subscription : Iterator [ list [ RecordedRaw ]], ) -> Iterator [ Recorded | None ]: while True : batch = next ( subscription ) yield self . _serde . deserialize_record ( batch [ 0 ]) if batch else None def build_batch ( self , size : int , timelimit : Seconds | timedelta , ) -> Iterator [ list [ Recorded ]]: seconds = self . _to_timedelta ( timelimit ) subscription = self . _build ( batch_size = size , timelimit = seconds ) return ( # pragma: no cover # apparently, bug in coverage.py [ self . _serde . deserialize_record ( e ) for e in batch ] for batch in subscription )","title":"SubscriptionBuilder"}]}