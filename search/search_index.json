{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Python Event Sourcery A library for event-based systems in Python. For event sourcing, CQRS, and event-driven architectures.","title":"Event Sourcery"},{"location":"#python-event-sourcery","text":"A library for event-based systems in Python. For event sourcing, CQRS, and event-driven architectures.","title":"Python Event Sourcery"},{"location":"concepts/basics/","text":"Basics What is an event? An event is a change of state. It is a piece of information stating a fact with extra information, letting to say what just actually happened and, potentially, react to it. Since events merely state the fact, that something happened, we write them in past tense. Examples Sprint started Payment failed Order cancelled Invoice issued You can stop reading for a moment and think about examples from the projects you're working on. When an event occurs it cannot be denied anymore. It can be only reacted to. Let's say that the payment failed but for some reason it should not. We can not negate the event, but we can always try to e.g. pay with another payment card. This is an example of reacting to an event. In an application that uses events explicitly, they will somehow be represented in the source code. They can be coded as simple data structures: @dataclass ( frozen = True ) class SprintStarted : when_started : datetime when_ends : datetime project_key : ProjectKey How Event Sourcery helps with using events? Event Sourcery provides a simple base class that is currently using Pydantic . It brings all goodies of that library plus provides a few basic fields, like unique identifier of an event or timestamp of event creation. from event_sourcery import Event class SprintStarted ( Event ): when_started : datetime when_ends : datetime project_key : ProjectKey event = SprintStarted ( when_started = datetime . now (), when_ends = datetime . now () + timedelta ( days = 7 ), project_key = \"PRO\" , ) # SprintStarted( # uuid=UUID('48c3ecb1-2d58-4b99-b964-2fb9ccfba601'), # created_at=datetime.datetime(2022, 8, 7, 16, 56, 35, 719248), # when_started=datetime.datetime(2022, 8, 7, 18, 56, 35, 719177), # when_ends=datetime.datetime(2022, 8, 14, 18, 56, 35, 719184), # project_key='PRO' # ) Other baked-in feature includes tracing ids - correlation id and causation id, useful for tracking flows of events in the system. What is Event-Driven Architecture? Systems that use Event-Driven Architecture (or EDA in short) use events for connecting its parts. One part of the system publishes an event, letting all other interested parts know that something significant happened. In turn, these parts may trigger some action on their side. This pattern is used with microservices publishing events via brokers, such as RabbitMQ or Apache Kafka . Event-Driven Distributed App Integration with events is a pattern that makes publishing part of the system ignorant of other parts, so there's loose coupling between them. For example, Order Service does not have to know that Payment Service or Invoice Service even exists. Another benefit from asynchronous, event-driven architecture is that even if something is temporarily wrong with Payment Service , system can still operate. Broker will receive messages and once Payment Service is back online, the process can continue. The same integration method can be used in much simpler environments, e.g. monorepo applications. One doesn't need a broker right away. How Event Sourcery helps with that? Event Sourcery provides implementation of Event Store and so-called Outbox. The former is a class to provide persistence of events while the Outbox makes sure they will be published eventually, even if there's something with the broker. First thing is to ask Event Store to not only save the event, but also to put it in the Outbox. You can do it using publish method instead of append . an_event = SomeEvent ( first_name = \"John\" ) # publish additionally puts an event in outbox event_store . append ( stream_id = uuid4 (), events = [ an_event ]) Then, one has to implement publishing mechanism - e.g. publishing to Kafka or RabbitMQ, depending on your stack. Event Sourcery does not provide this out of the box. What it does provide is Outbox class that accepts publisher argument to send the message. from event_sourcery import get_outbox outbox = get_outbox ( session = session , # SQLAlchemy session # a function that receives published event and does something with it publisher = lambda event : print ( event ), ) The last step is to run Outbox in a separate process, e.g. separate docker container in an infinite loop: while True : try : outbox . run_once () session . commit () except : session . rollback () logger . exception ( \"Outbox processing failed\" ) time . sleep ( 5 ) Regarding the simpler variant - that is monorepo app, running in a single process - Event Sourcery has a system of synchronous subscriptions. Simply saying, we can set up callbacks that will be triggered once a certain event is saved. store = get_event_store ( session = session , subscriptions = { UserRegistered : [ lambda event : send_email ( event . email )], }, ) stream_id = uuid4 () event = UserRegistered ( email = \"test@example.com\" ) store . append ( stream_id = stream_id , events = [ event ]) # here, callback runs and email gets sent! What is Event Sourcing? Recall any entity/model/being from a piece of software you recently worked on. Let's consider e-commerce Order . It might hold current status (new, confirmed, shipped, etc) and summaries \u2013 total price, shipping and taxes. Naturally, Order does not exist on its own. We usually wire it with another entity, OrderLine , that refers to a single product ordered with a quantity information. This structure could be represented in a relational database in a following way: orders id status total_price 1 new 169.99 order_lines id order_id product_id quantity 1 1 512 1 2 1 614 3 By storing data this way we can always cheaply get CURRENT state of our Order . We store a dump of serialized object after latest changes. Changing anything, for example switching status from new to shipped causes data overwrite. We irreversibly lose old state. What if we need to track all changes? Let\u2019s see how that fits in another database table: order_history id order_id event_name datetime data 1 1 OrderCreated 2018-01-20 18:33:12 2 1 LineAdded 2018-01-20 18:33:44 {\"product_id\": 512, \"quantity\": 1} 3 1 StatusChanged 2018-01-20 18:42:59 {\"status\": \"confirmed\"} Such a representation enables us to confidently say what was changed and when. But this order_history table plays only a second fiddle. It is merely an extra record of Order , added just to fulfill some business requirement. We still reach to original orders table when we want to know exact state of any Order in all other scenarios. However, notice that order_history is as good as orders table when we have to get current Order state. How so? We just have to fetch all entries for given Order and replay them from the start. In the end we\u2019ll get exactly the same information that is saved in orders table. So do we even need orders and orders_lines table as a source of truth...? Event Sourcing proposes we don't. We can still keep them around to optimize reading data for UI, but no longer have to rely on it in any situation that would actually change Order . To sum up, Event Sourcing comes down to: Keeping your business objects (called aggregates) as a series of replayable events. This is often called an event stream. Never deleting any events from a system, only appending new ones Using events as the only reliable way of telling in what state a given aggregate is If you need to query data or present them in a table-like format, keep a copy of them in a denormalized format. This is called projection Designing your aggregates to protect certain vital business invariants, such as Order encapsulates costs summary. A good rule of thumb is to keep aggregates as small as possible How Event Sourcery helps with that? Event Sourcery provides a base class for an aggregate and repository implementation that makes it much easy to create or read/change aggregates. class LightSwitch ( Aggregate ): \"\"\"A simple aggregate that models a light switch.\"\"\" class AlreadyTurnedOn ( Exception ): pass class AlreadyTurnedOff ( Exception ): pass def __init__ ( self , past_events : list [ Event ], changes : list [ Event ], stream_id : StreamId ) -> None : # init any state you need in aggregate class to check conditions self . _shines = False # required for base class super () . __init__ ( past_events , changes , stream_id ) def _apply ( self , event : Event ) -> None : # each aggregate need an _apply method to parse events match event : case TurnedOn () as event : self . _shines = True case TurnedOff () as event : self . _shines = False def turn_on ( self ) -> None : # this is one of command methods # we can rejest it (i.e. raise an exception) # if current state does not allow this to proceed # e.g. light is already on if self . _shines : raise LightSwitch . AlreadyTurnedOn self . _event ( TurnedOn ) def turn_off ( self ) -> None : if not self . _shines : raise LightSwitch . AlreadyTurnedOff self . _event ( TurnedOff ) To create a Repository tailored for a particular Aggregate class, we need that class and Event Store instance: repository = Repository [ LightSwitch ]( event_store , LightSwitch ) A Repository exposes method to create a new instance of Aggregate: stream_id = uuid4 () with repository . new ( stream_id = stream_id ) as switch : switch . turn_on () ...or to work with existing Aggregate, making sure changes are saved at the end: with repository . aggregate ( stream_id = stream_id ) as switch_second_incarnation : try : switch_second_incarnation . turn_on () except LightSwitch . AlreadyTurnedOn : # o mon Dieu, I made a mistake! switch_second_incarnation . turn_off () A Repository is a thin wrapper over Event Store. One can also write Aggregates even without using our base class and use EventStore directly!","title":"Basics"},{"location":"concepts/basics/#basics","text":"","title":"Basics"},{"location":"concepts/basics/#what-is-an-event","text":"An event is a change of state. It is a piece of information stating a fact with extra information, letting to say what just actually happened and, potentially, react to it. Since events merely state the fact, that something happened, we write them in past tense. Examples Sprint started Payment failed Order cancelled Invoice issued You can stop reading for a moment and think about examples from the projects you're working on. When an event occurs it cannot be denied anymore. It can be only reacted to. Let's say that the payment failed but for some reason it should not. We can not negate the event, but we can always try to e.g. pay with another payment card. This is an example of reacting to an event. In an application that uses events explicitly, they will somehow be represented in the source code. They can be coded as simple data structures: @dataclass ( frozen = True ) class SprintStarted : when_started : datetime when_ends : datetime project_key : ProjectKey","title":"What is an event?"},{"location":"concepts/basics/#how-event-sourcery-helps-with-using-events","text":"Event Sourcery provides a simple base class that is currently using Pydantic . It brings all goodies of that library plus provides a few basic fields, like unique identifier of an event or timestamp of event creation. from event_sourcery import Event class SprintStarted ( Event ): when_started : datetime when_ends : datetime project_key : ProjectKey event = SprintStarted ( when_started = datetime . now (), when_ends = datetime . now () + timedelta ( days = 7 ), project_key = \"PRO\" , ) # SprintStarted( # uuid=UUID('48c3ecb1-2d58-4b99-b964-2fb9ccfba601'), # created_at=datetime.datetime(2022, 8, 7, 16, 56, 35, 719248), # when_started=datetime.datetime(2022, 8, 7, 18, 56, 35, 719177), # when_ends=datetime.datetime(2022, 8, 14, 18, 56, 35, 719184), # project_key='PRO' # ) Other baked-in feature includes tracing ids - correlation id and causation id, useful for tracking flows of events in the system.","title":"How Event Sourcery helps with using events?"},{"location":"concepts/basics/#what-is-event-driven-architecture","text":"Systems that use Event-Driven Architecture (or EDA in short) use events for connecting its parts. One part of the system publishes an event, letting all other interested parts know that something significant happened. In turn, these parts may trigger some action on their side. This pattern is used with microservices publishing events via brokers, such as RabbitMQ or Apache Kafka . Event-Driven Distributed App Integration with events is a pattern that makes publishing part of the system ignorant of other parts, so there's loose coupling between them. For example, Order Service does not have to know that Payment Service or Invoice Service even exists. Another benefit from asynchronous, event-driven architecture is that even if something is temporarily wrong with Payment Service , system can still operate. Broker will receive messages and once Payment Service is back online, the process can continue. The same integration method can be used in much simpler environments, e.g. monorepo applications. One doesn't need a broker right away.","title":"What is Event-Driven Architecture?"},{"location":"concepts/basics/#how-event-sourcery-helps-with-that","text":"Event Sourcery provides implementation of Event Store and so-called Outbox. The former is a class to provide persistence of events while the Outbox makes sure they will be published eventually, even if there's something with the broker. First thing is to ask Event Store to not only save the event, but also to put it in the Outbox. You can do it using publish method instead of append . an_event = SomeEvent ( first_name = \"John\" ) # publish additionally puts an event in outbox event_store . append ( stream_id = uuid4 (), events = [ an_event ]) Then, one has to implement publishing mechanism - e.g. publishing to Kafka or RabbitMQ, depending on your stack. Event Sourcery does not provide this out of the box. What it does provide is Outbox class that accepts publisher argument to send the message. from event_sourcery import get_outbox outbox = get_outbox ( session = session , # SQLAlchemy session # a function that receives published event and does something with it publisher = lambda event : print ( event ), ) The last step is to run Outbox in a separate process, e.g. separate docker container in an infinite loop: while True : try : outbox . run_once () session . commit () except : session . rollback () logger . exception ( \"Outbox processing failed\" ) time . sleep ( 5 ) Regarding the simpler variant - that is monorepo app, running in a single process - Event Sourcery has a system of synchronous subscriptions. Simply saying, we can set up callbacks that will be triggered once a certain event is saved. store = get_event_store ( session = session , subscriptions = { UserRegistered : [ lambda event : send_email ( event . email )], }, ) stream_id = uuid4 () event = UserRegistered ( email = \"test@example.com\" ) store . append ( stream_id = stream_id , events = [ event ]) # here, callback runs and email gets sent!","title":"How Event Sourcery helps with that?"},{"location":"concepts/basics/#what-is-event-sourcing","text":"Recall any entity/model/being from a piece of software you recently worked on. Let's consider e-commerce Order . It might hold current status (new, confirmed, shipped, etc) and summaries \u2013 total price, shipping and taxes. Naturally, Order does not exist on its own. We usually wire it with another entity, OrderLine , that refers to a single product ordered with a quantity information. This structure could be represented in a relational database in a following way: orders id status total_price 1 new 169.99 order_lines id order_id product_id quantity 1 1 512 1 2 1 614 3 By storing data this way we can always cheaply get CURRENT state of our Order . We store a dump of serialized object after latest changes. Changing anything, for example switching status from new to shipped causes data overwrite. We irreversibly lose old state. What if we need to track all changes? Let\u2019s see how that fits in another database table: order_history id order_id event_name datetime data 1 1 OrderCreated 2018-01-20 18:33:12 2 1 LineAdded 2018-01-20 18:33:44 {\"product_id\": 512, \"quantity\": 1} 3 1 StatusChanged 2018-01-20 18:42:59 {\"status\": \"confirmed\"} Such a representation enables us to confidently say what was changed and when. But this order_history table plays only a second fiddle. It is merely an extra record of Order , added just to fulfill some business requirement. We still reach to original orders table when we want to know exact state of any Order in all other scenarios. However, notice that order_history is as good as orders table when we have to get current Order state. How so? We just have to fetch all entries for given Order and replay them from the start. In the end we\u2019ll get exactly the same information that is saved in orders table. So do we even need orders and orders_lines table as a source of truth...? Event Sourcing proposes we don't. We can still keep them around to optimize reading data for UI, but no longer have to rely on it in any situation that would actually change Order . To sum up, Event Sourcing comes down to: Keeping your business objects (called aggregates) as a series of replayable events. This is often called an event stream. Never deleting any events from a system, only appending new ones Using events as the only reliable way of telling in what state a given aggregate is If you need to query data or present them in a table-like format, keep a copy of them in a denormalized format. This is called projection Designing your aggregates to protect certain vital business invariants, such as Order encapsulates costs summary. A good rule of thumb is to keep aggregates as small as possible","title":"What is Event Sourcing?"},{"location":"concepts/basics/#how-event-sourcery-helps-with-that_1","text":"Event Sourcery provides a base class for an aggregate and repository implementation that makes it much easy to create or read/change aggregates. class LightSwitch ( Aggregate ): \"\"\"A simple aggregate that models a light switch.\"\"\" class AlreadyTurnedOn ( Exception ): pass class AlreadyTurnedOff ( Exception ): pass def __init__ ( self , past_events : list [ Event ], changes : list [ Event ], stream_id : StreamId ) -> None : # init any state you need in aggregate class to check conditions self . _shines = False # required for base class super () . __init__ ( past_events , changes , stream_id ) def _apply ( self , event : Event ) -> None : # each aggregate need an _apply method to parse events match event : case TurnedOn () as event : self . _shines = True case TurnedOff () as event : self . _shines = False def turn_on ( self ) -> None : # this is one of command methods # we can rejest it (i.e. raise an exception) # if current state does not allow this to proceed # e.g. light is already on if self . _shines : raise LightSwitch . AlreadyTurnedOn self . _event ( TurnedOn ) def turn_off ( self ) -> None : if not self . _shines : raise LightSwitch . AlreadyTurnedOff self . _event ( TurnedOff ) To create a Repository tailored for a particular Aggregate class, we need that class and Event Store instance: repository = Repository [ LightSwitch ]( event_store , LightSwitch ) A Repository exposes method to create a new instance of Aggregate: stream_id = uuid4 () with repository . new ( stream_id = stream_id ) as switch : switch . turn_on () ...or to work with existing Aggregate, making sure changes are saved at the end: with repository . aggregate ( stream_id = stream_id ) as switch_second_incarnation : try : switch_second_incarnation . turn_on () except LightSwitch . AlreadyTurnedOn : # o mon Dieu, I made a mistake! switch_second_incarnation . turn_off () A Repository is a thin wrapper over Event Store. One can also write Aggregates even without using our base class and use EventStore directly!","title":"How Event Sourcery helps with that?"},{"location":"recipes/data_privacy/","text":"\ud83d\udd12 Data Privacy: Crypto-Shredding Introduction Crypto-shredding is a technique for protecting privacy-sensitive data in event-sourced systems. It allows you to irreversibly remove access to encrypted data by deleting encryption keys, ensuring compliance with regulations like GDPR (\"right to be forgotten\"). This approach is especially useful in event sourcing, where data is immutable and cannot be physically deleted from the event log. Our framework makes it easy to mark fields for encryption and shredding using Python type annotations. Once a subject\u2019s data needs to be removed, you can shred their encryption keys, and all encrypted fields will be masked automatically. Use-cases GDPR Compliance: Instantly fulfill user requests to erase personal data by shredding their encryption keys. Audit & Compliance: Demonstrate that sensitive data is irreversibly masked, even in backups and logs. Access Control: Limit access to personal data after a user relationship ends, without deleting events. Testing & Debugging: Mask sensitive fields in test environments or logs to prevent accidental leaks. Quickstart Crypto-shredding lets you irreversibly remove access to privacy-sensitive data in events by deleting encryption keys. Mark fields for encryption and shredding using Python type annotations: from typing import Annotated from event_sourcery.event_store.event import Event , Encrypted , DataSubject class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str # unencrypted field Note: mask_value must match the type of the field. Usage Configure backend with encryption: factory = ( SQLAlchemyBackendFactory ( session ) . with_encryption ( key_storage = InMemoryKeyStorage (), strategy = FernetEncryptionStrategy (), ) ) # Usage (automatic through serialization) encryption = Encryption ( strategy = strategy , key_storage = key_storage ) encrypted_data = encryption . encrypt ( user_event , stream_id ) # Returns dict with encrypted fields decrypted_data = encryption . decrypt ( UserRegistered , encrypted_data , stream_id ) # Returns dict with decrypted fields Validation: DataSubject Required Every event with encrypted fields must have exactly one field marked as DataSubject . If you forget to add it, you'll get a clear error during class initialization or when appending to a stream. Example Traceback class NoSubjectEvent ( Event ): encrypted_text : Annotated [ str , Encrypted ( mask_value = \"[TEXT_REDACTED]\" )] plain : str = \"plain\" # Raises when appending to a stream: # event_sourcery.event_store.exceptions.NoSubjectIdFound: No subject id found for event 'NoSubjectEvent' in stream <StreamId> How to fix: Add a field with DataSubject annotation: class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str Custom subject field You can specify a different subject for an encrypted field: class CustomSubjectEvent ( Event ): subject_id : Annotated [ str , DataSubject ] secondary_subject_id : str custom_subject : Annotated [ str , Encrypted ( mask_value = \"[TEXT_REDACTED]\" , subject_field = \"secondary_subject_id\" ), ] Shredding To shred (irreversibly mask) all data for a subject: encryption . shred ( subject_id = \"user-123\" ) # All encrypted fields for this subject will now return their mask_value (e.g. \"[REDACTED]\") How it works Crypto-shredding in this framework is based on three main concepts: Field Markers: Use Python type annotations to mark fields for encryption ( [Encrypted](../reference/encrypted.md) ) and to identify the privacy subject ( DataSubject ). Each event must have exactly one DataSubject field. Encrypted fields will be associated with this subject by default. You can specify a different subject for an encrypted field using the subject_field parameter. Automatic Encryption & Decryption: When events are serialized, fields marked as [Encrypted](../reference/encrypted.md) are automatically encrypted using the configured strategy and key storage. When events are deserialized, encrypted fields are automatically decrypted if the key is available. If the key is shredded, the field will return its mask_value instead of the original data. Shredding: Shredding is performed by deleting the encryption key for a given subject. All encrypted fields for that subject will be irreversibly masked. Public fields remain accessible and are not affected by shredding. Best Practices Always specify a meaningful mask_value for each encrypted field. This value will be shown when data is shredded. Ensure every event with encrypted fields has a DataSubject field. This is required for correct operation and validation. Use custom subject_field only when you need to associate encrypted data with a different subject than the default. Regularly audit your key storage and shredding logic to ensure compliance with privacy regulations. Test shredding in development to verify that sensitive data is properly masked and cannot be recovered. FAQ What happens if I forget to add a DataSubject field? You will get a NoSubjectIdFound exception when appending the event to a stream. This ensures that every encrypted field is associated with a privacy subject. See the traceback example above for details. Can I shred only part of the data for a subject? No, shredding deletes the encryption key for the subject, which irreversibly masks all encrypted fields associated with that subject. If you need more granular control, use separate subjects for different fields. What if I need to change the mask value? You must update the event class definition and re-deploy. The mask value is evaluated at runtime, so no migrations is needed. Just ensure the new mask_value matches the type of the encrypted field. Is crypto-shredding suitable for all types of data? Crypto-shredding is ideal for privacy-sensitive fields that must be protected or deleted on request. Do not use it for fields that must remain accessible for business or legal reasons. \ud83d\udd0e If you see NoSubjectIdFound in your traceback, check that your event class defines a DataSubject field.","title":"\ud83d\udd12 Data privacy"},{"location":"recipes/data_privacy/#data-privacy-crypto-shredding","text":"","title":"\ud83d\udd12 Data Privacy: Crypto-Shredding"},{"location":"recipes/data_privacy/#introduction","text":"Crypto-shredding is a technique for protecting privacy-sensitive data in event-sourced systems. It allows you to irreversibly remove access to encrypted data by deleting encryption keys, ensuring compliance with regulations like GDPR (\"right to be forgotten\"). This approach is especially useful in event sourcing, where data is immutable and cannot be physically deleted from the event log. Our framework makes it easy to mark fields for encryption and shredding using Python type annotations. Once a subject\u2019s data needs to be removed, you can shred their encryption keys, and all encrypted fields will be masked automatically.","title":"Introduction"},{"location":"recipes/data_privacy/#use-cases","text":"GDPR Compliance: Instantly fulfill user requests to erase personal data by shredding their encryption keys. Audit & Compliance: Demonstrate that sensitive data is irreversibly masked, even in backups and logs. Access Control: Limit access to personal data after a user relationship ends, without deleting events. Testing & Debugging: Mask sensitive fields in test environments or logs to prevent accidental leaks.","title":"Use-cases"},{"location":"recipes/data_privacy/#quickstart","text":"Crypto-shredding lets you irreversibly remove access to privacy-sensitive data in events by deleting encryption keys. Mark fields for encryption and shredding using Python type annotations: from typing import Annotated from event_sourcery.event_store.event import Event , Encrypted , DataSubject class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str # unencrypted field Note: mask_value must match the type of the field.","title":"Quickstart"},{"location":"recipes/data_privacy/#usage","text":"Configure backend with encryption: factory = ( SQLAlchemyBackendFactory ( session ) . with_encryption ( key_storage = InMemoryKeyStorage (), strategy = FernetEncryptionStrategy (), ) ) # Usage (automatic through serialization) encryption = Encryption ( strategy = strategy , key_storage = key_storage ) encrypted_data = encryption . encrypt ( user_event , stream_id ) # Returns dict with encrypted fields decrypted_data = encryption . decrypt ( UserRegistered , encrypted_data , stream_id ) # Returns dict with decrypted fields","title":"Usage"},{"location":"recipes/data_privacy/#validation-datasubject-required","text":"Every event with encrypted fields must have exactly one field marked as DataSubject . If you forget to add it, you'll get a clear error during class initialization or when appending to a stream.","title":"Validation: DataSubject Required"},{"location":"recipes/data_privacy/#example-traceback","text":"class NoSubjectEvent ( Event ): encrypted_text : Annotated [ str , Encrypted ( mask_value = \"[TEXT_REDACTED]\" )] plain : str = \"plain\" # Raises when appending to a stream: # event_sourcery.event_store.exceptions.NoSubjectIdFound: No subject id found for event 'NoSubjectEvent' in stream <StreamId> How to fix: Add a field with DataSubject annotation: class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] public_info : str","title":"Example Traceback"},{"location":"recipes/data_privacy/#custom-subject-field","text":"You can specify a different subject for an encrypted field: class CustomSubjectEvent ( Event ): subject_id : Annotated [ str , DataSubject ] secondary_subject_id : str custom_subject : Annotated [ str , Encrypted ( mask_value = \"[TEXT_REDACTED]\" , subject_field = \"secondary_subject_id\" ), ]","title":"Custom subject field"},{"location":"recipes/data_privacy/#shredding","text":"To shred (irreversibly mask) all data for a subject: encryption . shred ( subject_id = \"user-123\" ) # All encrypted fields for this subject will now return their mask_value (e.g. \"[REDACTED]\")","title":"Shredding"},{"location":"recipes/data_privacy/#how-it-works","text":"Crypto-shredding in this framework is based on three main concepts: Field Markers: Use Python type annotations to mark fields for encryption ( [Encrypted](../reference/encrypted.md) ) and to identify the privacy subject ( DataSubject ). Each event must have exactly one DataSubject field. Encrypted fields will be associated with this subject by default. You can specify a different subject for an encrypted field using the subject_field parameter. Automatic Encryption & Decryption: When events are serialized, fields marked as [Encrypted](../reference/encrypted.md) are automatically encrypted using the configured strategy and key storage. When events are deserialized, encrypted fields are automatically decrypted if the key is available. If the key is shredded, the field will return its mask_value instead of the original data. Shredding: Shredding is performed by deleting the encryption key for a given subject. All encrypted fields for that subject will be irreversibly masked. Public fields remain accessible and are not affected by shredding.","title":"How it works"},{"location":"recipes/data_privacy/#best-practices","text":"Always specify a meaningful mask_value for each encrypted field. This value will be shown when data is shredded. Ensure every event with encrypted fields has a DataSubject field. This is required for correct operation and validation. Use custom subject_field only when you need to associate encrypted data with a different subject than the default. Regularly audit your key storage and shredding logic to ensure compliance with privacy regulations. Test shredding in development to verify that sensitive data is properly masked and cannot be recovered.","title":"Best Practices"},{"location":"recipes/data_privacy/#faq","text":"","title":"FAQ"},{"location":"recipes/data_privacy/#what-happens-if-i-forget-to-add-a-datasubject-field","text":"You will get a NoSubjectIdFound exception when appending the event to a stream. This ensures that every encrypted field is associated with a privacy subject. See the traceback example above for details.","title":"What happens if I forget to add a DataSubject field?"},{"location":"recipes/data_privacy/#can-i-shred-only-part-of-the-data-for-a-subject","text":"No, shredding deletes the encryption key for the subject, which irreversibly masks all encrypted fields associated with that subject. If you need more granular control, use separate subjects for different fields.","title":"Can I shred only part of the data for a subject?"},{"location":"recipes/data_privacy/#what-if-i-need-to-change-the-mask-value","text":"You must update the event class definition and re-deploy. The mask value is evaluated at runtime, so no migrations is needed. Just ensure the new mask_value matches the type of the encrypted field.","title":"What if I need to change the mask value?"},{"location":"recipes/data_privacy/#is-crypto-shredding-suitable-for-all-types-of-data","text":"Crypto-shredding is ideal for privacy-sensitive fields that must be protected or deleted on request. Do not use it for fields that must remain accessible for business or legal reasons. \ud83d\udd0e If you see NoSubjectIdFound in your traceback, check that your event class defines a DataSubject field.","title":"Is crypto-shredding suitable for all types of data?"},{"location":"recipes/defining_events/","text":"To define an event, write a class inheriting from Event base class: from event_sourcery.event_store import Event class InvoicePaid ( Event ): invoice_number : str Base class Event is a pydantic model and so will be every event you define.","title":"\ud83c\udf9e\ufe0f Defining events"},{"location":"recipes/event_sourcing/","text":"Building blocks Event Sourcery provides a few building blocks to work with event sourcing. These are Aggregate and Repository base classes. Usage You start from defining your own aggregate inheriting from Aggregate . There are three required attributes that need to be defined: category class-level constant that will be added to all streams from all aggregates of this type __init__ if defined, must not accept any arguments __apply__ method that will change internal state of the aggregate based on the event applied during reading state from the event store from event_sourcery.event_sourcing import Aggregate , Repository from event_sourcery.event_store import Event class SwitchedOn ( Event ): pass class LightSwitch ( Aggregate ): category = \"light_switch\" # 1 def __init__ ( self ) -> None : # 2 self . _switched_on = False def __apply__ ( self , event : Event ) -> None : # 3 match event : case SwitchedOn (): self . _switched_on = True case _ : raise NotImplementedError ( f \"Unexpected event { type ( event ) } \" ) def switch_on ( self ) -> None : if self . _switched_on : return # no op self . _emit ( SwitchedOn ()) To work with aggregate, you need to create repository. You need an instance of EventStore to do so: repository = Repository [ LightSwitch ]( backend . event_store ) From now on, regardless if you want to work with a given aggregate for the first time or load existing one, you should use repository.aggregate context manager: from event_sourcery.event_store import StreamUUID stream_id = StreamUUID ( name = \"light_switch/1\" ) with repository . aggregate ( stream_id , LightSwitch ()) as light_switch : light_switch . switch_on () light_switch . switch_on ()","title":"\ud83c\udf9b\ufe0f Event Sourcing"},{"location":"recipes/event_sourcing/#building-blocks","text":"Event Sourcery provides a few building blocks to work with event sourcing. These are Aggregate and Repository base classes.","title":"Building blocks"},{"location":"recipes/event_sourcing/#usage","text":"You start from defining your own aggregate inheriting from Aggregate . There are three required attributes that need to be defined: category class-level constant that will be added to all streams from all aggregates of this type __init__ if defined, must not accept any arguments __apply__ method that will change internal state of the aggregate based on the event applied during reading state from the event store from event_sourcery.event_sourcing import Aggregate , Repository from event_sourcery.event_store import Event class SwitchedOn ( Event ): pass class LightSwitch ( Aggregate ): category = \"light_switch\" # 1 def __init__ ( self ) -> None : # 2 self . _switched_on = False def __apply__ ( self , event : Event ) -> None : # 3 match event : case SwitchedOn (): self . _switched_on = True case _ : raise NotImplementedError ( f \"Unexpected event { type ( event ) } \" ) def switch_on ( self ) -> None : if self . _switched_on : return # no op self . _emit ( SwitchedOn ()) To work with aggregate, you need to create repository. You need an instance of EventStore to do so: repository = Repository [ LightSwitch ]( backend . event_store ) From now on, regardless if you want to work with a given aggregate for the first time or load existing one, you should use repository.aggregate context manager: from event_sourcery.event_store import StreamUUID stream_id = StreamUUID ( name = \"light_switch/1\" ) with repository . aggregate ( stream_id , LightSwitch ()) as light_switch : light_switch . switch_on () light_switch . switch_on ()","title":"Usage"},{"location":"recipes/integrate/","text":"Event Sourcery supports variety of backends and configurations. Integrating it with your project requires following steps: instantiating a corresponding factory class, depending on your storage optional configuration, like enabling additional features building so-called Backend that exposes features of the library SQLAlchemy KurrentDB (formerly EventStoreDB) Django Event Sourcery defines a few models to keep your events and streams in the database. When working with SQLAlchemy, you'll typically use alembic to manage your migrations. Before alembic can detect Event Sourcery models, you need to register them once via configure_models : from event_sourcery_sqlalchemy import configure_models configure_models ( Base ) # Base is your declarative base class Once our models are registered, migrations generated and executed, you can continue. You need an instance of Session to instantiate event_sourcery_sqlalchemy.SQLAlchemyBackend : from event_sourcery_sqlalchemy import SQLAlchemyBackend session = Session () # SQLAlchemy session backend = SQLAlchemyBackend ( session ) First, you need an instance of kurrentdbclient.KurrentDBClient that represents a connection to EventStoreDB. Then, you can pass it to event_sourcery_kurrentdb.KurrentDBBackend : from kurrentdbclient import KurrentDBClient from event_sourcery_kurrentdb import KurrentDBBackend client = KurrentDBClient ( uri = \"kurrentdb://localhost:2113?Tls=false\" ) backend = KurrentDBBackend ( client ) backend . event_store . load_stream ( StreamId ()) # test if connection works Your first step will be adding \"event_sourcery_django\" to the list of INSTALLED_APPS in your settings. Then you can simply create an instance of event_sourcery_django.DjangoBackend : from event_sourcery_django import DjangoBackend backend = DjangoBackend () This can be done once. Then, you can import backend from other parts of code to use it. From backend you can grab EventStore instance: backend . event_store You can now use EventStore to load events from a stream or append new events.","title":"\ud83d\udd17 Integrate with your app"},{"location":"recipes/multitenancy/","text":"Event Sourcery implements multitenancy by adding tenant id to all objects it stores. By default, EventStore works in so-called default context, tenant-less. Switching tenant To switch context to specific tenant, one should call scoped_for_tenant method: scoped_event_store = event_store . scoped_for_tenant ( \"tenant123\" ) Isolation rules There are three rules regarding isolation: streams and events from default, tenant-less context are not visible in any tenant-aware context streams and events from another tenant are not visible when working with tenant-aware context streams and events from any tenant are not visible when working with tenant-less, default context This table summarises visibility rules: visible? tenant-less tenant A tenant B tenant-less yes no no tenant A no yes no tenant B no no yes Multitenancy in other features Outbox and subscriptions Both Outbox and Subscriptions are meant to be used in a system context, for example to implement a projection of events onto a read model. However, you can always get tenant id when working with them. On any Recorded instance there is an attribute called tenant_id . For events that were created in a default, tenant-less context, tenant_id has value of event_sourcery.event_store.DEFAULT_TENANT . Value of this constant should not be relied upon and is considered an implementation details. In all places where you wish to check if an event was created in a default context, you should use this constant: from event_sourcery.event_store import DEFAULT_TENANT ... for recorded_event in subscription : if recorded_event is None : break elif recorded_event . tenant_id == DEFAULT_TENANT : print ( f \"Got tenant-less event! { recorded_event } \" ) else : ... Event Sourcing In case of Event Sourcing , whenever you construct a Repository make sure you pass a scoped EventStore instance: scoped_event_store = event_store . scoped_for_tenant ( \"tenant123\" ) repository = Repository [ ExampleAggregate ]( scoped_event_store )","title":"\ud83d\udc6b\ufe0f Multitenancy"},{"location":"recipes/multitenancy/#switching-tenant","text":"To switch context to specific tenant, one should call scoped_for_tenant method: scoped_event_store = event_store . scoped_for_tenant ( \"tenant123\" )","title":"Switching tenant"},{"location":"recipes/multitenancy/#isolation-rules","text":"There are three rules regarding isolation: streams and events from default, tenant-less context are not visible in any tenant-aware context streams and events from another tenant are not visible when working with tenant-aware context streams and events from any tenant are not visible when working with tenant-less, default context This table summarises visibility rules: visible? tenant-less tenant A tenant B tenant-less yes no no tenant A no yes no tenant B no no yes","title":"Isolation rules"},{"location":"recipes/multitenancy/#multitenancy-in-other-features","text":"","title":"Multitenancy in other features"},{"location":"recipes/multitenancy/#outbox-and-subscriptions","text":"Both Outbox and Subscriptions are meant to be used in a system context, for example to implement a projection of events onto a read model. However, you can always get tenant id when working with them. On any Recorded instance there is an attribute called tenant_id . For events that were created in a default, tenant-less context, tenant_id has value of event_sourcery.event_store.DEFAULT_TENANT . Value of this constant should not be relied upon and is considered an implementation details. In all places where you wish to check if an event was created in a default context, you should use this constant: from event_sourcery.event_store import DEFAULT_TENANT ... for recorded_event in subscription : if recorded_event is None : break elif recorded_event . tenant_id == DEFAULT_TENANT : print ( f \"Got tenant-less event! { recorded_event } \" ) else : ...","title":"Outbox and subscriptions"},{"location":"recipes/multitenancy/#event-sourcing","text":"In case of Event Sourcing , whenever you construct a Repository make sure you pass a scoped EventStore instance: scoped_event_store = event_store . scoped_for_tenant ( \"tenant123\" ) repository = Repository [ ExampleAggregate ]( scoped_event_store )","title":"Event Sourcing"},{"location":"recipes/outbox/","text":"About the pattern Outbox is a pattern that ensures a message is reliably sent from the system. It provides at-least-once semantics, making sure that every message that CAN be sent to e.g. RabbitMQ, gets there. Tip Use Outbox when you want to send events to a broker (e.g. RabbitMQ, Kafka) or external system for analytics purposes (e.g. Segment.io). Basic usage Configure Event Sourcery To use outbox, you have to add with_outbox method call on factory while setting up Event Sourcery : backend = ( SQLAlchemyBackend ( session ) . with_outbox () # enable outbox ) Write publishing function To publish messages, you need to write a little bit of glue code that will actually send a message. We need a publishing function that takes Recorded as the only argument and will do the sending. Take RabbitMQ and pika for example: # setting up connection and queue... connection = pika . BlockingConnection ( pika . ConnectionParameters ( \"localhost\" )) channel = connection . channel () channel . queue_declare ( queue = \"events\" ) def publish ( recorded : Recorded ) -> None : as_json = recorded . wrapped_event . event . model_dump_json () channel . basic_publish ( exchange = \"\" , routing_key = \"events\" , body = as_json ) Run outbox Now you can run outbox: backend . outbox . run ( publisher = publish ) This will loop over events and will try to call publishing function for each one of them. Optionally, you can specify a number of events to be processed in a single run: backend . outbox . run ( publisher = publish , limit = 50 ) By default, outbox.run will try to process 100 events. Transactional outbox If you use any of transactional backends (e.g. SQLAlchemy or Django), then every call to outbox.run should be wrapped with a database transaction. with session . begin (): backend . outbox . run ( publisher = publish ) session . commit () Warning Event Sourcery outbox keeps track of messages sent and attempts left. Without commiting a transaction, same messages will be sent over and over again. Running outbox Normally you'd be running outbox in an infinite loop, in a separate process - just like you'd do with a Celery worker: from time import sleep while True : with session . begin (): backend . outbox . run ( publish = publisher ) session . commit () sleep ( 3 ) # wait between runs to avoid hammering the database Optional filterer Warning This feature and/or its API is provisional and will probably change soon. Sometimes you want only specific events to be sent. You can pass an optional filterer argument to with_outbox . It should be a callable (e.g. a function) that accepts an event instance and returns True if an event should be published. False otherwise. backend = SQLAlchemyBackend ( session ) . with_outbox ( filterer = lambda e : \"InvoicePaid\" in e . name ) Handling retries Sending each event will be retried up to 3 times.","title":"\ud83d\udce4 Publish events - outbox"},{"location":"recipes/outbox/#about-the-pattern","text":"Outbox is a pattern that ensures a message is reliably sent from the system. It provides at-least-once semantics, making sure that every message that CAN be sent to e.g. RabbitMQ, gets there. Tip Use Outbox when you want to send events to a broker (e.g. RabbitMQ, Kafka) or external system for analytics purposes (e.g. Segment.io).","title":"About the pattern"},{"location":"recipes/outbox/#basic-usage","text":"","title":"Basic usage"},{"location":"recipes/outbox/#configure-event-sourcery","text":"To use outbox, you have to add with_outbox method call on factory while setting up Event Sourcery : backend = ( SQLAlchemyBackend ( session ) . with_outbox () # enable outbox )","title":"Configure Event Sourcery"},{"location":"recipes/outbox/#write-publishing-function","text":"To publish messages, you need to write a little bit of glue code that will actually send a message. We need a publishing function that takes Recorded as the only argument and will do the sending. Take RabbitMQ and pika for example: # setting up connection and queue... connection = pika . BlockingConnection ( pika . ConnectionParameters ( \"localhost\" )) channel = connection . channel () channel . queue_declare ( queue = \"events\" ) def publish ( recorded : Recorded ) -> None : as_json = recorded . wrapped_event . event . model_dump_json () channel . basic_publish ( exchange = \"\" , routing_key = \"events\" , body = as_json )","title":"Write publishing function"},{"location":"recipes/outbox/#run-outbox","text":"Now you can run outbox: backend . outbox . run ( publisher = publish ) This will loop over events and will try to call publishing function for each one of them. Optionally, you can specify a number of events to be processed in a single run: backend . outbox . run ( publisher = publish , limit = 50 ) By default, outbox.run will try to process 100 events.","title":"Run outbox"},{"location":"recipes/outbox/#transactional-outbox","text":"If you use any of transactional backends (e.g. SQLAlchemy or Django), then every call to outbox.run should be wrapped with a database transaction. with session . begin (): backend . outbox . run ( publisher = publish ) session . commit () Warning Event Sourcery outbox keeps track of messages sent and attempts left. Without commiting a transaction, same messages will be sent over and over again.","title":"Transactional outbox"},{"location":"recipes/outbox/#running-outbox","text":"Normally you'd be running outbox in an infinite loop, in a separate process - just like you'd do with a Celery worker: from time import sleep while True : with session . begin (): backend . outbox . run ( publish = publisher ) session . commit () sleep ( 3 ) # wait between runs to avoid hammering the database","title":"Running outbox"},{"location":"recipes/outbox/#optional-filterer","text":"Warning This feature and/or its API is provisional and will probably change soon. Sometimes you want only specific events to be sent. You can pass an optional filterer argument to with_outbox . It should be a callable (e.g. a function) that accepts an event instance and returns True if an event should be published. False otherwise. backend = SQLAlchemyBackend ( session ) . with_outbox ( filterer = lambda e : \"InvoicePaid\" in e . name )","title":"Optional filterer"},{"location":"recipes/outbox/#handling-retries","text":"Sending each event will be retried up to 3 times.","title":"Handling retries"},{"location":"recipes/saving_events/","text":"Basic usage Once you have an EventStore instance after integrating with your application and some events defined , you can persist them: invoice_paid = InvoicePaid ( invoice_number = \"1003\" ) stream_id = StreamId ( name = \"invoices/1003\" ) event_store . append ( invoice_paid , stream_id = stream_id ) Events can be later retrieved by using load_stream method: events = event_store . load_stream ( stream_id ) # [ # WrappedEvent( # event=InvoicePaid(invoice_number='1003'), # version=1, # uuid=UUID('831cd32b-02b9-48d2-a67c-28cf7dbb37fa'), # created_at=datetime.datetime(2025, 3, 16, 10, 3, 2, 138667), # context=Context(correlation_id=None, causation_id=None) # ) # ] load_stream returns a list of WrappedEvent objects. They contain a saved event under .event attribute with its metadata in other attributes. Version control All events within a stream by default have assigned version number. This can be used to detect a situation of concurrent writes to the same stream. You have a choice whether you want to check for versions conflict or not. Explicit versioning There is no need to add an expected version when adding some events to the stream for the first time, i.e. creating the stream: stream_id = StreamId ( name = \"invoices/1111\" ) an_event = InvoicePaid ( invoice_number = \"1111\" ) event_store . append ( an_event , stream_id = stream_id ) # no `expected_version` argument given However, when you add events to the stream for the second and subsequent times, you need to pass the expected version explicitly. Otherwise, appending will fail with an exception: another_event = InvoicePaid ( invoice_number = \"1112\" ) # \ud83d\udc47 this would raise an exception # event_store.append(another_event, stream_id=stream_id) Hence, it is assumed that you will use the events versioning and protection against concurrent writes. Info This is a deliberate design choice, so you must decide explicitly if you need a concurrent writes protection or not. In a typical flow, you'll first load a stream, perform some logic, then try to append new events. You'll then get the latest version from the last event loaded: present_events = event_store . load_stream ( stream_id ) last_version = present_events [ - 1 ] . version # ... some logic another_event = InvoicePaid ( invoice_number = \"1112\" ) event_store . append ( another_event , stream_id = stream_id , expected_version = last_version ) No versioning In case when you don't need protection against concurrent writes, you can disable versioning. NO_VERSIONING must be used consistently for every append to such a stream. stream_id = StreamId ( name = \"invoices/123\" ) an_event = InvoicePaid ( invoice_number = \"1111\" ) event_store . append ( an_event , stream_id = stream_id , expected_version = NO_VERSIONING ) another_event = InvoicePaid ( invoice_number = \"1112\" ) event_store . append ( another_event , stream_id = stream_id , expected_version = NO_VERSIONING ) Info Once a stream has been created with disabled versioning, you cannot enable it. It is also forbidden the other way around. You can always create a new stream and delete the old one.","title":"\ud83d\udcdd Save and read events"},{"location":"recipes/saving_events/#basic-usage","text":"Once you have an EventStore instance after integrating with your application and some events defined , you can persist them: invoice_paid = InvoicePaid ( invoice_number = \"1003\" ) stream_id = StreamId ( name = \"invoices/1003\" ) event_store . append ( invoice_paid , stream_id = stream_id ) Events can be later retrieved by using load_stream method: events = event_store . load_stream ( stream_id ) # [ # WrappedEvent( # event=InvoicePaid(invoice_number='1003'), # version=1, # uuid=UUID('831cd32b-02b9-48d2-a67c-28cf7dbb37fa'), # created_at=datetime.datetime(2025, 3, 16, 10, 3, 2, 138667), # context=Context(correlation_id=None, causation_id=None) # ) # ] load_stream returns a list of WrappedEvent objects. They contain a saved event under .event attribute with its metadata in other attributes.","title":"Basic usage"},{"location":"recipes/saving_events/#version-control","text":"All events within a stream by default have assigned version number. This can be used to detect a situation of concurrent writes to the same stream. You have a choice whether you want to check for versions conflict or not.","title":"Version control"},{"location":"recipes/saving_events/#explicit-versioning","text":"There is no need to add an expected version when adding some events to the stream for the first time, i.e. creating the stream: stream_id = StreamId ( name = \"invoices/1111\" ) an_event = InvoicePaid ( invoice_number = \"1111\" ) event_store . append ( an_event , stream_id = stream_id ) # no `expected_version` argument given However, when you add events to the stream for the second and subsequent times, you need to pass the expected version explicitly. Otherwise, appending will fail with an exception: another_event = InvoicePaid ( invoice_number = \"1112\" ) # \ud83d\udc47 this would raise an exception # event_store.append(another_event, stream_id=stream_id) Hence, it is assumed that you will use the events versioning and protection against concurrent writes. Info This is a deliberate design choice, so you must decide explicitly if you need a concurrent writes protection or not. In a typical flow, you'll first load a stream, perform some logic, then try to append new events. You'll then get the latest version from the last event loaded: present_events = event_store . load_stream ( stream_id ) last_version = present_events [ - 1 ] . version # ... some logic another_event = InvoicePaid ( invoice_number = \"1112\" ) event_store . append ( another_event , stream_id = stream_id , expected_version = last_version )","title":"Explicit versioning"},{"location":"recipes/saving_events/#no-versioning","text":"In case when you don't need protection against concurrent writes, you can disable versioning. NO_VERSIONING must be used consistently for every append to such a stream. stream_id = StreamId ( name = \"invoices/123\" ) an_event = InvoicePaid ( invoice_number = \"1111\" ) event_store . append ( an_event , stream_id = stream_id , expected_version = NO_VERSIONING ) another_event = InvoicePaid ( invoice_number = \"1112\" ) event_store . append ( another_event , stream_id = stream_id , expected_version = NO_VERSIONING ) Info Once a stream has been created with disabled versioning, you cannot enable it. It is also forbidden the other way around. You can always create a new stream and delete the old one.","title":"No versioning"},{"location":"recipes/snapshots/","text":"Snapshot is a frozen state of the stream in a given point. The point of the snapshot is identified by the version. Snapshots are a way to optimize loading a long stream of events. When a snapshot is present, it is always loaded. Then, any events newer then a snapshots are also loaded, if only they are present. Let's say we have a stream with 100 events: class TemperatureChanged ( Event ): reading_in_celsius : int stream_id = StreamId ( name = \"temperature_sensor/1\" ) events = [ TemperatureChanged ( reading_in_celsius = reading ) for reading in range ( 100 )] event_store . append ( * events , stream_id = stream_id ) loaded_events = event_store . load_stream ( stream_id ) print ( len ( loaded_events )) # 100 We could define a snapshot event that will capture all information relevant for us: class TemperatureSensorSnapshot ( Event ): readings_so_far : int last_reading_in_celsius : int Now we can save a snapshot, using dedicated EventStore method: last_version = loaded_events [ - 1 ] . version last_event = loaded_events [ - 1 ] . event snapshot = TemperatureSensorSnapshot ( readings_so_far = 100 , last_reading_in_celsius = last_event . reading_in_celsius ) wrapped_snapshot = WrappedEvent . wrap ( snapshot , last_version ) event_store . save_snapshot ( stream_id , wrapped_snapshot ) Note that version of the snapshot must equal to the version of the last event. Now when you'll try to load the stream you'll notice the method only returns a single event and this will be our snapshot: loaded_events = event_store . load_stream ( stream_id ) print ( len ( loaded_events )) # 1 assert isinstance ( loaded_events [ - 1 ] . event , TemperatureSensorSnapshot ) Warning Long streams are usually a sign of a poor stream design. Snapshots are an optimization that should be used only for a good reason. Use with caution!","title":"\ud83d\udcf7 Snapshots"},{"location":"recipes/subscriptions/","text":"Subscriptions allow for asynchronous processing of events in another process. Having backend object after integrating with your application , grab its .subscriber to start building a subscription. Iterating over events one by one Let's say we want to know about all paid invoices and we have an event for that - InvoicePaid . subscription = ( backend . subscriber . start_from ( 0 ) # read from position 0... . to_events ([ InvoicePaid ]) # ...InvoicePaid events... . build_iter ( timelimit = 1 ) # ... iterate events one by one... # ...and wait up to 1 second for a new event ) subscription is an iterable. Thus, it can be used with for loop: for recorded_event in subscription : if recorded_event is None : # no more events for now break # process the event print ( recorded_event . wrapped_event ) print ( recorded_event . position ) print ( recorded_event . tenant_id ) With every iteration we're getting an instance of Recorded or None if there are no new events available. Note subscription is an infinite iterator. Getting None means that you are 'up-to-date' with Event Store but as soon as a new event is appended, it will be returned in another iteration of the loop. Iterating over events in batches When we have to process a bigger amount of events it makes sense to do it in batches instead of processing events one by one. To do it, we need to slightly alter code responsible for building our subscription . Instead of build_iter we call build_batch : batch_subscription = ( backend . subscriber . start_from ( 0 ) . to_events ([ InvoicePaid ]) . build_batch ( size = 10 , timelimit = 1 ) # try getting 10 events at a time ) Then we can pass subscritpion to for loop: for batch in batch_subscription : # process the batch print ( batch ) if len ( batch ) == 0 : # no more events at the moment break In this example, batch is a list of Recorded . Just like in previous case, subscription is an infinite iterator. It will be returning batches of given size as long as there is enough events. If there are fewer 'new' events available, batch subscription will return whatever it can. For example, if you specify batch size of 10 but get a list with 7 events, it means there were only 7 new events and time limit has passed. When batch subscription catches up with event store, it will be returning empty lists. At least until some new events are saved.","title":"\ud83d\uddde\ufe0f React to events - subscriptions"},{"location":"recipes/subscriptions/#iterating-over-events-one-by-one","text":"Let's say we want to know about all paid invoices and we have an event for that - InvoicePaid . subscription = ( backend . subscriber . start_from ( 0 ) # read from position 0... . to_events ([ InvoicePaid ]) # ...InvoicePaid events... . build_iter ( timelimit = 1 ) # ... iterate events one by one... # ...and wait up to 1 second for a new event ) subscription is an iterable. Thus, it can be used with for loop: for recorded_event in subscription : if recorded_event is None : # no more events for now break # process the event print ( recorded_event . wrapped_event ) print ( recorded_event . position ) print ( recorded_event . tenant_id ) With every iteration we're getting an instance of Recorded or None if there are no new events available. Note subscription is an infinite iterator. Getting None means that you are 'up-to-date' with Event Store but as soon as a new event is appended, it will be returned in another iteration of the loop.","title":"Iterating over events one by one"},{"location":"recipes/subscriptions/#iterating-over-events-in-batches","text":"When we have to process a bigger amount of events it makes sense to do it in batches instead of processing events one by one. To do it, we need to slightly alter code responsible for building our subscription . Instead of build_iter we call build_batch : batch_subscription = ( backend . subscriber . start_from ( 0 ) . to_events ([ InvoicePaid ]) . build_batch ( size = 10 , timelimit = 1 ) # try getting 10 events at a time ) Then we can pass subscritpion to for loop: for batch in batch_subscription : # process the batch print ( batch ) if len ( batch ) == 0 : # no more events at the moment break In this example, batch is a list of Recorded . Just like in previous case, subscription is an infinite iterator. It will be returning batches of given size as long as there is enough events. If there are fewer 'new' events available, batch subscription will return whatever it can. For example, if you specify batch size of 10 but get a list with 7 events, it means there were only 7 new events and time limit has passed. When batch subscription catches up with event store, it will be returning empty lists. At least until some new events are saved.","title":"Iterating over events in batches"},{"location":"reference/aggregate/","text":"Source code in event_sourcery/event_sourcing/aggregate.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Aggregate : category : ClassVar [ str ] _changes : list [ Event ] @contextmanager def __persisting_changes__ ( self ) -> Iterator [ Iterator [ Event ]]: yield iter ( getattr ( self , \"_changes\" , [])) self . _changes = [] def __apply__ ( self , event : Event ) -> None : raise NotImplementedError def _emit ( self , event : Event ) -> None : if not hasattr ( self , \"_changes\" ): self . _changes = [] self . __apply__ ( event ) self . _changes . append ( event )","title":"Aggregate"},{"location":"reference/encrypted/","text":"Field annotation for marking event fields that should be encrypted. This annotation is used in combination with DataSubject to implement crypto-shredding, allowing for GDPR-compliant data removal in event-sourced systems. Parameters: Name Type Description Default mask_value Any Value to display when the data is shredded (deleted). Must match the field type. required subject_field str | None Optional. Name of the field containing the subject ID for this encrypted field. If not provided, the field marked with DataSubject will be used. None Example class UserRegistered ( Event ): user_id : Annotated [ str , DataSubject ] email : Annotated [ str , Encrypted ( mask_value = \"[REDACTED]\" )] # Custom subject field other_data : Annotated [ str , Encrypted ( mask_value = \"[HIDDEN]\" , subject_field = \"alternative_id\" , ), ] Note Every event with encrypted fields must have exactly one DataSubject field. The mask_value type must match the field type. After shredding, the field will always return its mask_value. Source code in event_sourcery/event_store/event/_dto.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @dataclass ( frozen = True ) class Encrypted : \"\"\"Field annotation for marking event fields that should be encrypted. This annotation is used in combination with DataSubject to implement crypto-shredding, allowing for GDPR-compliant data removal in event-sourced systems. Args: mask_value: Value to display when the data is shredded (deleted). Must match the field type. subject_field: Optional. Name of the field containing the subject ID for this encrypted field. If not provided, the field marked with DataSubject will be used. Example: ```python class UserRegistered(Event): user_id: Annotated[str, DataSubject] email: Annotated[str, Encrypted(mask_value=\"[REDACTED]\")] # Custom subject field other_data: Annotated[ str, Encrypted( mask_value=\"[HIDDEN]\", subject_field=\"alternative_id\", ), ] ``` Note: - Every event with encrypted fields must have exactly one DataSubject field. - The mask_value type must match the field type. - After shredding, the field will always return its mask_value. \"\"\" mask_value : Any subject_field : str | None = None","title":"Encrypted"},{"location":"reference/event/","text":"Bases: BaseModel Base class for all events. Example usage: class OrderCancelled(Event): order_id: OrderId Source code in event_sourcery/event_store/event/_dto.py 36 37 38 39 40 41 42 43 44 class Event ( BaseModel , extra = \"forbid\" ): \"\"\"Base class for all events. Example usage: ``` class OrderCancelled(Event): order_id: OrderId ``` \"\"\"","title":"Event"},{"location":"reference/event_registry/","text":"Keeps mappings between event types and their names. Normally, there is no need to use it directly. If one needs to have multiple registries or wants more granular control, they can pass an instance of EventRegistry to BackendFactory. Source code in event_sourcery/event_store/event/_registry.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class EventRegistry : \"\"\"Keeps mappings between event types and their names. Normally, there is no need to use it directly. If one needs to have multiple registries or wants more granular control, they can pass an instance of EventRegistry to BackendFactory.\"\"\" def __init__ ( self ) -> None : self . _types_to_names : dict [ type [ TEvent ], str ] = {} self . _names_to_types : dict [ str , type [ TEvent ]] = {} self . _encrypted_fields : dict [ type [ TEvent ], dict [ str , Encrypted ]] = {} self . _subject_fields : dict [ type [ TEvent ], str ] = {} self . _register_defined_events () def _register_defined_events ( self ) -> None : for event_type in Event . __subclasses__ (): not_registered_type = event_type not in self . _types_to_names not_registered_name = event_name ( event_type ) not in self . _names_to_types not_registered_type and not_registered_name and self . add ( event_type ) def add ( self , event : type [ TEvent ]) -> type [ TEvent ]: \"\"\"Add event subclass to the registry.\"\"\" if event in self . _types_to_names : raise DuplicatedEvent ( f \"Duplicated Event detected! { event } \" ) name = event_name ( event ) if name in self . _names_to_types : raise DuplicatedEvent ( f \"Duplicated Event name detected! { name } \" ) self . _types_to_names [ event ] = name self . _names_to_types [ name ] = event self . _encrypted_fields [ event ] = get_encrypted_fields ( of = event ) subject_field = get_data_subject_filed ( of = event ) if subject_field is not None : self . _subject_fields [ event ] = subject_field return event # for use as a decorator def type_for_name ( self , name : str ) -> type [ TEvent ]: if name not in self . _names_to_types : self . _register_defined_events () return self . _names_to_types [ name ] def name_for_type ( self , event : type [ TEvent ]) -> str : if event not in self . _types_to_names : self . _register_defined_events () return self . _types_to_names [ event ] def encrypted_fields ( self , of : type [ TEvent ]) -> dict [ str , Encrypted ]: return self . _encrypted_fields [ of ] def subject_filed ( self , for_field : str , of : type [ TEvent ]) -> str | None : metadata = self . _encrypted_fields [ of ] . get ( for_field ) return ( metadata and metadata . subject_field ) or self . _subject_fields . get ( of ) add ( event ) Add event subclass to the registry. Source code in event_sourcery/event_store/event/_registry.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def add ( self , event : type [ TEvent ]) -> type [ TEvent ]: \"\"\"Add event subclass to the registry.\"\"\" if event in self . _types_to_names : raise DuplicatedEvent ( f \"Duplicated Event detected! { event } \" ) name = event_name ( event ) if name in self . _names_to_types : raise DuplicatedEvent ( f \"Duplicated Event name detected! { name } \" ) self . _types_to_names [ event ] = name self . _names_to_types [ name ] = event self . _encrypted_fields [ event ] = get_encrypted_fields ( of = event ) subject_field = get_data_subject_filed ( of = event ) if subject_field is not None : self . _subject_fields [ event ] = subject_field return event # for use as a decorator","title":"EventRegistry"},{"location":"reference/event_registry/#event_sourcery.event_store.EventRegistry.add","text":"Add event subclass to the registry. Source code in event_sourcery/event_store/event/_registry.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def add ( self , event : type [ TEvent ]) -> type [ TEvent ]: \"\"\"Add event subclass to the registry.\"\"\" if event in self . _types_to_names : raise DuplicatedEvent ( f \"Duplicated Event detected! { event } \" ) name = event_name ( event ) if name in self . _names_to_types : raise DuplicatedEvent ( f \"Duplicated Event name detected! { name } \" ) self . _types_to_names [ event ] = name self . _names_to_types [ name ] = event self . _encrypted_fields [ event ] = get_encrypted_fields ( of = event ) subject_field = get_data_subject_filed ( of = event ) if subject_field is not None : self . _subject_fields [ event ] = subject_field return event # for use as a decorator","title":"add"},{"location":"reference/event_store/","text":"API for working with events. Source code in event_sourcery/event_store/event_store.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class EventStore : \"\"\"API for working with events.\"\"\" def __init__ ( self , storage_strategy : StorageStrategy , serde : Serde ) -> None : self . _storage_strategy = storage_strategy self . _serde = serde def load_stream ( self , stream_id : StreamId , start : int | None = None , stop : int | None = None , ) -> Sequence [ WrappedEvent ]: \"\"\"Loads events from a given stream. Examples: >>> event_store.load_stream(stream_id=StreamId(name=\"not_existing_stream\")) [] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\")) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\"), start=2, stop=3) [WrappedEvent(..., version=2)] Args: stream_id: The stream identifier to load events from. start: The stream version to start loading from (including). stop: The stream version to stop loading at (excluding). Returns: A sequence of events or empty list if the stream doesn't exist. \"\"\" events = self . _storage_strategy . fetch_events ( stream_id , start = start , stop = stop ) return self . _serde . deserialize_many ( events ) @singledispatchmethod def append ( self , first : WrappedEvent , * events : WrappedEvent , stream_id : StreamId , expected_version : int | Versioning = 0 , ) -> None : \"\"\"Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store.append(WrappedEvent(...), stream_id=StreamId()) None >>> event_store.append(WrappedEvent(...), stream_id=StreamId(), expected_version=1) None Args: first: The first event to append (WrappedEvent or Event). *events: The rest of the events to append (same type as first argument). stream_id: The stream identifier to append events to. expected_version: The expected version of the stream Returns: None \"\"\" self . _append ( stream_id = stream_id , events = ( first , * events ), expected_version = expected_version , ) @append . register def _append_events ( self , * events : Event , stream_id : StreamId , expected_version : int | Versioning = 0 , ) -> None : wrapped_events = self . _wrap_events ( expected_version , events ) self . append ( * wrapped_events , stream_id = stream_id , expected_version = expected_version , ) @singledispatchmethod def _wrap_events ( self , expected_version : int , events : Sequence [ Event ], ) -> Sequence [ WrappedEvent ]: return [ WrappedEvent . wrap ( event = event , version = version ) for version , event in enumerate ( events , start = expected_version + 1 ) ] @_wrap_events . register def _wrap_events_versioning ( self , expected_version : Versioning , events : Sequence [ Event ] ) -> Sequence [ WrappedEvent ]: return [ WrappedEvent . wrap ( event = event , version = None ) for event in events ] def _append ( self , stream_id : StreamId , events : Sequence [ WrappedEvent ], expected_version : int | Versioning , ) -> None : new_version = events [ - 1 ] . version versioning : Versioning if expected_version is not NO_VERSIONING : versioning = ExplicitVersioning ( expected_version = cast ( int , expected_version ), initial_version = cast ( int , new_version ), ) else : versioning = NO_VERSIONING self . _storage_strategy . insert_events ( stream_id = stream_id , versioning = versioning , events = self . _serde . serialize_many ( events , stream_id ), ) def delete_stream ( self , stream_id : StreamId ) -> None : \"\"\"Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store.delete_stream(StreamId()) None >>> event_store.delete_stream(StreamId(name=\"not_existing_stream\")) None Args: stream_id: The stream identifier to delete. Returns: None \"\"\" self . _storage_strategy . delete_stream ( stream_id ) def save_snapshot ( self , stream_id : StreamId , snapshot : WrappedEvent ) -> None : \"\"\"Saves a snapshot of the stream. Examples: >>> event_store.save_snapshot(StreamId(), WrappedEvent(...)) None >>> event_store.save_snapshot(StreamId(name=\"not_existing_stream\"), WrappedEvent(...)) None Args: stream_id: The stream identifier to save the snapshot. snapshot: The snapshot to save. Returns: None \"\"\" serialized = self . _serde . serialize ( event = snapshot , stream_id = stream_id ) self . _storage_strategy . save_snapshot ( serialized ) @property def position ( self ) -> Position | None : \"\"\"Returns the current position of the event store. Examples: >>> event_store.position None # nothing was saved yet >>> event_store.position Position(15) # Some events were saved \"\"\" return self . _storage_strategy . current_position position : Position | None property Returns the current position of the event store. Examples: >>> event_store . position None # nothing was saved yet >>> event_store . position Position(15) # Some events were saved append ( first , * events , stream_id , expected_version = 0 ) Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store . append ( WrappedEvent ( ... ), stream_id = StreamId ()) None >>> event_store . append ( WrappedEvent ( ... ), stream_id = StreamId (), expected_version = 1 ) None Parameters: Name Type Description Default first WrappedEvent The first event to append (WrappedEvent or Event). required *events WrappedEvent The rest of the events to append (same type as first argument). () stream_id StreamId The stream identifier to append events to. required expected_version int | Versioning The expected version of the stream 0 Returns: Type Description None None Source code in event_sourcery/event_store/event_store.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @singledispatchmethod def append ( self , first : WrappedEvent , * events : WrappedEvent , stream_id : StreamId , expected_version : int | Versioning = 0 , ) -> None : \"\"\"Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store.append(WrappedEvent(...), stream_id=StreamId()) None >>> event_store.append(WrappedEvent(...), stream_id=StreamId(), expected_version=1) None Args: first: The first event to append (WrappedEvent or Event). *events: The rest of the events to append (same type as first argument). stream_id: The stream identifier to append events to. expected_version: The expected version of the stream Returns: None \"\"\" self . _append ( stream_id = stream_id , events = ( first , * events ), expected_version = expected_version , ) delete_stream ( stream_id ) Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store . delete_stream ( StreamId ()) None >>> event_store . delete_stream ( StreamId ( name = \"not_existing_stream\" )) None Parameters: Name Type Description Default stream_id StreamId The stream identifier to delete. required Returns: Type Description None None Source code in event_sourcery/event_store/event_store.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def delete_stream ( self , stream_id : StreamId ) -> None : \"\"\"Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store.delete_stream(StreamId()) None >>> event_store.delete_stream(StreamId(name=\"not_existing_stream\")) None Args: stream_id: The stream identifier to delete. Returns: None \"\"\" self . _storage_strategy . delete_stream ( stream_id ) load_stream ( stream_id , start = None , stop = None ) Loads events from a given stream. Examples: >>> event_store . load_stream ( stream_id = StreamId ( name = \"not_existing_stream\" )) [] >>> event_store . load_stream ( stream_id = StreamId ( name = \"existing_stream\" )) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store . load_stream ( stream_id = StreamId ( name = \"existing_stream\" ), start = 2 , stop = 3 ) [WrappedEvent(..., version=2)] Parameters: Name Type Description Default stream_id StreamId The stream identifier to load events from. required start int | None The stream version to start loading from (including). None stop int | None The stream version to stop loading at (excluding). None Returns: Type Description Sequence [ WrappedEvent ] A sequence of events or empty list if the stream doesn't exist. Source code in event_sourcery/event_store/event_store.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def load_stream ( self , stream_id : StreamId , start : int | None = None , stop : int | None = None , ) -> Sequence [ WrappedEvent ]: \"\"\"Loads events from a given stream. Examples: >>> event_store.load_stream(stream_id=StreamId(name=\"not_existing_stream\")) [] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\")) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\"), start=2, stop=3) [WrappedEvent(..., version=2)] Args: stream_id: The stream identifier to load events from. start: The stream version to start loading from (including). stop: The stream version to stop loading at (excluding). Returns: A sequence of events or empty list if the stream doesn't exist. \"\"\" events = self . _storage_strategy . fetch_events ( stream_id , start = start , stop = stop ) return self . _serde . deserialize_many ( events ) save_snapshot ( stream_id , snapshot ) Saves a snapshot of the stream. Examples: >>> event_store . save_snapshot ( StreamId (), WrappedEvent ( ... )) None >>> event_store . save_snapshot ( StreamId ( name = \"not_existing_stream\" ), WrappedEvent ( ... )) None Parameters: Name Type Description Default stream_id StreamId The stream identifier to save the snapshot. required snapshot WrappedEvent The snapshot to save. required Returns: Type Description None None Source code in event_sourcery/event_store/event_store.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def save_snapshot ( self , stream_id : StreamId , snapshot : WrappedEvent ) -> None : \"\"\"Saves a snapshot of the stream. Examples: >>> event_store.save_snapshot(StreamId(), WrappedEvent(...)) None >>> event_store.save_snapshot(StreamId(name=\"not_existing_stream\"), WrappedEvent(...)) None Args: stream_id: The stream identifier to save the snapshot. snapshot: The snapshot to save. Returns: None \"\"\" serialized = self . _serde . serialize ( event = snapshot , stream_id = stream_id ) self . _storage_strategy . save_snapshot ( serialized )","title":"Event Store"},{"location":"reference/event_store/#event_sourcery.event_store.event_store.EventStore.position","text":"Returns the current position of the event store. Examples: >>> event_store . position None # nothing was saved yet >>> event_store . position Position(15) # Some events were saved","title":"position"},{"location":"reference/event_store/#event_sourcery.event_store.event_store.EventStore.append","text":"Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store . append ( WrappedEvent ( ... ), stream_id = StreamId ()) None >>> event_store . append ( WrappedEvent ( ... ), stream_id = StreamId (), expected_version = 1 ) None Parameters: Name Type Description Default first WrappedEvent The first event to append (WrappedEvent or Event). required *events WrappedEvent The rest of the events to append (same type as first argument). () stream_id StreamId The stream identifier to append events to. required expected_version int | Versioning The expected version of the stream 0 Returns: Type Description None None Source code in event_sourcery/event_store/event_store.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @singledispatchmethod def append ( self , first : WrappedEvent , * events : WrappedEvent , stream_id : StreamId , expected_version : int | Versioning = 0 , ) -> None : \"\"\"Appends events to a stream with a given ID. Implements optimistic locking to ensure stream wasn't modified since last read. To use it, pass the expected version of the stream. Examples: >>> event_store.append(WrappedEvent(...), stream_id=StreamId()) None >>> event_store.append(WrappedEvent(...), stream_id=StreamId(), expected_version=1) None Args: first: The first event to append (WrappedEvent or Event). *events: The rest of the events to append (same type as first argument). stream_id: The stream identifier to append events to. expected_version: The expected version of the stream Returns: None \"\"\" self . _append ( stream_id = stream_id , events = ( first , * events ), expected_version = expected_version , )","title":"append"},{"location":"reference/event_store/#event_sourcery.event_store.event_store.EventStore.delete_stream","text":"Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store . delete_stream ( StreamId ()) None >>> event_store . delete_stream ( StreamId ( name = \"not_existing_stream\" )) None Parameters: Name Type Description Default stream_id StreamId The stream identifier to delete. required Returns: Type Description None None Source code in event_sourcery/event_store/event_store.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def delete_stream ( self , stream_id : StreamId ) -> None : \"\"\"Deletes a stream with a given ID. If a stream does not exist, this method does nothing. Examples: >>> event_store.delete_stream(StreamId()) None >>> event_store.delete_stream(StreamId(name=\"not_existing_stream\")) None Args: stream_id: The stream identifier to delete. Returns: None \"\"\" self . _storage_strategy . delete_stream ( stream_id )","title":"delete_stream"},{"location":"reference/event_store/#event_sourcery.event_store.event_store.EventStore.load_stream","text":"Loads events from a given stream. Examples: >>> event_store . load_stream ( stream_id = StreamId ( name = \"not_existing_stream\" )) [] >>> event_store . load_stream ( stream_id = StreamId ( name = \"existing_stream\" )) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store . load_stream ( stream_id = StreamId ( name = \"existing_stream\" ), start = 2 , stop = 3 ) [WrappedEvent(..., version=2)] Parameters: Name Type Description Default stream_id StreamId The stream identifier to load events from. required start int | None The stream version to start loading from (including). None stop int | None The stream version to stop loading at (excluding). None Returns: Type Description Sequence [ WrappedEvent ] A sequence of events or empty list if the stream doesn't exist. Source code in event_sourcery/event_store/event_store.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def load_stream ( self , stream_id : StreamId , start : int | None = None , stop : int | None = None , ) -> Sequence [ WrappedEvent ]: \"\"\"Loads events from a given stream. Examples: >>> event_store.load_stream(stream_id=StreamId(name=\"not_existing_stream\")) [] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\")) [WrappedEvent(..., version=1), ..., WrappedEvent(..., version=3)] >>> event_store.load_stream(stream_id=StreamId(name=\"existing_stream\"), start=2, stop=3) [WrappedEvent(..., version=2)] Args: stream_id: The stream identifier to load events from. start: The stream version to start loading from (including). stop: The stream version to stop loading at (excluding). Returns: A sequence of events or empty list if the stream doesn't exist. \"\"\" events = self . _storage_strategy . fetch_events ( stream_id , start = start , stop = stop ) return self . _serde . deserialize_many ( events )","title":"load_stream"},{"location":"reference/event_store/#event_sourcery.event_store.event_store.EventStore.save_snapshot","text":"Saves a snapshot of the stream. Examples: >>> event_store . save_snapshot ( StreamId (), WrappedEvent ( ... )) None >>> event_store . save_snapshot ( StreamId ( name = \"not_existing_stream\" ), WrappedEvent ( ... )) None Parameters: Name Type Description Default stream_id StreamId The stream identifier to save the snapshot. required snapshot WrappedEvent The snapshot to save. required Returns: Type Description None None Source code in event_sourcery/event_store/event_store.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def save_snapshot ( self , stream_id : StreamId , snapshot : WrappedEvent ) -> None : \"\"\"Saves a snapshot of the stream. Examples: >>> event_store.save_snapshot(StreamId(), WrappedEvent(...)) None >>> event_store.save_snapshot(StreamId(name=\"not_existing_stream\"), WrappedEvent(...)) None Args: stream_id: The stream identifier to save the snapshot. snapshot: The snapshot to save. Returns: None \"\"\" serialized = self . _serde . serialize ( event = snapshot , stream_id = stream_id ) self . _storage_strategy . save_snapshot ( serialized )","title":"save_snapshot"},{"location":"reference/recorded/","text":"Bases: Entry DTO containing events saved in the event store. It contains position of the event in the event store and tenant id. Source code in event_sourcery/event_store/event/_dto.py 90 91 92 93 94 95 96 97 98 @dataclasses . dataclass ( frozen = True ) class Recorded ( Entry ): \"\"\"DTO containing events saved in the event store. It contains position of the event in the event store and tenant id. \"\"\" position : Position tenant_id : TenantId = DEFAULT_TENANT","title":"Recorded"},{"location":"reference/repository/","text":"Bases: Generic [ TAggregate ] Source code in event_sourcery/event_sourcing/repository.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class Repository ( Generic [ TAggregate ]): def __init__ ( self , event_store : EventStore ) -> None : self . _event_store = event_store @contextmanager def aggregate ( self , uuid : StreamUUID , aggregate : TAggregate , ) -> Iterator [ TAggregate ]: stream_id = StreamId ( uuid = uuid , name = uuid . name , category = aggregate . category ) old_version = self . _load ( stream_id , aggregate ) yield aggregate self . _save ( aggregate , old_version , stream_id ) def _load ( self , stream_id : StreamId , aggregate : TAggregate ) -> int : stream = self . _event_store . load_stream ( stream_id ) last_version = 0 for envelope in stream : aggregate . __apply__ ( envelope . event ) last_version = cast ( int , envelope . version ) return last_version def _save ( self , aggregate : TAggregate , old_version : int , stream_id : StreamId , ) -> None : with aggregate . __persisting_changes__ () as pending : start_from = old_version + 1 events = [ WrappedEvent . wrap ( event , version ) for version , event in enumerate ( pending , start = start_from ) ] if not events : return self . _event_store . append ( * events , stream_id = stream_id , expected_version = old_version , )","title":"Repository"},{"location":"reference/stream_id/","text":"Bases: StreamUUID Source code in event_sourcery/event_store/stream_id.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @dataclass ( frozen = True , repr = False , eq = False ) class StreamId ( StreamUUID ): category : Category | None = None def __repr__ ( self ) -> str : return ( f \" { type ( self ) . __name__ } \" f \"(hex= { self !s} , name= { self . name } , category= { self . category } )\" ) def __eq__ ( self , other : Any ) -> bool : if isinstance ( other , StreamId ): return super () . __eq__ ( other ) and self . category == other . category return NotImplemented def __hash__ ( self ) -> int : return hash (( self . category , super () . __hash__ ()))","title":"StreamId"},{"location":"reference/wrapped_event/","text":"Bases: Generic [ TEvent ] Wrapper for events with all relevant metadata. Returned from EventStore when loading events from a stream. Example usage: class OrderCancelled(Event): order_id: OrderId event = OrderCancelled(order_id=OrderId(\"#123\")) wrapped_event = WrappedEvent.wrap(event, version=1) Source code in event_sourcery/event_store/event/_dto.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @dataclasses . dataclass () class WrappedEvent ( Generic [ TEvent ]): \"\"\"Wrapper for events with all relevant metadata. Returned from EventStore when loading events from a stream. Example usage: ``` class OrderCancelled(Event): order_id: OrderId event = OrderCancelled(order_id=OrderId(\"#123\")) wrapped_event = WrappedEvent.wrap(event, version=1) ``` \"\"\" event : TEvent version : int | None uuid : UUID = dataclasses . field ( default_factory = uuid4 ) created_at : datetime = dataclasses . field ( default_factory = lambda : datetime . now ( timezone . utc ) . replace ( tzinfo = None ) ) context : Context = dataclasses . field ( default_factory = Context ) @classmethod def wrap ( cls , event : TEvent , version : int | None ) -> \"WrappedEvent[TEvent]\" : return WrappedEvent [ TEvent ]( event = event , version = version )","title":"WrappedEvent"}]}